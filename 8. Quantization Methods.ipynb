{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f9fdf2-90e6-4890-8a68-8cd46c4d85bc",
   "metadata": {},
   "source": [
    "## **8. Quantization**\n",
    "\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/quantization/overview\n",
    "\n",
    "```\n",
    "> bitsandbytes\n",
    "> gguf\n",
    "> torchao\n",
    "> quanto\n",
    "```\n",
    "\n",
    "- **Quantization** focuses on representing data with fewer bits while also trying to preserve the precision of the original data.\n",
    "  - This often means converting a data type to represent the same information with fewer bits.\n",
    "  - If your model weights are stored as 32-bit floating points and they’re quantized to 16-bit floating points, this halves the model size which makes it easier to store and reduces memory usage.\n",
    "  - Lower precision can also speedup inference because it takes less time to perform calculations with fewer bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575eaa57-48bd-43b4-a05a-ef28c525c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.quantizers import PipelineQuantizationConfig\n",
    "from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n",
    "from diffusers.quantizers.quantization_config import QuantoConfig\n",
    "from diffusers.quantizers import PipelineQuantizationConfig\n",
    "from diffusers import SD3Transformer2DModel\n",
    "from diffusers import FluxPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\n",
    "\n",
    "from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig\n",
    "\n",
    "from diffusers import AutoModel, BitsAndBytesConfig, TorchAoConfig, QuantoConfig\n",
    "from transformers import T5EncoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1c2e1-55b2-437d-839b-481aa960884b",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Pipeline-level quantization**\n",
    "- There are two ways you can use `PipelineQuantizationConfig` depending on the level of control you want over the quantization specifications of each model in the pipeline.\n",
    "  - for more basic and simple use cases, you only need to define the quant_backend, quant_kwargs, and `components_to_quantize`\n",
    "  - for more granular quantization control, provide a `quant_mapping` that provides the quantization specifications for the individual model components.\n",
    "\n",
    "- **Simple quantization**\n",
    "  - Initialize `PipelineQuantizationConfig` with the following parameters.\n",
    "    - `quant_backend` specifies which quantization backend to use. Currently supported backends include: `bitsandbytes_4bit`, `bitsandbytes_8bit`, `gguf`, `quanto`, and `torchao`.\n",
    "    - `quant_kwargs` contains the specific quantization arguments to use.\n",
    "    - `components_to_quantize` specifies which components of the pipeline to quantize.\n",
    "      - Typically, you should quantize the most compute intensive components like the transformer.\n",
    "      - The text encoder is another component to consider quantizing if a pipeline has more than one such as `FluxPipeline`.\n",
    "      - The example below quantizes the T5 text encoder in FluxPipeline while keeping the CLIP model intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c8afa-a11a-4c05-bc18-0ba73933e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_quant_config = PipelineQuantizationConfig(\n",
    "    quant_backend=\"bitsandbytes_4bit\",\n",
    "    quant_kwargs={\"load_in_4bit\": True, \"bnb_4bit_quant_type\": \"nf4\", \"bnb_4bit_compute_dtype\": torch.bfloat16},\n",
    "    components_to_quantize=[\"transformer\", \"text_encoder_2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6487b7e-f44b-4c0e-8934-4f1413d668f8",
   "metadata": {},
   "source": [
    "- Pass the `pipeline_quant_config` to `from_pretrained()` to quantize the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275c8fa-0d89-4403-9e84-a048022aa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    quantization_config=pipeline_quant_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipe(\"photo of a cute dog\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae516c-4ddb-41b6-8baa-096e9229933a",
   "metadata": {},
   "source": [
    "#### quant_mapping\n",
    "- The `quant_mapping` argument provides more flexible options for how to quantize each individual component in a pipeline, like combining different quantization backends.\n",
    "  - Initialize `PipelineQuantizationConfig` and pass a `quant_mapping` to it.\n",
    "  - The `quant_mapping` allows you to specify the quantization options for each component in the pipeline such as the transformer and text encoder.\n",
    "\n",
    "The example below uses two quantization backends, ~quantizers.QuantoConfig and transformers.BitsAndBytesConfig, for the transformer and text encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743445a-715c-462c-9820-de7c4de6e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_quant_config = PipelineQuantizationConfig(\n",
    "    quant_mapping={\n",
    "        \"transformer\": QuantoConfig(weights_dtype=\"int8\"),\n",
    "        \"text_encoder_2\": TransformersBitsAndBytesConfig(\n",
    "            load_in_4bit=True, compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6432699-7e8b-4b37-86bb-2adc74953899",
   "metadata": {},
   "source": [
    "- There is a separate bitsandbytes backend in Transformers.\n",
    "  - You need to import and use `transformers.BitsAndBytesConfig` for components that come from `Transformers`.\n",
    "  - `text_encoder_2` in `FluxPipeline` is a `T5EncoderModel` from Transformers so you need to use transformers.\n",
    "  - `BitsAndBytesConfig` instead of `diffusers.BitsAndBytesConfig`.\n",
    "\n",
    "- Pass the `pipeline_quant_config` to `from_pretrained()` to quantize the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729f232-f487-422f-9fbb-47e63714ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_quant_config = PipelineQuantizationConfig(\n",
    "    quant_mapping={\n",
    "        \"transformer\": DiffusersBitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16),\n",
    "        \"text_encoder_2\": TransformersBitsAndBytesConfig(\n",
    "            load_in_4bit=True, compute_dtype=torch.bfloat16\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    quantization_config=pipeline_quant_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipe(\"photo of a cute dog\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595f302-e601-4a95-8b33-30bf3d1870a3",
   "metadata": {},
   "source": [
    "---------\n",
    "### **bitsandbytes**\n",
    "- **bitsandbytes** is the easiest option for quantizing a model to 8 and 4-bit.\n",
    "  - 8-bit quantization multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16.\n",
    "  - This reduces the degradative effect outlier values have on a model’s performance.\n",
    "\n",
    "- 4-bit quantization compresses a model even further, and it is commonly used with `QLoRA` to finetune quantized LLMs.\n",
    "- Quantize a model by passing a `BitsAndBytesConfig` to `from_pretrained()`.\n",
    "  - This works for any model in any modality, as long as it supports loading with `Accelerate` and contains `torch.nn.Linear` layers.\n",
    " \n",
    "- **8-bit**\n",
    "  - Quantizing a model in 8-bit halves the memory-usage:\n",
    "    - bitsandbytes is supported in both `Transformers` and `Diffusers`, so you can quantize both the `FluxTransformer2DModel` and `T5EncoderModel`.\n",
    "    - For Ada and higher-series GPUs. we recommend changing `torch_dtype` to `torch.bfloat16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6084f72-b37c-43c5-a341-f1b4099ba4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = TransformersBitsAndBytesConfig(load_in_8bit=True,)\n",
    "\n",
    "text_encoder_2_8bit = T5EncoderModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"text_encoder_2\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True,)\n",
    "\n",
    "transformer_8bit = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5514f1-6c17-43ab-9062-add9aefc9261",
   "metadata": {},
   "source": [
    "- **4-bit**\n",
    "  - Quantizing a model in 4-bit reduces your memory-usage by 4x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2cc7b9-f8bc-4633-86c0-33643812ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = TransformersBitsAndBytesConfig(load_in_4bit=True,)\n",
    "\n",
    "text_encoder_2_4bit = T5EncoderModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"text_encoder_2\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "quant_config = DiffusersBitsAndBytesConfig(load_in_4bit=True,)\n",
    "\n",
    "transformer_4bit = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694afc3-6d04-42bd-8b2d-906f14d7033b",
   "metadata": {},
   "source": [
    "- By default, all the other modules such as `torch.nn.LayerNorm` are converted to `torch.float16`.\n",
    "  - You can change the data type of these modules with the `torch_dtype` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58456607-7ebf-4b41-852b-132fae5e7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_4bit = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quant_config,\n",
    "# +   torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ff4ab-ee14-4ae7-b353-5160a263d5db",
   "metadata": {},
   "source": [
    "- generate an image using our quantized models.\n",
    "\n",
    "Setting device_map=\"auto\" automatically fills all available space on the GPU(s) first, then the CPU, and finally, the hard drive (the absolute slowest option) if there is still not enough memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d566877-fb1a-4c68-a803-aa5d489a8dc7",
   "metadata": {},
   "source": [
    "- Generate an image using our quantized models.\n",
    "  - Setting `device_map=\"auto\"` automatically fills all available space on the GPU(s) first, then the CPU, and finally, the hard drive (the absolute slowest option) if there is still not enough memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f71223-3d63-4b61-8cee-efae7436c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    transformer=transformer_4bit,\n",
    "    text_encoder_2=text_encoder_2_4bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipe_kwargs = {\n",
    "    \"prompt\": \"A cat holding a sign that says hello world\",\n",
    "    \"height\": 1024,\n",
    "    \"width\": 1024,\n",
    "    \"guidance_scale\": 3.5,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"max_sequence_length\": 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28255720-0400-4bc5-827c-6b0eb9fd8a5f",
   "metadata": {},
   "source": [
    "- When there is enough memory, you can also directly move the pipeline to the GPU with `.to(\"cuda\")` and apply `enable_model_cpu_offload()` to optimize GPU memory usage.\n",
    "  - Once a model is quantized, you can push the model to the Hub with the `push_to_hub()` method.\n",
    "  - The quantization `config.json` file is pushed first, followed by the quantized model weights.\n",
    "  - You can also save the serialized 4-bit models locally with `save_pretrained()`.\n",
    "\n",
    "- Training with 8-bit and 4-bit weights are only supported for training extra parameters.\n",
    "  - Check your memory footprint with the `get_memory_footprint` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d842d94-4723-4a9d-ba6f-2ac876e089de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e96a5d-edae-4285-b603-c73b9f746a39",
   "metadata": {},
   "source": [
    "- This only tells you the memory footprint of the model params and does not estimate the inference memory requirements.\n",
    "  - Quantized models can be loaded from the `from_pretrained()` method without needing to specify the `quantization_config parameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c91508c-d238-4ffb-af49-73e8aea865e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model_4bit = AutoModel.from_pretrained(\n",
    "    \"hf-internal-testing/flux.1-dev-nf4-pkg\", subfolder=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a392032-7925-44b8-8096-16f84dc81ae5",
   "metadata": {},
   "source": [
    "#### 8-bit (LLM.int8() algorithm)\n",
    "- **Outlier threshold**\n",
    "  - An “outlier” is a hidden state value greater than a certain threshold, and these values are computed in `fp16`.\n",
    "  - While the values are usually normally distributed (`[-3.5, 3.5]`), this distribution can be very different for large models (`[-60, 6]` or `[6, 60]`).\n",
    "    - 8-bit quantization works well for values `~5`, but beyond that, there is a significant performance penalty.\n",
    "    - A good default threshold value is `6`, but a lower threshold may be needed for more unstable models (small models or finetuning).\n",
    "\n",
    "- To find the best threshold for your model, we recommend experimenting with the llm_int8_threshold parameter in BitsAndBytesConfig:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2d0e8-2430-49f7-813f-86f493e467b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, llm_int8_threshold=10,\n",
    ")\n",
    "\n",
    "model_8bit = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745803ca-ff57-4631-a71f-4e92f1ff6384",
   "metadata": {},
   "source": [
    "- **Skip module conversion**\n",
    "  - For some models, you don’t need to quantize every module to 8-bit which can actually cause instability.\n",
    "  - For example, for diffusion models like `Stable Diffusion 3`, the `proj_out` module can be skipped using the `llm_int8_skip_modules` parameter in `BitsAndBytesConfig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f82ab4-2ddf-4d09-8218-2033c347a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, llm_int8_skip_modules=[\"proj_out\"],\n",
    ")\n",
    "\n",
    "model_8bit = SD3Transformer2DModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57a41e-7ca1-437b-9f9f-e334a25be702",
   "metadata": {},
   "source": [
    "#### 4-bit (QLoRA algorithm)\n",
    "- Explores some of the specific features of 4-bit models, such as changing the compute data type, using the Normal Float 4 (NF4) data type, and using nested quantization.\n",
    "\n",
    "- **Compute data type**\n",
    "  - To speedup computation, you can change the data type from `float32` (the default value) to `bf16` using the `bnb_4bit_compute_dtype` parameter in `BitsAndBytesConfig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb06ed-1c14-42ae-be24-5bda05bb7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1d9dd-781e-4cd0-9256-2a706883bb96",
   "metadata": {},
   "source": [
    "- **Normal Float 4 (NF4)**\n",
    "  - NF4 is a 4-bit data type from the QLoRA paper, adapted for weights initialized from a normal distribution.\n",
    "    - You should use NF4 for training 4-bit base models.\n",
    "    - This can be configured with the `bnb_4bit_quant_type` parameter in the `BitsAndBytesConfig`:\n",
    "  - For inference, the `bnb_4bit_quant_type` does not have a huge impact on performance.\n",
    "    - However, to remain consistent with the model weights, you should use the `bnb_4bit_compute_dtype` and `torch_dtype` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fd405-3154-45c3-b94f-4c61a885c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = TransformersBitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "text_encoder_2_4bit = T5EncoderModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"text_encoder_2\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "quant_config = DiffusersBitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "transformer_4bit = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d21a83-58c8-4bb3-bdf4-f98a7151f11d",
   "metadata": {},
   "source": [
    "- **Nested quantization**\n",
    "  - Nested quantization is a technique that can save additional memory at no additional performance cost.\n",
    "  - This feature performs a second quantization of the already quantized weights to save an additional 0.4 bits/parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55feb0-020f-4045-8d81-43f4b6c81d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = TransformersBitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "text_encoder_2_4bit = T5EncoderModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"text_encoder_2\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "quant_config = DiffusersBitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "transformer_4bit = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb393e-a79c-47cd-8825-082dfe479b77",
   "metadata": {},
   "source": [
    "#### Dequantizing bitsandbytes models\n",
    "- Once quantized, you can dequantize a model to its original precision, but this might result in a small loss of quality.\n",
    "  - Make sure you have enough GPU RAM to fit the dequantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd4918c-8536-4272-97ae-542cc5d4d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = TransformersBitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "text_encoder_2_4bit = T5EncoderModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"text_encoder_2\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "quant_config = DiffusersBitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "transformer_4bit = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "text_encoder_2_4bit.dequantize()\n",
    "transformer_4bit.dequantize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e4034-abf9-466a-bc6d-c14833295d18",
   "metadata": {},
   "source": [
    "------------\n",
    "### **GGUF**\n",
    "- The **GGUF** file format is typically used to store models for inference with GGML and supports a variety of block wise quantization options.\n",
    "  - Diffusers supports loading checkpoints prequantized and saved in the GGUF format via `from_single_file` loading with Model classes.\n",
    "  - Loading GGUF checkpoints via Pipelines is currently not supported.\n",
    "\n",
    "- Since GGUF is a single file format, use `~FromSingleFileMixin.from_single_file` to load the model and pass in the `GGUFQuantizationConfig`.\n",
    "  - When using GGUF checkpoints, the quantized weights remain in a low memory dtype(typically `torch.uint8`) and are dynamically dequantized and cast to the configured `compute_dtype` during each module’s forward pass through the model.\n",
    "  - The `GGUFQuantizationConfig` allows you to set the `compute_dtype`.\n",
    "\n",
    "- The functions used for dynamic dequantizatation are based on the great work done by `city96`, who created the Pytorch ports of the original numpy implementation by compilade.\n",
    "\n",
    "- Supported Quantization Types\n",
    "  - `BF16`, `Q4_0`, `Q4_1`, `Q5_0`, `Q5_1`, `Q8_0`, `Q2_K`, `Q3_K`, `Q4_K`, `Q5_K`, `Q6_K`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161adba-a296-4663-9483-15817e59e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = (\n",
    "    \"https://huggingface.co/city96/FLUX.1-dev-gguf/blob/main/flux1-dev-Q2_K.gguf\"\n",
    ")\n",
    "transformer = FluxTransformer2DModel.from_single_file(\n",
    "    ckpt_path,\n",
    "    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    transformer=transformer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "prompt = \"A cat holding a sign that says hello world\"\n",
    "image = pipe(prompt, generator=torch.manual_seed(0)).images[0]\n",
    "image.save(\"flux-gguf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be7f04-3645-4b25-985c-66d4ab461d67",
   "metadata": {},
   "source": [
    "-----\n",
    "### **torchao**\n",
    "- `TorchAO` is an architecture optimization library for PyTorch.\n",
    "  - It provides high-performance dtypes, optimization techniques, and kernels for inference and training, featuring composability with native PyTorch features like torch.compile, FullyShardedDataParallel (FSDP), and more.\n",
    "\n",
    "- Quantize a model by passing `TorchAoConfig` to `from_pretrained()` (you can also load pre-quantized models).\n",
    "  - This works for any model in any modality, as long as it supports loading with `Accelerate` and contains `torch.nn.Linear` layers.\n",
    "\n",
    "- The example below only quantizes the weights to `int8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f498b25-278d-4f52-8abf-45be3319cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "quantization_config = TorchAoConfig(\"int8wo\")\n",
    "transformer = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    transformer=transformer,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# Without quantization: ~31.447 GB\n",
    "# With quantization: ~20.40 GB\n",
    "print(f\"Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB\")\n",
    "\n",
    "prompt = \"A cat holding a sign that says hello world\"\n",
    "image = pipe(\n",
    "    prompt, num_inference_steps=50, guidance_scale=4.5, max_sequence_length=512\n",
    ").images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f9df6-d4c0-4b40-96f6-3dfbe4443705",
   "metadata": {},
   "source": [
    "- `TorchAO` is fully compatible with torch.compile, setting it apart from other quantization methods.\n",
    "  - This makes it easy to speed up inference with just one line of code.\n",
    " \n",
    "- `torchao` also supports an automatic quantization API through autoquant.\n",
    "  - Autoquantization determines the best quantization strategy applicable to a model by comparing the performance of each technique on chosen input types and shapes.\n",
    "  - Currently, this can be used directly on the underlying modeling components.\n",
    "  - Diffusers will also expose an autoquant configuration option in the future.\n",
    "\n",
    "- The `TorchAoConfig` class accepts three parameters:\n",
    "  - `quant_type`: A string value mentioning one of the quantization types below.\n",
    "  - `modules_to_not_convert`: A list of module full/partial module names for which quantization should not be performed.\n",
    "    - For example, to not perform any quantization of the `FluxTransformer2DModel`’s first block.\n",
    "    - one would specify: `modules_to_not_convert=[\"single_transformer_blocks.0\"]`.\n",
    "  - `kwargs`: A dict of keyword arguments to pass to the underlying quantization method which will be invoked based on `quant_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55558c0-a837-4926-b4f1-d2d479e610a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above code, add the following after initializing the transformer\n",
    "transformer = torch.compile(transformer, mode=\"max-autotune\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e911dbf-38b4-4040-92e6-44e585e33166",
   "metadata": {},
   "source": [
    "#### Supported quantization types\n",
    "- `torchao` supports weight-only quantization and weight and dynamic-activation quantization for `int8`, `float3-float8`, and `uint1-uint7`.\n",
    "  - Weight-only quantization stores the model weights in a specific low-bit data type but performs computation with a higher-precision data type, like `bfloat16`.\n",
    "  - This lowers the memory requirements from model weights but retains the memory peaks for activation computation.\n",
    "\n",
    "- Dynamic activation quantization stores the model weights in a low-bit dtype, while also quantizing the activations on-the-fly to save additional memory.\n",
    "  - This lowers the memory requirements from model weights, while also lowering the memory overhead from activation computations.\n",
    "  - This may come at a quality tradeoff at times, so it is recommended to test different models thoroughly.\n",
    "\n",
    "- Some quantization methods are aliases (for example, `int8wo` is the commonly used shorthand for `int8_weight_only`).\n",
    "  - This allows using the quantization methods described in the torchao docs as-is, while also making it convenient to remember their shorthand notations.\n",
    "\n",
    "- Refer to the official torchao documentation for a better understanding of the available quantization methods and the exhaustive list of configuration options available.\n",
    "\n",
    "\n",
    "#### Serializing and Deserializing quantized models\n",
    "- To serialize a quantized model in a given dtype, first load the model with the desired quantization dtype and then save it using the `save_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cf6c5-4df6-4e4f-9626-e61d230c1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = TorchAoConfig(\"int8wo\")\n",
    "transformer = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/Flux.1-Dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "transformer.save_pretrained(\"/path/to/flux_int8wo\", safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade6326-c416-480c-adfa-8647fab0b58b",
   "metadata": {},
   "source": [
    "- To load a serialized quantized model, use the `from_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e21b18-5c48-4088-8365-893087b1560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = AutoModel.from_pretrained(\"/path/to/flux_int8wo\", torch_dtype=torch.bfloat16, use_safetensors=False)\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/Flux.1-Dev\", transformer=transformer, torch_dtype=torch.bfloat16)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"A cat holding a sign that says hello world\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.0).images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c71bc-d667-4481-a6db-c5ae53588d7f",
   "metadata": {},
   "source": [
    "- If you are using `torch<=2.6.0`, some quantization methods, such as `uint4wo`, cannot be loaded directly and may result in an `UnpicklingError` when trying to load the models, but work as expected when saving them.\n",
    "  - In order to work around this, one can load the state dict manually into the model.\n",
    "  - This requires using `weights_only=False` in `torch.load`, so it should be run only if the weights were obtained from a trustable source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b14280-9200-45a4-9ff3-5900802b5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the model\n",
    "transformer = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/Flux.1-Dev\",\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=TorchAoConfig(\"uint4wo\"),\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "transformer.save_pretrained(\"/path/to/flux_uint4wo\", safe_serialization=False, max_shard_size=\"50GB\")\n",
    "# ...\n",
    "\n",
    "# Load the model\n",
    "state_dict = torch.load(\"/path/to/flux_uint4wo/diffusion_pytorch_model.bin\", weights_only=False, map_location=\"cpu\")\n",
    "with init_empty_weights():\n",
    "    transformer = AutoModel.from_config(\"/path/to/flux_uint4wo/config.json\")\n",
    "transformer.load_state_dict(state_dict, strict=True, assign=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca205e42-b3fa-4130-88d3-cf53e2b60560",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Quanto**\n",
    "- **Quanto** is a PyTorch quantization backend for Optimum. It has been designed with versatility and simplicity in mind:\n",
    "  - All features are available in eager mode (works with non-traceable models)\n",
    "  - Supports quantization aware training\n",
    "  - Quantized models are compatible with torch.compile\n",
    "  - Quantized models are Device agnostic (e.g CUDA,XPU,MPS,CPU)\n",
    "\n",
    "- Quantize a model by passing the `QuantoConfig` object to the `from_pretrained()` method.\n",
    "  - Although the `Quanto` library does allow quantizing `nn.Conv2d` and `nn.LayerNorm` modules\n",
    "  - Diffusers only supports quantizing the weights in the `nn.Linear` layers of a model.\n",
    "  - The following snippet demonstrates how to apply `float8` quantization with `Quanto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfbf34-639b-445c-82d7-ba656516782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "quantization_config = QuantoConfig(weights_dtype=\"float8\")\n",
    "transformer = FluxTransformer2DModel.from_pretrained(\n",
    "      model_id,\n",
    "      subfolder=\"transformer\",\n",
    "      quantization_config=quantization_config,\n",
    "      torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=torch_dtype)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"A cat holding a sign that says hello world\"\n",
    "image = pipe(\n",
    "    prompt, num_inference_steps=50, guidance_scale=4.5, max_sequence_length=512\n",
    ").images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1734b-b332-4636-99cd-22de49c9a2d9",
   "metadata": {},
   "source": [
    "#### Skipping Quantization on specific modules\n",
    "- It is possible to skip applying quantization on certain modules using the `modules_to_not_convert` argument in the `QuantoConfig`.   - Ensure that the modules passed in to this argument match the keys of the modules in the `state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074939a-b32a-4564-91f8-9683013559d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "quantization_config = QuantoConfig(weights_dtype=\"float8\", modules_to_not_convert=[\"proj_out\"])\n",
    "transformer = FluxTransformer2DModel.from_pretrained(\n",
    "      model_id,\n",
    "      subfolder=\"transformer\",\n",
    "      quantization_config=quantization_config,\n",
    "      torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbd3d9-9322-41d0-ad86-6eaf0850f92b",
   "metadata": {},
   "source": [
    "#### Using from_single_file with the Quanto Backend\n",
    "- `QuantoConfig` is compatible with `~FromOriginalModelMixin.from_single_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c84874-32fe-4765-aef7-eae51fe228e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/flux1-dev.safetensors\"\n",
    "quantization_config = QuantoConfig(weights_dtype=\"float8\")\n",
    "transformer = FluxTransformer2DModel.from_single_file(ckpt_path, quantization_config=quantization_config, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54625c-0469-4f44-b1b6-12e20036be8c",
   "metadata": {},
   "source": [
    "#### Saving Quantized models\n",
    "- Diffusers supports serializing Quanto models using the `~ModelMixin.save_pretrained` method.\n",
    "  - The serialization and loading requirements are different for models quantized directly with the `Quanto` library and models quantized with Diffusers using `Quanto` as the backend.\n",
    "  - It is currently not possible to load models quantized directly with Quanto into Diffusers using `~ModelMixin.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf6a2f-52af-4855-9e2e-71569e74c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "quantization_config = QuantoConfig(weights_dtype=\"float8\")\n",
    "transformer = FluxTransformer2DModel.from_pretrained(\n",
    "      model_id,\n",
    "      subfolder=\"transformer\",\n",
    "      quantization_config=quantization_config,\n",
    "      torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# save quantized model to reuse\n",
    "transformer.save_pretrained(\"<your quantized model save path>\")\n",
    "\n",
    "# you can reload your quantized model with\n",
    "model = FluxTransformer2DModel.from_pretrained(\"<your quantized model save path>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e75a6-f70e-41bc-92a5-c5fb2fc9071d",
   "metadata": {},
   "source": [
    "#### Using torch.compile with Quanto\n",
    "- Currently the `Quanto` backend supports `torch.compile` for the following quantization types:\n",
    "- `int8` weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c898f-cf2a-49ad-8590-b8c4b0d11356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "quantization_config = QuantoConfig(weights_dtype=\"int8\")\n",
    "transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "transformer = torch.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    model_id, transformer=transformer, torch_dtype=torch_dtype\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "images = pipe(\"A cat holding a sign that says hello\").images[0]\n",
    "images.save(\"flux-quanto-compile.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-t2i",
   "language": "python",
   "name": "diffusers-t2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
