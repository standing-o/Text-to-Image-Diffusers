{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756273de-4907-4c9f-b92f-db51de658e7b",
   "metadata": {},
   "source": [
    "## **[Optional] Inpainting Pipelines**\n",
    "\n",
    "> Original Source: https://huggingface.co/docs/diffusers/v0.33.1/en/using-diffusers/sdxl_turbo\n",
    "\n",
    "```\n",
    "> Stable Diffusion XL\n",
    "> Kandinsky\n",
    "> IP-Adapter\n",
    "> Perturbed-Attention Guidance(PAG)\n",
    "> ControlNet\n",
    "> Latent Consistency Model(LCM)\n",
    "> Trajectory Consistency Distillation-LoRA\n",
    "```\n",
    "\n",
    "\n",
    "- Stable Diffusion **inpainting** is a generative process that fills in missing or masked parts of an image using deep learning.\n",
    "  - Unlike traditional inpainting, it operates in a latent space, where images are first compressed into a lower-dimensional representation.\n",
    "- The user provides three main inputs: the original image, a binary mask indicating the region to modify, and an optional text prompt to guide the generation.\n",
    "  - During the inpainting process, the model keeps the unmasked (black) regions fixed and only generates new content for the masked (white) areas.\n",
    "  - It introduces noise into the masked region and then iteratively denoises it, guided by the surrounding context and the text prompt.\n",
    "    - Because this is done in latent space, the model can generate high-quality, semantically meaningful results more efficiently.\n",
    "\n",
    "- By leveraging powerful text-to-image capabilities, users can not only restore missing parts but also transform the image creatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b043c0-f515-406d-9457-15d223fe3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "from diffusers.utils import load_image, make_image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b018a22-9ae3-4adb-bf59-f04efa07304b",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Stable Diffusion XL**\n",
    "- `Stable Diffusion XL (SDXL)` is a powerful text-to-image generation model that iterates on the previous Stable Diffusion models in three key ways:\n",
    "  - The `UNet` is 3x larger and `SDXL` combines a second text encoder (`OpenCLIP ViT-bigG/14`) with the original text encoder to significantly increase the number of parameters\n",
    "  - Introduces size and crop-conditioning to preserve training data from being discarded and gain more control over how a generated image should be cropped\n",
    "  - Introduces a two-stage model process; the base model (can also be run as a standalone model) generates an image as an input to the refiner model which adds additional high-quality details\n",
    "\n",
    "- Install:\n",
    "```\n",
    "pip install -q diffusers transformers accelerate invisible-watermark>=0.2.0\n",
    "```\n",
    "\n",
    "- We recommend installing the `invisible-watermark` library to help identify images that are generated. If the invisible-watermark library is installed, it is used by default. To disable the watermarker:\n",
    "\n",
    "```\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(..., add_watermarker=False)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Model weights may be stored in separate subfolders on the Hub or locally, in which case, you should use the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cb112-1559-4e81-bae1-d39c0faf5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline, AutoPipelineForImage2Image\n",
    "from diffusers import StableDiffusionXLInpaintPipeline, AutoPipelineForInpainting\n",
    "\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db19e3-7b62-4094-8860-bf09413a8b73",
   "metadata": {},
   "source": [
    "- For inpainting, you’ll need the original image and a mask of what you want to replace in the original image.\n",
    "  - Create a prompt to describe what you want to replace the masked area with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b0c30-b270-457a-b716-a5439df491cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953052c-c654-45f8-81d7-c5eb09a3be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use from_pipe to avoid consuming additional memory when loading a checkpoint\n",
    "pipeline = AutoPipelineForInpainting.from_pipe(pipeline_text2image).to(\"cuda\")\n",
    "\n",
    "img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n",
    "mask_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-inpaint-mask.png\"\n",
    "\n",
    "init_image = load_image(img_url)\n",
    "mask_image = load_image(mask_url)\n",
    "\n",
    "prompt = \"A deep sea diver floating\"\n",
    "image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, strength=0.85, guidance_scale=12.5).images[0]\n",
    "make_image_grid([init_image, mask_image, image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82460b95-6eb7-4913-8db9-5a90e0f75fd1",
   "metadata": {},
   "source": [
    "- The refiner model can also be used for inpainting in the `StableDiffusionXLInpaintPipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73028c4b-fa86-48a7-bd13-fd2fd2b9a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "refiner = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "init_image = load_image(img_url)\n",
    "mask_image = load_image(mask_url)\n",
    "\n",
    "prompt = \"A majestic tiger sitting on a bench\"\n",
    "num_inference_steps = 75\n",
    "high_noise_frac = 0.7\n",
    "\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "    image=init_image,\n",
    "    mask_image=mask_image,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    denoising_end=high_noise_frac,\n",
    "    output_type=\"latent\",\n",
    ").images\n",
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    image=image,\n",
    "    mask_image=mask_image,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    denoising_start=high_noise_frac,\n",
    ").images[0]\n",
    "make_image_grid([init_image, mask_image, image.resize((512, 512))], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45c5a3-325e-40ae-b176-ed62c82774f0",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Kandinsky**\n",
    "- The Kandinsky models are a series of multilingual text-to-image generation models.\n",
    "  - The `Kandinsky 2.0` model uses two multilingual text encoders and concatenates those results for the UNet.\n",
    "  - `Kandinsky 2.1` changes the architecture to include an image prior model (CLIP) to generate a mapping between text and image embeddings and uses a `Modulating Quantized Vectors (MoVQ)` decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.\n",
    "  - `Kandinsky 2.2` improves on the previous model by replacing the image encoder of the image prior model with a larger `CLIP-ViT-G` model to improve quality.\n",
    "    - The only difference with `Kandinsky 2.1` is `Kandinsky 2.2` doesn’t accept prompt as an input when decoding the latents. Instead, `Kandinsky 2.2` only accepts `image_embeds` during decoding.\n",
    "  - `Kandinsky 3` simplifies the architecture and shifts away from the two-stage generation process involving the prior model and diffusion model and uses `Flan-UL2` to encode text, a `UNet` with BigGan-deep blocks, and `Sber-MoVQGAN` to decode the latents into images.\n",
    "    - Text understanding and generated image quality are primarily achieved by using a larger text encoder and UNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10e8bf-f6d9-4bf2-a317-9d7cddba9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import KandinskyPriorPipeline, KandinskyPipeline\n",
    "from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\n",
    "from diffusers import Kandinsky3Pipeline\n",
    "\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline\n",
    "from diffusers import KandinskyV22Img2ImgPipeline\n",
    "from diffusers import Kandinsky3Img2ImgPipeline\n",
    "\n",
    "from diffusers import KandinskyInpaintPipeline\n",
    "from diffusers import KandinskyV22InpaintPipeline, KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n",
    "\n",
    "from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\n",
    "\n",
    "from diffusers.models.attention_processor import AttnAddedKVProcessor2_0\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.utils import make_image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c3042-17ca-4daa-a0cf-9f80a163133e",
   "metadata": {},
   "source": [
    "- The Kandinsky models use white pixels to represent the masked area now instead of black pixels.\n",
    "  - If you are using `KandinskyInpaintPipeline` in production, you need to change the mask to use white pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223a3a0-96b5-41b6-9f30-891186025f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PIL input\n",
    "import PIL.ImageOps\n",
    "mask = PIL.ImageOps.invert(mask)\n",
    "\n",
    "# For PyTorch and NumPy input\n",
    "mask = 1 - mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c70b2-9621-4a3b-9e28-69632ea13d30",
   "metadata": {},
   "source": [
    "- For inpainting, you’ll need the original image, a mask of the area to replace in the original image, and a text prompt of what to inpaint. Load the prior pipeline.\n",
    "- Load an initial image and create a mask.\n",
    "  - Generate the embeddings with the prior pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c87a5-9b3f-4bb3-a3fb-ad8bd1482926",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
    "pipeline = KandinskyV22InpaintPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n",
    "mask = np.zeros((768, 768), dtype=np.float32)\n",
    "# mask area above cat's head\n",
    "mask[:250, 250:-250] = 1\n",
    "\n",
    "prompt = \"a hat\"\n",
    "prior_output = prior_pipeline(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53122d96-c086-4097-bcd5-71694a5830e4",
   "metadata": {},
   "source": [
    "- Pass the initial image, mask, and prompt and embeddings to the pipeline to generate an image:\n",
    "  - You can also use the end-to-end `KandinskyInpaintCombinedPipeline` and `KandinskyV22InpaintCombinedPipeline` to call the prior and decoder pipelines together under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab88380-1180-43ce-a803-3d28e575a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = pipeline(image=init_image, mask_image=mask, **prior_output, height=768, width=768, num_inference_steps=150).images[0]\n",
    "mask = Image.fromarray((mask*255).astype('uint8'), 'L')\n",
    "make_image_grid([init_image, mask, output_image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d743c1-0c9d-495e-a59d-bffacf5a167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForInpainting.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n",
    "mask = np.zeros((768, 768), dtype=np.float32)\n",
    "# mask area above cat's head\n",
    "mask[:250, 250:-250] = 1\n",
    "prompt = \"a hat\"\n",
    "\n",
    "output_image = pipe(prompt=prompt, image=original_image, mask_image=mask).images[0]\n",
    "mask = Image.fromarray((mask*255).astype('uint8'), 'L')\n",
    "make_image_grid([init_image, mask, output_image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ee381-a1a2-4036-88d7-6e11565daf44",
   "metadata": {},
   "source": [
    "-----\n",
    "### **IP-Adapter**\n",
    "- IP-Adapter is an image prompt adapter that can be plugged into diffusion models to enable image prompting without any changes to the underlying model.\n",
    "  - This adapter can be reused with other models finetuned from the same base model and it can be combined with other adapters like ControlNet.\n",
    "  - The key idea behind `IP-Adapter` is the decoupled cross-attention mechanism which adds a separate cross-attention layer just for image features instead of using the same cross-attention layer for both text and image features.\n",
    "    - This allows the model to learn more image-specific features.\n",
    "   \n",
    "<br>\n",
    "\n",
    "- `set_ip_adapter_scale()` method controls the amount of text or image conditioning to apply to the model.\n",
    "  - A value of `1.0` means the model is only conditioned on the image prompt.\n",
    "  - Lowering this value encourages the model to produce more diverse images, but they may not be as aligned with the image prompt.\n",
    "  - Typically, a value of 0.5 achieves a good balance between the two prompt types and produces good results.\n",
    "\n",
    "- Try adding `low_cpu_mem_usage=True` to the `load_ip_adapter()` method to speed up the loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e6e48-cb20-46f7-8a7a-744f5e8a8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from diffusers.image_processor import IPAdapterMaskProcessor\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoPipelineForText2Image\n",
    "from diffusers import DiffusionPipeline, LCMScheduler\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76e209-fb02-42cd-a0f1-d86fb4649477",
   "metadata": {},
   "source": [
    "- `IP-Adapter` is also useful for inpainting because the image prompt allows you to be much more specific about what you’d like to generate.\n",
    "  - Pass a prompt, the original image, mask image, and the IP-Adapter image prompt to the pipeline to generate an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a49ff9-978c-4f1d-872d-495a247163e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForInpainting.from_pretrained(\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)\n",
    "\n",
    "mask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_mask.png\")\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_bear_1.png\")\n",
    "ip_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_gummy.png\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(4)\n",
    "images = pipeline(\n",
    "    prompt=\"a cute gummy bear waving\",\n",
    "    image=image,\n",
    "    mask_image=mask_image,\n",
    "    ip_adapter_image=ip_image,\n",
    "    generator=generator,\n",
    "    num_inference_steps=100,\n",
    ").images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8473c46-c57b-4147-920a-04d1d5afeab9",
   "metadata": {},
   "source": [
    "----\n",
    "### **Perturbed-Attention Guidance(PAG)**\n",
    "- `Perturbed-Attention Guidance(PAG)` is a new diffusion sampling guidance that improves sample quality across both unconditional and conditional settings, achieving this without requiring further training or the integration of external modules.\n",
    "  - `PAG` is designed to progressively enhance the structure of synthesized samples throughout the denoising process by considering the self-attention mechanisms’ ability to capture structural information.\n",
    "  - It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, and guiding the denoising process away from these degraded samples.\n",
    "\n",
    "<br>\n",
    "\n",
    "- You can apply PAG to the `StableDiffusionXLPipeline` for tasks such as text-to-image, image-to-image, and inpainting.\n",
    "- To enable PAG for a specific task, load the pipeline using the AutoPipeline API with the `enable_pag=True` flag and the `pag_applied_layers` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3495a0-c4a0-4605-abde-28906338a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image, ControlNetModel\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2188b-8010-41a2-968b-70cdee1a5299",
   "metadata": {},
   "source": [
    "- You can enable `PAG` on an exisiting inpainting pipeline like this\n",
    "  - This still works when your pipeline has a different task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a7dd8-d47d-4c76-b04b-96436936e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    enable_pag=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5a44c-4534-4f29-a6b2-d7fc6cd15330",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_inpaint = AutoPipelineForInpaiting.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\n",
    "pipeline = AutoPipelineForInpaiting.from_pipe(pipeline_inpaint, enable_pag=True)\n",
    "\n",
    "pipeline_t2i = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\n",
    "pipeline = AutoPipelineForInpaiting.from_pipe(pipeline_t2i, enable_pag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea31d3-1495-4c75-8b4b-9e49516487ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "init_image = load_image(img_url).convert(\"RGB\")\n",
    "mask_image = load_image(mask_url).convert(\"RGB\")\n",
    "\n",
    "prompt = \"A majestic tiger sitting on a bench\"\n",
    "\n",
    "pag_scales =  3.0\n",
    "guidance_scales = 7.5\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(1)\n",
    "images = pipeline(\n",
    "    prompt=prompt,\n",
    "    image=init_image,\n",
    "    mask_image=mask_image,\n",
    "    strength=0.8,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=generator,\n",
    "    pag_scale=pag_scale,\n",
    ").images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d632740-fcd1-48db-a55b-20ac99e5a05a",
   "metadata": {},
   "source": [
    "----\n",
    "### **ControlNet**\n",
    "- `ControlNet` is a type of model for controlling image diffusion models by conditioning the model with an additional input image.\n",
    "  - There are many types of conditioning inputs (canny edge, user sketching, human pose, depth, and more) you can use to control a diffusion model.\n",
    "  - This is hugely useful because it affords you greater control over image generation, making it easier to generate specific images without experimenting with different text prompts or denoising values as much.\n",
    "\n",
    "- A `ControlNet` model has two sets of weights (or blocks) connected by a zero-convolution layer:\n",
    "  - a locked copy keeps everything a large pretrained diffusion model has learned\n",
    "  - a trainable copy is trained on the additional conditioning input\n",
    "\n",
    "<br>\n",
    "\n",
    "- Since the locked copy preserves the pretrained model, training and implementing a `ControlNet` on a new conditioning input is as fast as finetuning any other model because you aren’t training the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894941f4-1c50-4b4c-ba6f-d5b2eabe26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers import StableDiffusionControlNetImg2ImgPipeline\n",
    "from diffusers import StableDiffusionControlNetInpaintPipeline\n",
    "\n",
    "from diffusers import StableDiffusionXLControlNetPipeline, AutoencoderKL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd57656-3d16-4346-b543-a6082d73172d",
   "metadata": {},
   "source": [
    "- For inpainting, you need an initial image, a mask image, and a prompt describing what to replace the mask with.\n",
    "  - `ControlNet` models allow you to add another control image to condition a model with.\n",
    "  - `ControlNet` can use the inpainting mask as a control to guide the model to generate an image within the mask area.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Load an initial image and a mask image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3769585-9ddd-42bb-91b4-251da83a7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg\"\n",
    ")\n",
    "init_image = init_image.resize((512, 512))\n",
    "\n",
    "mask_image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg\"\n",
    ")\n",
    "mask_image = mask_image.resize((512, 512))\n",
    "make_image_grid([init_image, mask_image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c50c3-1e49-48b0-9527-356856b60756",
   "metadata": {},
   "source": [
    "- Create a function to prepare the control image from the initial and mask images.\n",
    "  - Create a tensor to mark the pixels in `init_image` as masked if the corresponding pixel in `mask_image` is over a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964aa0df-68f6-4a82-9e6e-1459fc7303eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inpaint_condition(image, image_mask):\n",
    "    image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "    image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n",
    "\n",
    "    assert image.shape[0:1] == image_mask.shape[0:1]\n",
    "    image[image_mask > 0.5] = -1.0  # set as masked pixel\n",
    "    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "control_image = make_inpaint_condition(init_image, mask_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee809596-1302-4b4e-bf85-871aa98ee3bd",
   "metadata": {},
   "source": [
    "- Load a `ControlNet` model conditioned on inpainting and pass it to the `StableDiffusionControlNetInpaintPipeline`.\n",
    "  - Use the faster `UniPCMultistepScheduler` and enable model offloading to speed up inference and reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd24590a-d015-4ec1-b7b1-5ab67f5d70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23be4c-0fd9-4748-bb08-ce7bf0fc2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(\n",
    "    \"corgi face with large ears, detailed, pixar, animated, disney\",\n",
    "    num_inference_steps=20,\n",
    "    eta=1.0,\n",
    "    image=init_image,\n",
    "    mask_image=mask_image,\n",
    "    control_image=control_image,\n",
    ").images[0]\n",
    "make_image_grid([init_image, mask_image, output], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd397fb6-8c45-4162-b966-8496deb19e72",
   "metadata": {},
   "source": [
    "----\n",
    "### **Latent Consistency Model(LCMs)**\n",
    "- `Latent Consistency Models (LCMs)` enable fast high-quality image generation by directly predicting the reverse diffusion process in the latent rather than pixel space.\n",
    "  - `LCMs` try to predict the noiseless image from the noisy image in contrast to typical diffusion models that iteratively remove noise from the noisy image.\n",
    "  - By avoiding the iterative sampling process, `LCMs` are able to generate high-quality images in 2-4 steps instead of 20-30 steps.\n",
    "\n",
    "- `LCMs` are distilled from pretrained models which requires ~32 hours of A100 compute.\n",
    "  - To speed this up, `LCM-LoRAs` train a `LoRA` adapter which have much fewer parameters to train compared to the full model.\n",
    "  - The `LCM-LoRA` can be plugged into a diffusion model once it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d40d17-10e0-41b9-af2f-85fad75003f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler, AutoPipelineForInpainting\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace625ab-ae67-4437-bd96-ab57bae35465",
   "metadata": {},
   "source": [
    "- To use `LCM-LoRAs` for inpainting, you need to replace the scheduler with the `LCMScheduler` and load the `LCM-LoRA` weights with the `load_lora_weights()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ae2e6-eb4a-4973-a698-1005eccd612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\n",
    "mask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n",
    "\n",
    "prompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    image=init_image,\n",
    "    mask_image=mask_image,\n",
    "    generator=generator,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=4,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46422a13-6d5b-4269-bb8d-023e2dcfa78f",
   "metadata": {},
   "source": [
    "----\n",
    "### **Trajectory Consistency Distillation-LoRA**\n",
    "- `Trajectory Consistency Distillation (TCD)` enables a model to generate higher quality and more detailed images with fewer steps.\n",
    "- Owing to the effective error mitigation during the distillation process, `TCD` demonstrates superior performance even under conditions of large inference steps.\n",
    "\n",
    "- The major advantages of TCD are:\n",
    "  - Better than Teacher: TCD demonstrates superior generative quality at both small and large inference steps and exceeds the performance of `DPM-Solver++(2S)` with `Stable Diffusion XL (SDXL)`.\n",
    "\n",
    "- For large models like `SDXL`, `TCD` is trained with `LoRA` to reduce memory usage.\n",
    "  - This is also useful because you can reuse LoRAs between different finetuned models, as long as they share the same base model, without further training.\n",
    " \n",
    "<br>\n",
    "\n",
    "#### General tasks\n",
    "- Let’s use the `StableDiffusionXLPipeline` and the `TCDScheduler`.\n",
    "  - Use the `load_lora_weights()` method to load the `SDXL-compatible TCD-LoRA` weights.\n",
    "\n",
    "- A few tips to keep in mind for TCD-LoRA inference are to:\n",
    "  - Keep the `num_inference_steps` between 4 and 50\n",
    "  - Set `eta` (used to control stochasticity at each step) between 0 and 1.\n",
    "  - You should use a higher eta when increasing the number of inference steps, but the downside is that a larger eta in `TCDScheduler` leads to blurrier images.\n",
    "  - A value of `0.3` is recommended to produce good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d918e-0e5d-4c6a-9b03-4519135ff1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForInpainting, TCDScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0480425-4d4b-4629-8984-f50c12588f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "base_model_id = \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "pipe = AutoPipelineForInpainting.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "init_image = load_image(img_url).resize((1024, 1024))\n",
    "mask_image = load_image(mask_url).resize((1024, 1024))\n",
    "\n",
    "prompt = \"a tiger sitting on a park bench\"\n",
    "\n",
    "image = pipe(\n",
    "  prompt=prompt,\n",
    "  image=init_image,\n",
    "  mask_image=mask_image,\n",
    "  num_inference_steps=8,\n",
    "  guidance_scale=0,\n",
    "  eta=0.3,\n",
    "  strength=0.99,  # make sure to use `strength` below 1.0\n",
    "  generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]\n",
    "\n",
    "grid_image = make_image_grid([init_image, mask_image, image], rows=1, cols=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-t2i",
   "language": "python",
   "name": "diffusers-t2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
