{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f585930d-3722-4151-bf22-7f153cb5db61",
   "metadata": {},
   "source": [
    "## **9. Accelerate Inference and Reduce Memory**\n",
    "\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/optimization/fp16\n",
    "\n",
    "```\n",
    "> Accelerate inference\n",
    "> Reduce memory usage\n",
    "> Diffusers supports: PyTorch 2.0, xFormers, Token merging, DeepCache, TGATE, xDiT, ParaAttention\n",
    "> Optimized Model Format: JAX/Flax, ONNX, OpenVINO, CoreML\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d759dc-6389-4d0e-8d64-f38198339a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "import torch\n",
    "import tomesd\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    " from diffusers import StableDiffusion3Pipeline\n",
    "from diffusers import UNet2DConditionModel, LCMScheduler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from torchao import apply_dynamic_quant\n",
    "\n",
    "from diffusers import AutoModel\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "from diffusers.hooks import apply_group_offloading\n",
    "from diffusers import CogVideoXPipeline, CogVideoXTransformer3DModel\n",
    "from diffusers.utils import export_to_video\n",
    "from diffusers.hooks import apply_layerwise_casting\n",
    "\n",
    "from DeepCache import DeepCacheSDHelper\n",
    "from diffusers import PixArtAlphaPipeline\n",
    "\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "from tgate import TgatePixArtLoader\n",
    "from tgate import TgateSDXLLoader\n",
    "from tgate import TgateSDXLDeepCacheLoader\n",
    "\n",
    " from xfuser import xFuserArgs, xDiTParallel\n",
    " from xfuser.config import FlexibleArgumentParser\n",
    " from xfuser.core.distributed import get_world_group\n",
    "\n",
    "from diffusers import FluxPipeline\n",
    "import torch.distributed as dist\n",
    "\n",
    "import jax\n",
    "import jax.tools.colab_tpu\n",
    "\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "from optimum.onnxruntime import ORTStableDiffusionPipeline\n",
    "from optimum.onnxruntime import ORTStableDiffusionXLPipeline\n",
    "from optimum.intel import OVStableDiffusionPipeline\n",
    "from optimum.intel import OVStableDiffusionXLPipeline\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa9641-99fc-42ee-95f7-bc35a930b75c",
   "metadata": {},
   "source": [
    "----\n",
    "### **Accelerate inference**\n",
    "- Diffusion models are slow at inference because generation is an iterative process where noise is gradually refined into an image or video over a certain number of “steps”.\n",
    "  - To speedup this process, you can try experimenting with different schedulers, reduce the precision of the model weights for faster computations, use more memory-efficient attention mechanisms, and more.\n",
    "  - Combine and use these techniques together to make inference faster than using any single technique on its own.\n",
    "\n",
    "#### Model data type\n",
    "- The precision and data type of the model weights affect inference speed because a higher precision requires more memory to load and more time to perform the computations.\n",
    "  - PyTorch loads model weights in float32 or full precision by default, so changing the data type is a simple way to quickly get faster inference.\n",
    " \n",
    "- `bfloat16`\n",
    "  - `bfloat16` is similar to `float16` but it is more robust to numerical errors.\n",
    "  - Hardware support for `bfloat16` varies, but most modern GPUs are capable of supporting `bfloat16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46671ac6-c8cd-45dd-8a27-108f84a7168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "pipeline(prompt, num_inference_steps=30).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d282f0-7f98-47eb-99a0-f65ac7b0ac0f",
   "metadata": {},
   "source": [
    "- `float16`\n",
    "  - `float16` is similar to `bfloat16` but may be more prone to numerical errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae8220-706e-42b8-92d8-0ea6e02c9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "pipeline(prompt, num_inference_steps=30).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549c850-f80f-4d8b-9aae-9f8894a98fa1",
   "metadata": {},
   "source": [
    "- `TensorFloat-32 (tf32)`\n",
    "  - `TensorFloat-32 (tf32)` mode is supported on NVIDIA Ampere GPUs and it computes the convolution and matrix multiplication operations in `tf32`.\n",
    "  - Storage and other operations are kept in `float32`. This enables significantly faster computations when combined with `bfloat16` or `float16`.\n",
    "  - PyTorch only enables `tf32` mode for convolutions by default and you’ll need to explicitly enable it for matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe44bc-b139-498f-be1d-2a6d4122a9d8",
   "metadata": {},
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "pipeline(prompt, num_inference_steps=30).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11677a20-4574-49df-bfe1-651b9a8b8938",
   "metadata": {},
   "source": [
    "#### Scaled dot product attention\n",
    "- Scaled dot product attention (SDPA) implements several attention backends, FlashAttention, xFormers, and a native C++ implementation.\n",
    "  - It automatically selects the most optimal backend for your hardware.\n",
    "\n",
    "- SDPA is enabled by default if you’re using `PyTorch >= 2.0` and no additional changes are required to your code.\n",
    "  - You could try experimenting with other attention backends though if you’d like to choose your own.\n",
    "  - The example below uses the `torch.nn.attention.sdpa_kernel` context manager to enable efficient attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27866998-56f3-40b2-bb12-bac970dd1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n",
    "  image = pipeline(prompt, num_inference_steps=30).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f15e0-0115-457d-a6ec-8dc626b64c76",
   "metadata": {},
   "source": [
    "#### `torch.compile`\n",
    "- `torch.compile` accelerates inference by compiling PyTorch code and operations into optimized kernels.\n",
    "  - Diffusers typically compiles the more compute-intensive models like the UNet, transformer, or VAE.\n",
    "  - Enable the following compiler settings for maximum speed (refer to the full list for more options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14a1bc-adc2-41e7-b674-83445bae819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._inductor.config.conv_1x1_as_mm = True\n",
    "torch._inductor.config.coordinate_descent_tuning = True\n",
    "torch._inductor.config.epilogue_fusion = False\n",
    "torch._inductor.config.coordinate_descent_check_all_directions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122ade4-d6af-4627-b37a-bbd1f0fa15bb",
   "metadata": {},
   "source": [
    "- Load and compile the UNet and VAE.\n",
    "- There are several different modes you can choose from, but `\"max-autotune\"` optimizes for the fastest speed by compiling to a CUDA graph.\n",
    "  - CUDA graphs effectively reduces the overhead by launching multiple GPU operations through a single CPU operation.\n",
    "- Changing the memory layout to channels_last also optimizes memory and inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520258b-7bb0-4ffb-82d4-ed49071a4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipeline.unet.to(memory_format=torch.channels_last)\n",
    "pipeline.vae.to(memory_format=torch.channels_last)\n",
    "pipeline.unet = torch.compile(\n",
    "    pipeline.unet, mode=\"max-autotune\", fullgraph=True\n",
    ")\n",
    "pipeline.vae.decode = torch.compile(\n",
    "    pipeline.vae.decode,\n",
    "    mode=\"max-autotune\",\n",
    "    fullgraph=True\n",
    ")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "pipeline(prompt, num_inference_steps=30).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf918b3-0270-479f-bcba-1113e3833365",
   "metadata": {},
   "source": [
    "- Compilation is slow the first time, but once compiled, it is significantly faster.\n",
    "  - Try to only use the compiled pipeline on the same type of inference operations.\n",
    "  - Calling the compiled pipeline on a different image size retriggers compilation which is slow and inefficient.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Graph breaks**\n",
    "  - It is important to specify `fullgraph=True` in `torch.compile` to ensure there are no graph breaks in the underlying model.\n",
    "  - This allows you to take advantage of torch.compile without any performance degradation.\n",
    "  - For the UNet and VAE, this changes how you access the return variables.\n",
    "  ```\n",
    "    - latents = unet(\n",
    "    -   latents, timestep=timestep, encoder_hidden_states=prompt_embeds\n",
    "    -).sample\n",
    "    \n",
    "    + latents = unet(\n",
    "    +   latents, timestep=timestep, encoder_hidden_states=prompt_embeds, return_dict=False\n",
    "    +)[0]\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "- **GPU sync**\n",
    "  - The `step()` function is called on the scheduler each time after the denoiser makes a prediction, and the sigmas variable is indexed.\n",
    "    - When placed on the GPU, it introduces latency because of the communication sync between the CPU and GPU.\n",
    "    - It becomes more evident when the denoiser has already been compiled.\n",
    "  - In general, the sigmas should stay on the CPU to avoid the communication sync and latency.\n",
    "\n",
    "#### Dynamic quantization\n",
    "- Dynamic quantization improves inference speed by reducing precision to enable faster math operations.\n",
    "  - This particular type of quantization determines how to scale the activations based on the data at runtime rather than using a fixed scaling factor.\n",
    "  - As a result, the scaling factor is more accurately aligned with the data.\n",
    "\n",
    "- The example below applies dynamic `int8` quantization to the UNet and VAE with the `torchao` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7241e5-396c-4f4e-81ec-35fc67457eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._inductor.config.conv_1x1_as_mm = True\n",
    "torch._inductor.config.coordinate_descent_tuning = True\n",
    "torch._inductor.config.epilogue_fusion = False\n",
    "torch._inductor.config.coordinate_descent_check_all_directions = True\n",
    "torch._inductor.config.force_fuse_int_mm_with_mul = True\n",
    "torch._inductor.config.use_mixed_mm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6b629-6083-4732-ae22-d0c95a73be96",
   "metadata": {},
   "source": [
    "- Filter out some linear layers in the UNet and VAE which don’t benefit from dynamic quantization with the `dynamic_quant_filter_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae8b49-2393-472f-af4c-99e59d5f0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "\n",
    "apply_dynamic_quant(pipeline.unet, dynamic_quant_filter_fn)\n",
    "apply_dynamic_quant(pipeline.vae, dynamic_quant_filter_fn)\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "pipeline(prompt, num_inference_steps=30).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01950a80-f7db-4902-a715-ba586ec2327c",
   "metadata": {},
   "source": [
    "#### Fused projection matrices\n",
    "- An input is projected into three subspaces, represented by the projection matrices Q, K, and V, in an attention block.\n",
    "  - These projections are typically calculated separately, but you can horizontally combine these into a single matrix and perform the projection in a single step.\n",
    "  - It increases the size of the matrix multiplications of the input projections and also improves the impact of quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76987732-b34f-4245-a129-452895a44d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fuse_qkv_projections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d277a99-5070-41c0-bc99-e994df2a7982",
   "metadata": {},
   "source": [
    "----\n",
    "### **Reduce Memory Usage**\n",
    "- Modern diffusion models like Flux and Wan have billions of parameters that take up a lot of memory on your hardware for inference.   - This is challenging because common GPUs often don’t have sufficient memory.\n",
    "  - To overcome the memory limitations, you can use more than one GPU (if available), offload some of the pipeline components to the CPU, and more.\n",
    "\n",
    "- Keep in mind these techniques may need to be adjusted depending on the model.\n",
    "  - ex. a transformer-based diffusion model may not benefit equally from these inference speed optimizations as a UNet-based model.\n",
    "\n",
    "#### Multiple GPUs\n",
    "- If you have access to more than one GPU, there a few options for efficiently loading and distributing a large model across your hardware.\n",
    "\n",
    "- **Sharded checkpoints**\n",
    "  - Loading large checkpoints in several shards in useful because the shards are loaded one at a time.\n",
    "  - This keeps memory usage low, only requiring enough memory for the model size and the largest shard size.\n",
    "  - We recommend sharding when the `fp32` checkpoint is greater than 5GB. The default shard size is 5GB.\n",
    "  - Shard a checkpoint in `save_pretrained()` with the `max_shard_size parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca70e28-3f3c-4c16-b838-70b98f2e4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = AutoModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"unet\"\n",
    ")\n",
    "unet.save_pretrained(\"sdxl-unet-sharded\", max_shard_size=\"5GB\")\n",
    "\n",
    "unet = AutoModel.from_pretrained(\n",
    "    \"username/sdxl-unet-sharded\", torch_dtype=torch.float16\n",
    ")\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    unet=unet,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d80b20-1e63-4249-a53b-09dce0b310ef",
   "metadata": {},
   "source": [
    "- **Device placement**\n",
    "  - The `device_map` parameter controls how the model components in a pipeline are distributed across devices.\n",
    "  - The balanced device placement strategy evenly splits the pipeline across all available devices.\n",
    "  - You can inspect a pipeline’s device map with `hf_device_map`.\n",
    "  - The `device_map` parameter also works on the model-level.\n",
    "    - This is useful for loading large models, such as the Flux diffusion transformer which has 12.5B parameters.\n",
    "    - Instead of balanced, set it to \"auto\" to automatically distribute a model across the fastest device first before moving to slower devices.\n",
    "  - For more fine-grained control, pass a dictionary to enforce the maximum GPU memory to use on each device.\n",
    "    - If a device is not in `max_memory`, it is ignored and pipeline components won’t be distributed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca02d2b-0f05-457a-a278-4bac3d9ccd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"balanced\"\n",
    ")\n",
    "\n",
    "print(pipeline.hf_device_map)\n",
    "\n",
    "transformer = AutoModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\", \n",
    "    subfolder=\"transformer\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac83dd-da23-4211-9d70-adce0421fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory = {0:\"1GB\", 1:\"1GB\"}\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"balanced\",\n",
    "    max_memory=max_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd2b3f9-f7d5-4838-92da-724f55e89ff6",
   "metadata": {},
   "source": [
    "- Diffusers uses the maxmium memory of all devices by default, but if they don’t fit on the GPUs, then you’ll need to use a single GPU and offload to the CPU with the methods below.\n",
    "  - `enable_model_cpu_offload()` only works on a single GPU but a very large model may not fit on it\n",
    "  - `enable_sequential_cpu_offload()` may work but it is extremely slow and also limited to a single GPU\n",
    "- Use the `reset_device_map()` method to reset the device_map.\n",
    "  - This is necessary if you want to use methods like `.to()`, `enable_sequential_cpu_offload()`, and `enable_model_cpu_offload()` on a pipeline that was device-mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d72f6f-4273-490e-8888-517fc9773bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.reset_device_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1f6d7-e783-4a28-9cef-c4cb4ea19557",
   "metadata": {},
   "source": [
    "#### VAE slicing\n",
    "- VAE slicing saves memory by splitting large batches of inputs into a single batch of data and separately processing them.\n",
    "  - This method works best when generating more than one image at a time.\n",
    "  - ex. if you’re generating 4 images at once, decoding would increase peak activation memory by `4x`.\n",
    "  - VAE slicing reduces this by only decoding 1 image at a time instead of all 4 images at once.\n",
    "\n",
    "- Call `enable_vae_slicing()` to enable sliced VAE.\n",
    "  - You can expect a small increase in performance when decoding multi-image batches and no performance impact for single-image batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0cc28-c0dc-4e8a-b9af-e3e3141c08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "pipeline.enable_vae_slicing()\n",
    "pipeline([\"An astronaut riding a horse on Mars\"]*32).images[0]\n",
    "print(f\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1d138-bcd4-40c9-aaa5-0e541d8423f8",
   "metadata": {},
   "source": [
    "#### VAE tiling\n",
    "- VAE tiling saves memory by dividing an image into smaller overlapping tiles instead of processing the entire image at once.\n",
    "  - This also reduces peak memory usage because the GPU is only processing a tile at a time.\n",
    "\n",
    "- Call `enable_vae_tiling()` to enable VAE tiling.\n",
    "  - The generated image may have some tone variation from tile-to-tile because they’re decoded separately, but there shouldn’t be any obvious seams between the tiles.\n",
    "  - Tiling is disabled for resolutions lower than a pre-specified (but configurable) limit.\n",
    "  - ex. this limit is 512x512 for the VAE in `StableDiffusionPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f461066-5879-4907-b605-a26a24a0fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipeline.enable_vae_tiling()\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl-init.png\")\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "pipeline(prompt, image=init_image, strength=0.5).images[0]\n",
    "print(f\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e00bee-ecc8-4830-921a-263aebcc154a",
   "metadata": {},
   "source": [
    "#### CPU offloading\n",
    "- CPU offloading selectively moves weights from the GPU to the CPU.\n",
    "  - When a component is required, it is transferred to the GPU and when it isn’t required, it is moved to the CPU.\n",
    "  - This method works on submodules rather than whole models.\n",
    "  - It saves memory by avoiding storing the entire model on the GPU.\n",
    "\n",
    "- CPU offloading dramatically reduces memory usage, but it is also extremely slow because submodules are passed back and forth multiple times between devices.\n",
    "  - It can often be impractical due to how slow it is.\n",
    "\n",
    "- Don’t move the pipeline to CUDA before calling `enable_sequential_cpu_offload()`, otherwise the amount of memory saved is only minimal (refer to this issue for more details).\n",
    "  - This is a stateful operation that installs hooks on the model.\n",
    "\n",
    "- Call `enable_sequential_cpu_offload()` to enable it on a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9648711-1e07-43d3-83f9-f792ebda8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipeline.enable_sequential_cpu_offload()\n",
    "\n",
    "pipeline(\n",
    "    prompt=\"An astronaut riding a horse on Mars\",\n",
    "    guidance_scale=0.,\n",
    "    height=768,\n",
    "    width=1360,\n",
    "    num_inference_steps=4,\n",
    "    max_sequence_length=256,\n",
    ").images[0]\n",
    "print(f\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95243c76-386f-4aeb-a219-eb151658989b",
   "metadata": {},
   "source": [
    "#### Model offloading\n",
    "- Model offloading moves entire models to the GPU instead of selectively moving some layers or model components.\n",
    "  - One of the main pipeline models, usually the text encoder, UNet, and VAE, is placed on the GPU while the other components are held on the CPU.\n",
    "  - Components like the UNet that run multiple times stays on the GPU until its completely finished and no longer needed.\n",
    "  - This eliminates the communication overhead of CPU offloading and makes model offloading a faster alternative.\n",
    "  - The tradeoff is memory savings won’t be as large.\n",
    "\n",
    "- If models are reused outside the pipeline after hookes have been installed (see Removing Hooks for more details), you need to run the entire pipeline and models in the expected order to properly offload them.\n",
    "  - This is a stateful operation that installs hooks on the model.\n",
    "\n",
    "- Call `enable_model_cpu_offload()` to enable it on a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b51cb-be80-4b32-834a-c24321c26ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipline.enable_model_cpu_offload()\n",
    "\n",
    "pipeline(\n",
    "    prompt=\"An astronaut riding a horse on Mars\",\n",
    "    guidance_scale=0.,\n",
    "    height=768,\n",
    "    width=1360,\n",
    "    num_inference_steps=4,\n",
    "    max_sequence_length=256,\n",
    ").images[0]\n",
    "print(f\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6eb6c0-b8ca-48fa-96db-523a03d8b050",
   "metadata": {},
   "source": [
    "#### Group offloading\n",
    "- Group offloading moves groups of internal layers (`torch.nn.ModuleList` or `torch.nn.Sequential`) to the CPU.\n",
    "  - It uses less memory than model offloading and it is faster than CPU offloading because it reduces communication overhead.\n",
    "\n",
    "- Group offloading may not work with all models if the forward implementation contains weight-dependent device casting of inputs because it may clash with group offloading’s device casting mechanism.\n",
    "\n",
    "- Call `enable_group_offload()` to enable it for standard Diffusers model components that inherit from `ModelMixin`.\n",
    "  - For other model components that don’t inherit from `ModelMixin`, such as a generic `torch.nn.Module`, use `apply_group_offloading()` instead.\n",
    "  - The offload_type parameter can be set to `block_level` or `leaf_level`.\n",
    "\n",
    "- `block_level` offloads groups of layers based on the `num_blocks_per_group parameter`.\n",
    "  - ex. if `num_blocks_per_group=2` on a model with 40 layers, 2 layers are onloaded and offloaded at a time (20 total onloads/offloads).\n",
    "  - This drastically reduces memory requirements.\n",
    "  - `leaf_level` offloads individual layers at the lowest level and is equivalent to CPU offloading.\n",
    "    - But it can be made faster if you use streams without giving up inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b2fb3-b98e-4ddd-be16-28b87f8ed126",
   "metadata": {},
   "outputs": [],
   "source": [
    "onload_device = torch.device(\"cuda\")\n",
    "offload_device = torch.device(\"cpu\")\n",
    "pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-5b\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Use the enable_group_offload method for Diffusers model implementations\n",
    "pipeline.transformer.enable_group_offload(onload_device=onload_device, offload_device=offload_device, offload_type=\"leaf_level\")\n",
    "pipeline.vae.enable_group_offload(onload_device=onload_device, offload_type=\"leaf_level\")\n",
    "\n",
    "# Use the apply_group_offloading method for other model components\n",
    "apply_group_offloading(pipeline.text_encoder, onload_device=onload_device, offload_type=\"block_level\", num_blocks_per_group=2)\n",
    "\n",
    "prompt = (\n",
    "    \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. \"\n",
    "    \"The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other \"\n",
    "    \"pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, \"\n",
    "    \"casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. \"\n",
    "    \"The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical \"\n",
    "    \"atmosphere of this unique musical performance.\"\n",
    ")\n",
    "video = pipeline(prompt=prompt, guidance_scale=6, num_inference_steps=50).frames[0]\n",
    "print(f\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "export_to_video(video, \"output.mp4\", fps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b8580-a81b-4f17-9f12-60d881e3fa19",
   "metadata": {},
   "source": [
    "#### CUDA stream\n",
    "- The `use_stream` parameter can be activated for CUDA devices that support asynchronous data transfer streams to reduce overall execution time compared to CPU offloading.\n",
    "  - It overlaps data transfer and computation by using layer prefetching.\n",
    "  - The next layer to be executed is loaded onto the GPU while the current layer is still being executed.\n",
    "  - It can increase CPU memory significantly so ensure you have 2x the amount of memory as the model size.\n",
    "\n",
    "- Set `record_stream=True` for more of a speedup at the cost of slightly increased memory usage.\n",
    "  - Refer to the `torch.Tensor.record_stream` docs to learn more.\n",
    "\n",
    "- When `use_stream=True` on VAEs with tiling enabled, make sure to do a dummy forward pass (possible with dummy inputs as well) before inference to avoid device mismatch errors.\n",
    "  - This may not work on all implementations, so feel free to open an issue if you encounter any problems.\n",
    "\n",
    "- If you’re using `block_level` group offloading with use_stream enabled, the `num_blocks_per_group` parameter should be set to 1, otherwise a warning will be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616205d-2a36-46b1-b21d-12cef4f36aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transformer.enable_group_offload(onload_device=onload_device, offload_device=offload_device, offload_type=\"leaf_level\", use_stream=True, record_stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b4507-2cb4-4467-a9a1-2c574d693c46",
   "metadata": {},
   "source": [
    "- The `low_cpu_mem_usage` parameter can be set to `True` to reduce CPU memory usage when using streams during group offloading.\n",
    "  - It is best for `leaf_level` offloading and when CPU memory is bottlenecked.\n",
    "  - Memory is saved by creating pinned tensors on the fly instead of pre-pinning them.\n",
    "  - However, this may increase overall execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7814e3-9473-4a69-ac27-129f58fd5966",
   "metadata": {},
   "source": [
    "#### Layerwise casting\n",
    "- Layerwise casting stores weights in a smaller data format (for example, `torch.float8_e4m3fn` and `torch.float8_e5m2`) to use less memory and upcasts those weights to a higher precision like `torch.float16` or `torch.bfloat16` for computation.\n",
    "  - Certain layers (normalization and modulation related weights) are skipped because storing them in fp8 can degrade generation quality.\n",
    "  - Call `enable_layerwise_casting()` to set the storage and computation datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d747c-6bfa-41f7-bf8d-a755c7bb558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = CogVideoXTransformer3DModel.from_pretrained(\n",
    "    \"THUDM/CogVideoX-5b\",\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.bfloat16)\n",
    "\n",
    "pipeline = CogVideoXPipeline.from_pretrained(\"THUDM/CogVideoX-5b\",\n",
    "    transformer=transformer,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "prompt = (\n",
    "    \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. \"\n",
    "    \"The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other \"\n",
    "    \"pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, \"\n",
    "    \"casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. \"\n",
    "    \"The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical \"\n",
    "    \"atmosphere of this unique musical performance.\"\n",
    ")\n",
    "video = pipeline(prompt=prompt, guidance_scale=6, num_inference_steps=50).frames[0]\n",
    "print(f\"Max memory reserved: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "export_to_video(video, \"output.mp4\", fps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd1b97-de4a-4035-8a96-967bc90f67fd",
   "metadata": {},
   "source": [
    "- The `apply_layerwise_casting()` method can also be used if you need more control and flexibility.\n",
    "  - It can be partially applied to model layers by calling it on specific internal modules.\n",
    "  - Use the `skip_modules_pattern` or `skip_modules_classes` parameters to specify modules to avoid, such as the normalization and modulation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f4664-6436-41f8-8fbc-6678d81cef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = CogVideoXTransformer3DModel.from_pretrained(\n",
    "    \"THUDM/CogVideoX-5b\",\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# skip the normalization layer\n",
    "apply_layerwise_casting(\n",
    "    transformer,\n",
    "    storage_dtype=torch.float8_e4m3fn,\n",
    "    compute_dtype=torch.bfloat16,\n",
    "    skip_modules_classes=[\"norm\"],\n",
    "    non_blocking=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70d1b3-d6aa-4649-8a1c-3fa5375f63bc",
   "metadata": {},
   "source": [
    "#### `torch.channels_last`\n",
    "- `torch.channels_last` flips how tensors are stored from (batch size, channels, height, width) to (batch size, heigh, width, channels).\n",
    "  - This aligns the tensors with how the hardware sequentially accesses the tensors stored in memory and avoids skipping around in memory to access the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f1a23-4822-4ed9-8eeb-df12386654af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline.unet.conv_out.state_dict()[\"weight\"].stride())  # (2880, 9, 3, 1)\n",
    "pipeline.unet.to(memory_format=torch.channels_last)  # in-place operation\n",
    "print(\n",
    "    pipeline.unet.conv_out.state_dict()[\"weight\"].stride()\n",
    ")  # (2880, 1, 960, 320) having a stride of 1 for the 2nd dimension proves that it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffbfea-3602-40de-8a90-02a454af331d",
   "metadata": {},
   "source": [
    "#### `torch.jit.trace`\n",
    "- `torch.jit.trace` records the operations a model performs on a sample input and creates a new, optimized representation of the model based on the recorded execution path.\n",
    "  - During tracing, the model is optimized to reduce overhead from Python and dynamic control flows and operations are fused together for more efficiency.\n",
    "  - The returned executable or `ScriptFunction` can be compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddd9d7-b42b-4782-abff-e56c424685c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch disable grad\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# set variables\n",
    "n_experiments = 2\n",
    "unet_runs_per_experiment = 50\n",
    "\n",
    "# load sample inputs\n",
    "def generate_inputs():\n",
    "    sample = torch.randn((2, 4, 64, 64), device=\"cuda\", dtype=torch.float16)\n",
    "    timestep = torch.rand(1, device=\"cuda\", dtype=torch.float16) * 999\n",
    "    encoder_hidden_states = torch.randn((2, 77, 768), device=\"cuda\", dtype=torch.float16)\n",
    "    return sample, timestep, encoder_hidden_states\n",
    "\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "unet = pipeline.unet\n",
    "unet.eval()\n",
    "unet.to(memory_format=torch.channels_last)  # use channels_last memory format\n",
    "unet.forward = functools.partial(unet.forward, return_dict=False)  # set return_dict=False as default\n",
    "\n",
    "# warmup\n",
    "for _ in range(3):\n",
    "    with torch.inference_mode():\n",
    "        inputs = generate_inputs()\n",
    "        orig_output = unet(*inputs)\n",
    "\n",
    "# trace\n",
    "print(\"tracing..\")\n",
    "unet_traced = torch.jit.trace(unet, inputs)\n",
    "unet_traced.eval()\n",
    "print(\"done tracing\")\n",
    "\n",
    "# warmup and optimize graph\n",
    "for _ in range(5):\n",
    "    with torch.inference_mode():\n",
    "        inputs = generate_inputs()\n",
    "        orig_output = unet_traced(*inputs)\n",
    "\n",
    "# benchmarking\n",
    "with torch.inference_mode():\n",
    "    for _ in range(n_experiments):\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        for _ in range(unet_runs_per_experiment):\n",
    "            orig_output = unet_traced(*inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"unet traced inference took {time.time() - start_time:.2f} seconds\")\n",
    "    for _ in range(n_experiments):\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        for _ in range(unet_runs_per_experiment):\n",
    "            orig_output = unet(*inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"unet inference took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# save the model\n",
    "unet_traced.save(\"unet_traced.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c9f9ec-71f7-44fc-8c17-ea7717c623c7",
   "metadata": {},
   "source": [
    "- Replace the pipeline’s UNet with the traced version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c7994-eeb7-4cc0-8e18-ff64f801af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class UNet2DConditionOutput:\n",
    "    sample: torch.Tensor\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# use jitted unet\n",
    "unet_traced = torch.jit.load(\"unet_traced.pt\")\n",
    "\n",
    "# del pipeline.unet\n",
    "class TracedUNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_channels = pipe.unet.config.in_channels\n",
    "        self.device = pipe.unet.device\n",
    "\n",
    "    def forward(self, latent_model_input, t, encoder_hidden_states):\n",
    "        sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]\n",
    "        return UNet2DConditionOutput(sample=sample)\n",
    "\n",
    "pipeline.unet = TracedUNet()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    image = pipe([prompt] * 1, num_inference_steps=50).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0ee21-2a45-4c06-af94-5d6575009131",
   "metadata": {},
   "source": [
    "#### Memory-efficient attention\n",
    "- The Transformers attention mechanism is memory-intensive, especially for long sequences, so you can try using different and more memory-efficient attention types.\n",
    "  - By default, if PyTorch >= 2.0 is installed, scaled dot-product attention (SDPA) is used. You don’t need to make any additional changes to your code.\n",
    "\n",
    "- SDPA supports `FlashAttention` and xFormers as well as a native C++ PyTorch implementation.\n",
    "  - It automatically selects the most optimal implementation based on your input.\n",
    "  - You can explicitly use xFormers with the `enable_xformers_memory_efficient_attention()` method.\n",
    "  - Call `disable_xformers_memory_efficient_attention()` to disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34940fc-fe4d-4e92-9b5f-eef95b4e425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "pipeline.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "# pipeline.disable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea1e22-8505-4bcd-b6a9-45b5f92245e4",
   "metadata": {},
   "source": [
    "-----\n",
    "### **PyTorch 2.0**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/optimization/torch2.0\n",
    "\n",
    "- Diffusers supports the latest optimizations from `PyTorch 2.0` which include:\n",
    "  - A memory-efficient attention implementation, scaled dot product attention, without requiring any extra dependencies such as xFormers.\n",
    "  - `torch.compile`, a just-in-time (JIT) compiler to provide an extra performance boost when individual models are compiled.\n",
    "\n",
    "\n",
    "---------\n",
    "### **xFormers**\n",
    "- We recommend `xFormers` for both inference and training.\n",
    "  - In our tests, the optimizations performed in the attention blocks allow for both faster speed and reduced memory consumption.\n",
    "\n",
    "- You can use `enable_xformers_memory_efficient_attention()` for faster inference and reduced memory consumption as shown in this section.\n",
    "\n",
    "---------\n",
    "### **Token merging**\n",
    "- `Token merging (ToMe)` merges redundant tokens/patches progressively in the forward pass of a Transformer-based network which can speed-up the inference latency of `StableDiffusionPipeline`.\n",
    "\n",
    "- The `apply_patch` function exposes a number of arguments to help strike a balance between pipeline inference speed and the quality of the generated tokens.\n",
    "  - The most important argument is ratio which controls the number of tokens that are merged during the forward pass.\n",
    "\n",
    "- `ToMe` can greatly preserve the quality of the generated images while boosting inference speed.\n",
    "  - By increasing the ratio, you can speed-up inference even further, but at the cost of some degraded image quality.\n",
    "  - To test the quality of the generated images, we sampled a few prompts from Parti Prompts and performed inference with the `StableDiffusionPipeline` with the following settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f0be6-4b32-4439-985a-0f5f81a74f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# + tomesd.apply_patch(pipeline, ratio=0.5)\n",
    "\n",
    "image = pipeline(\"a photo of an astronaut riding a horse on mars\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a408959-0671-4e92-a96c-860b44b13efa",
   "metadata": {},
   "source": [
    "----\n",
    "### **DeepCache**\n",
    "- `DeepCache` accelerates `StableDiffusionPipeline` and `StableDiffusionXLPipeline` by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture.\n",
    "- Load and enable the `DeepCacheSDHelper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903548ee-b668-40f5-acd0-1864de1ab889",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained('stable-diffusion-v1-5/stable-diffusion-v1-5', torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "helper = DeepCacheSDHelper(pipe=pipe)\n",
    "helper.set_params(cache_interval=3, cache_branch_id=0)\n",
    "helper.enable()\n",
    "\n",
    "image = pipe(\"a photo of an astronaut on a moon\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401bef2-eff7-414b-a51a-7bfd97883093",
   "metadata": {},
   "source": [
    "- The `set_params` method accepts two arguments: `cache_interval` and `cache_branch_id`.\n",
    "  - `cache_interval` means the frequency of feature caching, specified as the number of steps between each cache operation.\n",
    "  - `cache_branch_id` identifies which branch of the network (ordered from the shallowest to the deepest layer) is responsible for executing the caching processes.\n",
    "  - Opting for a lower `cache_branch_id` or a larger `cache_interval` can lead to faster inference speed at the expense of reduced image quality (ablation experiments of these two hyperparameters can be found in the paper).\n",
    "  - Once those arguments are set, use the enable or disable methods to activate or deactivate the `DeepCacheSDHelper`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b1ef68-b8ea-4dba-bd3e-f46ce96c25c9",
   "metadata": {},
   "source": [
    "----\n",
    "### **T-GATE**\n",
    "- `T-GATE` accelerates inference for `Stable Diffusion`, `PixArt`, and `Latency Consistency Model` pipelines by skipping the cross-attention calculation once it converges.\n",
    "  - This method doesn’t require any additional training and it can speed up inference from 10-50%.\n",
    "    - `T-GATE` is also compatible with other optimization methods like `DeepCache`.\n",
    "   \n",
    "- Create a `TgateLoader` with a pipeline, the gate step (the time step to stop calculating the cross attention), and the number of inference steps.\n",
    "  - Call the tgate method on the pipeline with a prompt, gate step, and the number of inference steps.\n",
    "\n",
    "- Accelerate `PixArtAlphaPipeline`, `StableDiffusionXLPipeline`, `StableDiffusionXLPipeline` and `latent-consistency/lcm-sdxl` with `T-GATE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807fc7b-b5fd-4c9a-984b-b48e1c63b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PixArt\n",
    "pipe = PixArtAlphaPipeline.from_pretrained(\"PixArt-alpha/PixArt-XL-2-1024-MS\", torch_dtype=torch.float16)\n",
    "\n",
    "gate_step = 8\n",
    "inference_step = 25\n",
    "pipe = TgatePixArtLoader(\n",
    "       pipe,\n",
    "       gate_step=gate_step,\n",
    "       num_inference_steps=inference_step,\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipe.tgate(\n",
    "       \"An alpaca made of colorful building blocks, cyberpunk.\",\n",
    "       gate_step=gate_step,\n",
    "       num_inference_steps=inference_step,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834a800-ac0a-4083-acd4-9d3471bb673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable Diffusion XL\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",\n",
    "            use_safetensors=True,\n",
    ")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "gate_step = 10\n",
    "inference_step = 25\n",
    "pipe = TgateSDXLLoader(\n",
    "       pipe,\n",
    "       gate_step=gate_step,\n",
    "       num_inference_steps=inference_step,\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipe.tgate(\n",
    "       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n",
    "       gate_step=gate_step,\n",
    "       num_inference_steps=inference_step\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8692c-74e7-415d-8bf5-c481099a1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable Diffusion XL with DeepCache\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",\n",
    "            use_safetensors=True,\n",
    ")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "gate_step = 10\n",
    "inference_step = 25\n",
    "pipe = TgateSDXLDeepCacheLoader(\n",
    "       pipe,\n",
    "       cache_interval=3,\n",
    "       cache_branch_id=0,\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipe.tgate(\n",
    "       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n",
    "       gate_step=gate_step,\n",
    "       num_inference_steps=inference_step\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac1ca0-afa0-4422-b8e0-f072d20bc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Consistency Model\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"latent-consistency/lcm-sdxl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    unet=unet,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "gate_step = 1\n",
    "inference_step = 4\n",
    "pipe = TgateSDXLLoader(\n",
    "       pipe,\n",
    "       gate_step=gate_step,\n",
    "       num_inference_steps=inference_step,\n",
    "       lcm=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipe.tgate(\n",
    "       \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.\",\n",
    "       gate_step=gate_step,\n",
    "       num_inference_steps=inference_step\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329171c-60ce-4002-9ada-debb7aa478a1",
   "metadata": {},
   "source": [
    "----\n",
    "### **xDiT**\n",
    "- `xDiT` is an inference engine designed for the large scale parallel deployment of Diffusion Transformers (DiTs).\n",
    "  - `xDiT` provides a suite of efficient parallel approaches for Diffusion Models, as well as GPU kernel accelerations.\n",
    "\n",
    "- There are four parallel methods supported in `xDiT`, including Unified Sequence Parallelism, PipeFusion, CFG parallelism and data parallelism.\n",
    "  - The four parallel methods in xDiT can be configured in a hybrid manner, optimizing communication patterns to best suit the underlying network hardware.\n",
    "\n",
    "- Optimization orthogonal to parallelization focuses on accelerating single GPU performance.\n",
    "  - In addition to utilizing well-known Attention optimization libraries, we leverage compilation acceleration technologies such as torch.compile and onediff.\n",
    " \n",
    "- Using `xDiT` to accelerate inference of a Diffusers model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd98f64-b451-43ee-832e-175260719046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = FlexibleArgumentParser(description=\"xFuser Arguments\")\n",
    "    args = xFuserArgs.add_cli_args(parser).parse_args()\n",
    "    engine_args = xFuserArgs.from_cli_args(args)\n",
    "    engine_config, input_config = engine_args.create_config()\n",
    "\n",
    "     local_rank = get_world_group().local_rank\n",
    "     pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "         pretrained_model_name_or_path=engine_config.model_config.model,\n",
    "         torch_dtype=torch.float16,\n",
    "     ).to(f\"cuda:{local_rank}\")\n",
    "    \n",
    "# do anything you want with pipeline here\n",
    "\n",
    "    pipe = xDiTParallel(pipe, engine_config, input_config)\n",
    "\n",
    "     pipe(\n",
    "         height=input_config.height,\n",
    "         width=input_config.height,\n",
    "         prompt=input_config.prompt,\n",
    "         num_inference_steps=input_config.num_inference_steps,\n",
    "         output_type=input_config.output_type,\n",
    "         generator=torch.Generator(device=\"cuda\").manual_seed(input_config.seed),\n",
    "     )\n",
    "\n",
    "    if input_config.output_type == \"pil\":\n",
    "        pipe.save(\"results\", \"stable_diffusion_3\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ad8a7-e628-41f1-9176-ce96997d1920",
   "metadata": {},
   "source": [
    "- We only need to use xFuserArgs from `xDiT` to get configuration parameters, and pass these parameters along with the pipeline object from the Diffusers library into `xDiTParallel` to complete the parallelization of a specific pipeline in Diffusers.\n",
    "  - `xDiT` runtime parameters can be viewed in the command line using `-h`, and you can refer to this usage example for more details.\n",
    "  - `xDiT` needs to be launched using torchrun to support its multi-node, multi-GPU parallel capabilities.\n",
    "  - The following command can be used for 8-GPU parallel inference:\n",
    "  ```\n",
    "  torchrun --nproc_per_node=8 ./inference.py --model models/FLUX.1-dev --data_parallel_degree 2 --ulysses_degree 2 --ring_degree 2 --prompt \"A snowy mountain\" \"A small dog\" --num_inference_steps 50\n",
    "\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f6a52-cd1d-4496-8212-a6098b31c747",
   "metadata": {},
   "source": [
    "----\n",
    "### **ParaAttention**\n",
    "- Large image generation models, such as `FLUX.1-dev`, can be an inference challenge for real-time applications and deployment because of their size.\n",
    "  - `ParaAttention` is a library that implements context parallelism and first block cache, and can be combined with other techniques (`torch.compile`, `fp8` dynamic quantization), to accelerate inference.\n",
    "  - How to apply `ParaAttention` to `FLUX.1-dev` and `HunyuanVideo` on NVIDIA L20 GPUs.\n",
    "  - `FLUX.1-dev` is able to generate a 1024x1024 resolution image in 28 steps in 26.36 seconds.\n",
    "\n",
    "#### First Block Cache\n",
    "- Caching the output of the transformers blocks in the model and reusing them in the next inference steps reduces the computation cost and makes inference faster.\n",
    "  - However, it is hard to decide when to reuse the cache to ensure quality generated images or videos.\n",
    "  - `ParaAttention` directly uses the residual difference of the first transformer block output to approximate the difference among model outputs.\n",
    "  - When the difference is small enough, the residual difference of previous inference steps is reused.\n",
    "  - In other words, the denoising step is skipped.\n",
    "\n",
    "- This achieves a 2x speedup on `FLUX.1-dev` inference with very good quality.\n",
    "  - To apply first block cache on `FLUX.1-dev`, call `apply_cache_on_pipe` as shown below.\n",
    "  - `0.08` is the default residual difference value for FLUX models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05318f7b-062e-44b9-a290-aedace00109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\n",
    "\n",
    "apply_cache_on_pipe(pipe, residual_diff_threshold=0.08)\n",
    "\n",
    "# Enable memory savings\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "begin = time.time()\n",
    "image = pipe(\n",
    "    \"A cat holding a sign that says hello world\",\n",
    "    num_inference_steps=28,\n",
    ").images[0]\n",
    "end = time.time()\n",
    "print(f\"Time: {end - begin:.2f}s\")\n",
    "\n",
    "print(\"Saving image to flux.png\")\n",
    "image.save(\"flux.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2da6f-95ed-490d-abf0-6fdeb333fcaf",
   "metadata": {},
   "source": [
    "#### `fp8` quantization\n",
    "- `fp8` with dynamic quantization further speeds up inference and reduces memory usage.\n",
    "  - Both the activations and weights must be quantized in order to use the 8-bit NVIDIA Tensor Cores.\n",
    "  - Use `float8_weight_only` and `float8_dynamic_activation_float8_weight` to quantize the text encoder and transformer model.\n",
    "\n",
    "- The default quantization method is per tensor quantization, but if your GPU supports row-wise quantization, you can also try it for better accuracy.\n",
    "  - `torch.compile` with `mode=\"max-autotune-no-cudagraphs\"` or `mode=\"max-autotune\"` selects the best kernel for performance.\n",
    "  - Compilation can take a long time if it’s the first time the model is called, but it is worth it once the model has been compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5cf98-a81e-41ad-8fd2-6bf7d9ed7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\n",
    "\n",
    "apply_cache_on_pipe(\n",
    "    pipe,\n",
    "    residual_diff_threshold=0.12,  # Use a larger value to make the cache take effect\n",
    ")\n",
    "\n",
    "from torchao.quantization import quantize_, float8_dynamic_activation_float8_weight, float8_weight_only\n",
    "\n",
    "quantize_(pipe.text_encoder, float8_weight_only())\n",
    "quantize_(pipe.transformer, float8_dynamic_activation_float8_weight())\n",
    "pipe.transformer = torch.compile(\n",
    "   pipe.transformer, mode=\"max-autotune-no-cudagraphs\",\n",
    ")\n",
    "\n",
    "# Enable memory savings\n",
    "# pipe.enable_model_cpu_offload()\n",
    "# pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "for i in range(2):\n",
    "    begin = time.time()\n",
    "    image = pipe(\n",
    "        \"A cat holding a sign that says hello world\",\n",
    "        num_inference_steps=28,\n",
    "    ).images[0]\n",
    "    end = time.time()\n",
    "    if i == 0:\n",
    "        print(f\"Warm up time: {end - begin:.2f}s\")\n",
    "    else:\n",
    "        print(f\"Time: {end - begin:.2f}s\")\n",
    "\n",
    "print(\"Saving image to flux.png\")\n",
    "image.save(\"flux.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b025a-c3d8-4abf-b74f-f8d105ae316c",
   "metadata": {},
   "source": [
    "#### Context Parallelism\n",
    "- Context Parallelism parallelizes inference and scales with multiple GPUs.\n",
    "  - The ParaAttention compositional design allows you to combine Context Parallelism with First Block Cache and dynamic quantization.\n",
    "  - If the inference process needs to be persistent and serviceable, it is suggested to use `torch.multiprocessing` to write your own inference processor.\n",
    "  - This can eliminate the overhead of launching the process and loading and recompiling the model.\n",
    "  - This combines `First Block Cache`, `fp8` dynamic quantization, `torch.compile`, and Context Parallelism for the fastest inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffb9d2-57ed-4ba7-8931-7d837cc2ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.init_process_group()\n",
    "\n",
    "torch.cuda.set_device(dist.get_rank())\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from para_attn.context_parallel import init_context_parallel_mesh\n",
    "from para_attn.context_parallel.diffusers_adapters import parallelize_pipe\n",
    "from para_attn.parallel_vae.diffusers_adapters import parallelize_vae\n",
    "\n",
    "mesh = init_context_parallel_mesh(\n",
    "    pipe.device.type,\n",
    "    max_ring_dim_size=2,\n",
    ")\n",
    "parallelize_pipe(\n",
    "    pipe,\n",
    "    mesh=mesh,\n",
    ")\n",
    "parallelize_vae(pipe.vae, mesh=mesh._flatten())\n",
    "\n",
    "from para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe\n",
    "\n",
    "apply_cache_on_pipe(\n",
    "    pipe,\n",
    "    residual_diff_threshold=0.12,  # Use a larger value to make the cache take effect\n",
    ")\n",
    "\n",
    "from torchao.quantization import quantize_, float8_dynamic_activation_float8_weight, float8_weight_only\n",
    "\n",
    "quantize_(pipe.text_encoder, float8_weight_only())\n",
    "quantize_(pipe.transformer, float8_dynamic_activation_float8_weight())\n",
    "torch._inductor.config.reorder_for_compute_comm_overlap = True\n",
    "pipe.transformer = torch.compile(\n",
    "   pipe.transformer, mode=\"max-autotune-no-cudagraphs\",\n",
    ")\n",
    "\n",
    "# Enable memory savings\n",
    "# pipe.enable_model_cpu_offload(gpu_id=dist.get_rank())\n",
    "# pipe.enable_sequential_cpu_offload(gpu_id=dist.get_rank())\n",
    "\n",
    "for i in range(2):\n",
    "    begin = time.time()\n",
    "    image = pipe(\n",
    "        \"A cat holding a sign that says hello world\",\n",
    "        num_inference_steps=28,\n",
    "        output_type=\"pil\" if dist.get_rank() == 0 else \"pt\",\n",
    "    ).images[0]\n",
    "    end = time.time()\n",
    "    if dist.get_rank() == 0:\n",
    "        if i == 0:\n",
    "            print(f\"Warm up time: {end - begin:.2f}s\")\n",
    "        else:\n",
    "            print(f\"Time: {end - begin:.2f}s\")\n",
    "\n",
    "if dist.get_rank() == 0:\n",
    "    print(\"Saving image to flux.png\")\n",
    "    image.save(\"flux.png\")\n",
    "\n",
    "dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d10f42-c445-46fd-accd-ba9cc2e8dad3",
   "metadata": {},
   "source": [
    "- Save to `run_flux.py` and launch it with torchrun.\n",
    "```\n",
    "# Use --nproc_per_node to specify the number of GPUs\n",
    "torchrun --nproc_per_node=2 run_flux.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dcc4b7-f273-44da-8102-4e58bd263370",
   "metadata": {},
   "source": [
    "----\n",
    "### **Optimized Model Formats | JaX/Flax**\n",
    "- Diffusers supports `Flax` for super fast inference on Google TPUs, such as those available in Colab, Kaggle or Google Cloud Platform.\n",
    "- You should also make sure you’re using a TPU backend.\n",
    "  - While JAX does not run exclusively on TPUs, you’ll get the best performance on a TPU because each server has 8 TPU accelerators working in parallel.\n",
    "  - If you are running this guide in Colab, select Runtime in the menu above, select the option Change runtime type, and then select TPU under the Hardware accelerator setting. Import JAX and quickly check whether you’re using a TPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65dd7f7-de65-4a71-9a00-ceefdbf0e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "num_devices = jax.device_count()\n",
    "device_type = jax.devices()[0].device_kind\n",
    "\n",
    "print(f\"Found {num_devices} JAX devices of type {device_type}.\")\n",
    "assert (\n",
    "    \"TPU\" in device_type,\n",
    "    \"Available device is not a TPU, please select TPU from Runtime > Change runtime type > Hardware accelerator\"\n",
    ")\n",
    "# Found 8 JAX devices of type Cloud TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b77a6-6944-48ef-91fa-0cdf6411e7e3",
   "metadata": {},
   "source": [
    "#### Load a model\n",
    "- `Flax` is a functional framework, so models are stateless and parameters are stored outside of them.\n",
    "  - Loading a pretrained `Flax` pipeline returns both the pipeline and the model weights (or parameters).\n",
    "  - You will use `bfloat16`, a more efficient half-float type that is supported by TPUs (you can also use `float32` for full precision if you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c552917-f5f8-47bf-9cd8-f96d5b7b085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = jnp.bfloat16\n",
    "pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    variant=\"bf16\",\n",
    "    dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c755092-eaf2-4839-8ba3-b58fc3bcfc1d",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "- TPUs usually have 8 devices working in parallel, so let’s use the same prompt for each device.\n",
    "  - This means you can perform inference on 8 devices at once, with each device generating one image.\n",
    "  - You’ll get 8 images in the same amount of time it takes for one chip to generate a single image.\n",
    "- After replicating the prompt, get the tokenized text ids by calling the `prepare_inputs` function on the pipeline.\n",
    "  - The length of the tokenized text is set to 77 tokens as required by the configuration of the underlying CLIP text model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939ea03-8758-427f-b0cf-7b697b2bea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A cinematic film still of Morgan Freeman starring as Jimi Hendrix, portrait, 40mm lens, shallow depth of field, close up, split lighting, cinematic\"\n",
    "prompt = [prompt] * jax.device_count()\n",
    "prompt_ids = pipeline.prepare_inputs(prompt)\n",
    "prompt_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063830de-97b6-4f6c-b431-a2671f7cf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "p_params = replicate(params)\n",
    "\n",
    "# arrays\n",
    "prompt_ids = shard(prompt_ids)\n",
    "prompt_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ec24b-b9a1-4119-9e6a-507d1a474a70",
   "metadata": {},
   "source": [
    "- This shape means each one of the 8 devices receives as an input a jnp array with shape (1, 77), where 1 is the batch size per device.\n",
    "  - On TPUs with sufficient memory, you could have a batch size larger than 1 if you want to generate multiple images (per chip) at once.\n",
    "\n",
    "- Create a random number generator to pass to the generation function.\n",
    "  - This is standard procedure in `Flax`, which is very serious and opinionated about random numbers.\n",
    "  - All functions that deal with random numbers are expected to receive a generator to ensure reproducibility, even when you’re training across multiple distributed devices.\n",
    "- The helper function below uses a seed to initialize a random number generator.\n",
    "  - As long as you use the same seed, you’ll get the exact same results.\n",
    "  - Feel free to use different seeds when exploring results later in the guide.\n",
    "  - The helper function, or rng, is split 8 times so each device receives a different generator and generates a different image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1639c0f-9d62-41ac-9f8b-3d6014dd07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key(seed=0):\n",
    "    return jax.random.PRNGKey(seed)\n",
    "\n",
    "rng = create_key(0)\n",
    "rng = jax.random.split(rng, jax.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a30ad4-0a24-468f-b13b-235cb2b7607a",
   "metadata": {},
   "source": [
    "- To take advantage of JAX’s optimized speed on a TPU, pass `jit=True` to the pipeline to compile the JAX code into an efficient representation and to ensure the model runs in parallel across the 8 devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81efc08b-4431-4ec9-9b6f-0ec4e815b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "images = pipeline(prompt_ids, p_params, rng, jit=True)[0]\n",
    "\n",
    "# CPU times: user 56.2 s, sys: 42.5 s, total: 1min 38s\n",
    "# Wall time: 1min 29s\n",
    "\n",
    "images = images.reshape((images.shape[0] * images.shape[1],) + images.shape[-3:])\n",
    "images = pipeline.numpy_to_pil(images)\n",
    "make_image_grid(images, rows=2, cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bca79e-7ba8-49bf-8a5d-724a349853b5",
   "metadata": {},
   "source": [
    "#### Using different prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8bfae-896d-4b2d-960b-74c2d14942c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Labrador in the style of Hokusai\",\n",
    "    \"Painting of a squirrel skating in New York\",\n",
    "    \"HAL-9000 in the style of Van Gogh\",\n",
    "    \"Times Square under water, with fish and a dolphin swimming around\",\n",
    "    \"Ancient Roman fresco showing a man working on his laptop\",\n",
    "    \"Close-up photograph of young black woman against urban background, high quality, bokeh\",\n",
    "    \"Armchair in the shape of an avocado\",\n",
    "    \"Clown astronaut in space, with Earth in the background\",\n",
    "]\n",
    "\n",
    "prompt_ids = pipeline.prepare_inputs(prompts)\n",
    "prompt_ids = shard(prompt_ids)\n",
    "\n",
    "images = pipeline(prompt_ids, p_params, rng, jit=True).images\n",
    "images = images.reshape((images.shape[0] * images.shape[1],) + images.shape[-3:])\n",
    "images = pipeline.numpy_to_pil(images)\n",
    "\n",
    "make_image_grid(images, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a58ab8-4964-4276-8413-123063626353",
   "metadata": {},
   "source": [
    "#### How does parallelization work?\n",
    "- The `Flax` pipeline in Diffusers automatically compiles the model and runs it in parallel on all available devices.\n",
    "  - `JAX` parallelization can be done in multiple ways.\n",
    "  - The easiest one revolves around using the jax.pmap function to achieve single-program multiple-data (SPMD) parallelization.\n",
    "  - It means running several copies of the same code, each on different data inputs.\n",
    "  - More sophisticated approaches are possible, and you can go over to the `JAX` documentation to explore this topic in more detail if you are interested.\n",
    "\n",
    "- `jax.pmap` does two things:\n",
    "  - Compiles (or `”jits”`) the code which is similar to `jax.jit()`.\n",
    "    - This does not happen when you call pmap, and only the first time the pmapped function is called.\n",
    "  - Ensures the compiled code runs in parallel on all available devices.\n",
    "\n",
    "- To demonstrate, call pmap on the pipeline’s `_generate` method (this is a private method that generates images and may be renamed or removed in future releases of Diffusers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56305a94-3b9a-42ee-b96c-9b6ea0b1ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_generate = pmap(pipeline._generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb7fb8-d69f-4ad0-893a-a0a310d412cd",
   "metadata": {},
   "source": [
    "- After calling pmap, the prepared function p_generate will:\n",
    "\n",
    "Make a copy of the underlying function, pipeline._generate, on each device.\n",
    "Send each device a different portion of the input arguments (this is why it’s necessary to call the shard function). In this case, prompt_ids has shape (8, 1, 77, 768) so the array is split into 8 and each copy of _generate receives an input with shape (1, 77, 768).\n",
    "The most important thing to pay attention to here is the batch size (1 in this example), and the input dimensions that make sense for your code. You don’t have to change anything else to make the code work in parallel.\n",
    "\n",
    "The first time you call the pipeline takes more time, but the calls afterward are much faster. The block_until_ready function is used to correctly measure inference time because JAX uses asynchronous dispatch and returns control to the Python loop as soon as it can. You don’t need to use that in your code; blocking occurs automatically when you want to use the result of a computation that has not yet been materialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b1ac0-18c8-4a08-acdd-77d1dc6f822a",
   "metadata": {},
   "source": [
    "- After calling pmap, the prepared function `p_generate` will:\n",
    "  - Make a copy of the underlying function, `pipeline._generate`, on each device.\n",
    "  - Send each device a different portion of the input arguments (this is why it’s necessary to call the shard function).\n",
    "    - In this case, `prompt_ids` has shape `(8, 1, 77, 768)` so the array is split into 8 and each copy of `_generate` receives an input with shape `(1, 77, 768)`.\n",
    "  - The most important thing to pay attention to here is the batch size (1 in this example), and the input dimensions that make sense for your code.\n",
    "    - You don’t have to change anything else to make the code work in parallel.\n",
    "\n",
    "- The first time you call the pipeline takes more time, but the calls afterward are much faster.\n",
    "  - The `block_until_ready` function is used to correctly measure inference time because `JAX` uses asynchronous dispatch and returns control to the Python loop as soon as it can.\n",
    "  - You don’t need to use that in your code; blocking occurs automatically when you want to use the result of a computation that has not yet been materialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc906e-df29-4393-b460-4ae709ef5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "images = p_generate(prompt_ids, p_params, rng)\n",
    "images = images.block_until_ready()\n",
    "\n",
    "# CPU times: user 1min 15s, sys: 18.2 s, total: 1min 34s\n",
    "# Wall time: 1min 15s\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a8c095-8ee4-44d4-9967-e1003bac4348",
   "metadata": {},
   "source": [
    "----\n",
    "### **Optimized Model Formats | ONNX**\n",
    "- `Optimum` provides a Stable Diffusion pipeline compatible with ONNX Runtime\n",
    "\n",
    "#### Stable Diffusion\n",
    "- To load and run inference, use the `ORTStableDiffusionPipeline`.\n",
    "  - If you want to load a PyTorch model and convert it to the ONNX format on-the-fly, set `export=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90086f-69a1-4f21-bdc7-2dd85bc0b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "pipeline = ORTStableDiffusionPipeline.from_pretrained(model_id, export=True)\n",
    "prompt = \"sailing ship in storm by Leonardo da Vinci\"\n",
    "image = pipeline(prompt).images[0]\n",
    "pipeline.save_pretrained(\"./onnx-stable-diffusion-v1-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959ca46-b239-4be8-b84f-9b5af66388d4",
   "metadata": {},
   "source": [
    "- To export the pipeline in the `ONNX` format offline and use it later for inference, use the `optimum-cli` export command:\n",
    "```\n",
    "optimum-cli export onnx --model stable-diffusion-v1-5/stable-diffusion-v1-5 sd_v15_onnx/\n",
    "```\n",
    "\n",
    "- Then to perform inference (you don’t have to specify `export=True` again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd1ccc-3cd5-4680-9bc2-7eb5f713714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sd_v15_onnx\"\n",
    "pipeline = ORTStableDiffusionPipeline.from_pretrained(model_id)\n",
    "prompt = \"sailing ship in storm by Leonardo da Vinci\"\n",
    "image = pipeline(prompt).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7058c1b-4ac2-4979-b287-8bc4a89ebc9e",
   "metadata": {},
   "source": [
    "#### Stable Diffusion XL\n",
    "- To load and run inference with `SDXL`, use the `ORTStableDiffusionXLPipeline`.\n",
    "\n",
    "- To export the pipeline in the ONNX format and use it later for inference, use the optimum-cli export command:\n",
    "```\n",
    "optimum-cli export onnx --model stabilityai/stable-diffusion-xl-base-1.0 --task stable-diffusion-xl sd_xl_onnx/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ca6fa-1e32-4c81-a74e-2350fffcce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "pipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\n",
    "prompt = \"sailing ship in storm by Leonardo da Vinci\"\n",
    "image = pipeline(prompt).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61bbd0-bc08-4191-8963-f9ee4d2abeb5",
   "metadata": {},
   "source": [
    "----\n",
    "### **Optimized Model Formats | OpenVINO**\n",
    "- Optimum provides Stable Diffusion pipelines compatible with `OpenVINO` to perform inference on a variety of Intel processors.\n",
    "\n",
    "#### Stable Diffusion\n",
    "- To load and run inference, use the `OVStableDiffusionPipeline`.\n",
    "  - If you want to load a PyTorch model and convert it to the `OpenVINO` format on-the-fly, set `export=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4d541-4106-4328-b3f8-b49cbbdcf900",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "pipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\n",
    "prompt = \"sailing ship in storm by Rembrandt\"\n",
    "image = pipeline(prompt).images[0]\n",
    "\n",
    "# Don't forget to save the exported model\n",
    "pipeline.save_pretrained(\"openvino-sd-v1-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6b15f-b22d-49c8-beeb-b782f179608f",
   "metadata": {},
   "source": [
    "- To further speed-up inference, statically reshape the model.\n",
    "  - If you change any parameters such as the outputs height or width, you’ll need to statically reshape your model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb302d8-b2b8-4c4d-8b8c-3bb8a94bd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the shapes related to the inputs and desired outputs\n",
    "batch_size, num_images, height, width = 1, 1, 512, 512\n",
    "\n",
    "# Statically reshape the model\n",
    "pipeline.reshape(batch_size, height, width, num_images)\n",
    "# Compile the model before inference\n",
    "pipeline.compile()\n",
    "\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    num_images_per_prompt=num_images,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b233a-7d01-471c-98fc-85df015f92b1",
   "metadata": {},
   "source": [
    "#### Stable Diffusion XL\n",
    "- To load and run inference with `SDXL`, use the `OVStableDiffusionXLPipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcf597-d6ac-419f-8728-455ad352cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\n",
    "prompt = \"sailing ship in storm by Rembrandt\"\n",
    "image = pipeline(prompt).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838a129-b02f-4ffb-a27e-1dc728ba507b",
   "metadata": {},
   "source": [
    "----\n",
    "### **Optimized Model Formats | CoreML**\n",
    "- `Core ML` is the model format and machine learning library supported by Apple frameworks.\n",
    "  - If you are interested in running Stable Diffusion models inside your macOS or iOS/iPadOS apps, this guide will show you how to convert existing PyTorch checkpoints into the Core ML format and use them for inference with Python or Swift.\n",
    "- Core ML models can leverage all the compute engines available in Apple devices: the CPU, the GPU, and the Apple Neural Engine (or ANE, a tensor-optimized accelerator available in Apple Silicon Macs and modern iPhones/iPads).\n",
    "  - Depending on the model and the device it’s running on, Core ML can mix and match compute engines too, so some portions of the model may run on the CPU while others run on GPU, for example.\n",
    "\n",
    "- You can also run the diffusers Python codebase on Apple Silicon Macs using the mps accelerator built into PyTorch. This approach is explained in depth in the mps guide, but it is not compatible with native apps.\n",
    "\n",
    "#### Core ML Inference in Python\n",
    "- Install the following libraries to run Core ML inference in Python:\n",
    "```\n",
    "pip install huggingface_hub\n",
    "pip install git+https://github.com/apple/ml-stable-diffusion\n",
    "```\n",
    "\n",
    "- To run inference in Python, use one of the versions stored in the packages folders because the compiled ones are only compatible with Swift.\n",
    "  - You may choose whether you want to use original or split_einsum attention.\n",
    "  - This is how you’d download the original attention variant from the Hub to a directory called models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bfc61-9b80-4008-a1dd-d911f4df3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"apple/coreml-stable-diffusion-v1-4\"\n",
    "variant = \"original/packages\"\n",
    "\n",
    "model_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\n",
    "snapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "print(f\"Model downloaded at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66dd26-e2ad-4460-9b5d-47e0da9bfe1b",
   "metadata": {},
   "source": [
    "- Once you have downloaded a snapshot of the model, you can test it using Apple’s Python script.\n",
    "```\n",
    "python -m python_coreml_stable_diffusion.pipeline --prompt \"a photo of an astronaut riding a horse on mars\" -i ./models/coreml-stable-diffusion-v1-4_original_packages/original/packages -o </path/to/output/image> --compute-unit CPU_AND_GPU --seed 93\n",
    "```\n",
    "\n",
    "- Pass the path of the downloaded checkpoint with -i flag to the script. `--compute-unit` indicates the hardware you want to allow for inference.\n",
    "  - It must be one of the following options: ALL, CPU_AND_GPU, CPU_ONLY, CPU_AND_NE.\n",
    "  - You may also provide an optional output path, and a seed for reproducibility.\n",
    "\n",
    "- The inference script assumes you’re using the original version of the Stable Diffusion model, `CompVis/stable-diffusion-v1-4`.\n",
    "  - If you use another model, you have to specify its Hub id in the inference command line, using the `--model-version` option.\n",
    "  - This works for models already supported and custom models you trained or fine-tuned yourself.\n",
    "\n",
    "- If you want to use `stable-diffusion-v1-5/stable-diffusion-v1-5`:\n",
    "```\n",
    "python -m python_coreml_stable_diffusion.pipeline --prompt \"a photo of an astronaut riding a horse on mars\" --compute-unit ALL -o output --seed 93 -i models/coreml-stable-diffusion-v1-5_original_packages --model-version stable-diffusion-v1-5/stable-diffusion-v1-5\n",
    "```\n",
    "\n",
    "#### Core ML inference in Swift\n",
    "- Running inference in Swift is slightly faster than in Python because the models are already compiled in the mlmodelc format.\n",
    "  - This is noticeable on app startup when the model is loaded but shouldn’t be noticeable if you run several generations afterward.\n",
    "- To run inference in Swift on your Mac, you need one of the compiled checkpoint versions.\n",
    "  - We recommend you download them locally using Python code similar to the previous example, but with one of the compiled variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bb5fc-cefa-4479-8b8c-57b4fba0bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"apple/coreml-stable-diffusion-v1-4\"\n",
    "variant = \"original/compiled\"\n",
    "\n",
    "model_path = Path(\"./models\") / (repo_id.split(\"/\")[-1] + \"_\" + variant.replace(\"/\", \"_\"))\n",
    "snapshot_download(repo_id, allow_patterns=f\"{variant}/*\", local_dir=model_path, local_dir_use_symlinks=False)\n",
    "print(f\"Model downloaded at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af18147-f951-4b6e-be07-73faef79e17c",
   "metadata": {},
   "source": [
    "- To run inference, please clone Apple’s repo:\n",
    "```\n",
    "git clone https://github.com/apple/ml-stable-diffusion\n",
    "cd ml-stable-diffusion\n",
    "```\n",
    "\n",
    "- And then use Apple’s command line tool, Swift Package Manager:\n",
    "```\n",
    "swift run StableDiffusionSample --resource-path models/coreml-stable-diffusion-v1-4_original_compiled --compute-units all \"a photo of an astronaut riding a horse on mars\"\n",
    "```\n",
    "\n",
    "- You have to specify in `--resource-path` one of the checkpoints downloaded in the previous step, so please make sure it contains compiled Core ML bundles with the extension `.mlmodelc`.\n",
    "  - The `--compute-units` has to be one of these values: all, cpuOnly, cpuAndGPU, cpuAndNeuralEngine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-t2i",
   "language": "python",
   "name": "diffusers-t2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
