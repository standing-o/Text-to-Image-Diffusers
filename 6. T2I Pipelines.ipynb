{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572d69cf-0ff7-41cc-982e-1e62282c686a",
   "metadata": {},
   "source": [
    "## **6. T2I <sup>Text-to-image</sup> Pipelines**\n",
    "\n",
    "> Original Source: https://huggingface.co/docs/diffusers/v0.33.1/en/using-diffusers/sdxl_turbo\n",
    "\n",
    "```\n",
    "> Stable Diffusion XL\n",
    "> Stable Diffusion XL Turbo\n",
    "> Kandinsky\n",
    "> IP-Adapter\n",
    "> OmniGen\n",
    "> Perturbed-Attention Guidance(PAG)\n",
    "> ControlNet\n",
    "> T2I-Adapter\n",
    "> Latent Consistency Model\n",
    "> Textual inversion\n",
    "> DiffEdit\n",
    "> Trajectory Consistency Distillation-LoRA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3129c887-9b31-4d70-9d62-566b818a8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "from diffusers.utils import load_image, make_image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0269aa-fba8-4748-a8b2-fd08d5673b4b",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Stable Diffusion XL**\n",
    "- `Stable Diffusion XL (SDXL)` is a powerful text-to-image generation model that iterates on the previous Stable Diffusion models in three key ways:\n",
    "  - The `UNet` is 3x larger and `SDXL` combines a second text encoder (`OpenCLIP ViT-bigG/14`) with the original text encoder to significantly increase the number of parameters\n",
    "  - Introduces size and crop-conditioning to preserve training data from being discarded and gain more control over how a generated image should be cropped\n",
    "  - Introduces a two-stage model process; the base model (can also be run as a standalone model) generates an image as an input to the refiner model which adds additional high-quality details.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Install:\n",
    "```\n",
    "pip install -q diffusers transformers accelerate invisible-watermark>=0.2.0\n",
    "```\n",
    "\n",
    "- We recommend installing the `invisible-watermark` library to help identify images that are generated. If the invisible-watermark library is installed, it is used by default. To disable the watermarker:\n",
    "\n",
    "```\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(..., add_watermarker=False)\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### Load model checkpoints\n",
    "- Model weights may be stored in separate subfolders on the Hub or locally, in which case, you should use the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36533d39-8552-43ba-8b7c-9c18da518919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline, AutoPipelineForImage2Image\n",
    "from diffusers import StableDiffusionXLInpaintPipeline, AutoPipelineForInpainting\n",
    "\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47560f4c-02e0-4b77-83ca-dbb496ca4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e43f9-7641-4088-bee4-17dc4dd219e2",
   "metadata": {},
   "source": [
    "- You can also use the `from_single_file()` method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`) from the Hub or locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb1d5b-f79a-4a6e-be4f-13516621558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_single_file(\n",
    "    \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "refiner = StableDiffusionXLImg2ImgPipeline.from_single_file(\n",
    "    \"https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/blob/main/sd_xl_refiner_1.0.safetensors\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf0af3-ad8d-45ab-91e9-edf3e4d050c4",
   "metadata": {},
   "source": [
    "#### Text-to-image\n",
    "- For text-to-image, pass a text prompt. \n",
    "- By default, `SDXL` generates a 1024x1024 image for the best results.\n",
    "  - You can try setting the height and width parameters to 768x768 or 512x512, but anything below 512x512 is not likely to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76ce21-e43b-43ba-b035-6dd188df1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "image = pipeline_text2image(prompt=prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6f332-50c2-4b89-8366-26cab5b8e099",
   "metadata": {},
   "source": [
    "#### Refine image quality\n",
    "- `SDXL` includes a refiner model specialized in denoising low-noise stage images to generate higher-quality images from the base model.\n",
    "  - Use the base and refiner models together to produce a refined image\n",
    "  - Use the base model to produce an image, and subsequently use the refiner model to add more details to the image (this is how `SDXL` was originally trained)\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Base + refiner model**\n",
    "  - When you use the base and refiner model together to generate an image, this is known as an ensemble of expert denoisers.\n",
    "  - The ensemble of expert denoisers approach requires fewer overall denoising steps versus passing the base model’s output to the refiner model, so it should be significantly faster to run.\n",
    "  - However, you won’t be able to inspect the base model’s output because it still contains a large amount of noise.\n",
    "  - As an ensemble of expert denoisers, the base model serves as the expert during the high-noise diffusion stage and the refiner model serves as the expert during the low-noise diffusion stage. Load the base and refiner model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a315b7-d363-4f52-aba4-8bad0d087022",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f67bc6-bf70-4321-a09c-daca22489bd5",
   "metadata": {},
   "source": [
    "- To use this approach, you need to **define the number of timesteps for each model** to run through their respective stages.\n",
    "  - For the base model, this is controlled by the `denoising_end` parameter and for the refiner model, it is controlled by the `denoising_start` parameter.\n",
    " \n",
    "- The `denoising_end` and `denoising_start` parameters should be a float between 0 and 1.\n",
    "  - These parameters are represented as a proportion of discrete timesteps as defined by the scheduler.\n",
    "  - If you’re also using the strength parameter, it’ll be ignored because the number of denoising steps is determined by the discrete timesteps the model is trained on and the declared fractional cutoff.\n",
    "\n",
    "- Let’s set `denoising_end=0.8` so the base model performs the first 80% of denoising the high-noise timesteps and set `denoising_start=0.8` so the refiner model performs the last 20% of denoising the low-noise timesteps.\n",
    "  - The base model output should be in latent space instead of a PIL image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cbba80-ab20-41d3-bc61-4c4405921bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A majestic lion jumping from a big stone at night\"\n",
    "\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=40,\n",
    "    denoising_end=0.8,\n",
    "    output_type=\"latent\",\n",
    ").images\n",
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=40,\n",
    "    denoising_start=0.8,\n",
    "    image=image,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc671f70-9ed3-40a6-983b-7ebd5fd578fe",
   "metadata": {},
   "source": [
    "#### Base to refiner model\n",
    "- `SDXL` gets a boost in image quality by using the refiner model to add additional high-quality details to the fully-denoised image from the base model, in an image-to-image setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0785b5-f315-42be-a51d-faee598bfad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82137228-83ae-4e82-85ad-533ad32a1c57",
   "metadata": {},
   "source": [
    "- You can use `SDXL` refiner with a different base model.\n",
    "  - You can use the `Hunyuan-DiT` or `PixArt-Sigma` pipelines to generate images with better prompt adherence.\n",
    "  - Once you have generated an image, you can pass it to the `SDXL` refiner model to enhance final generation quality.\n",
    "- Set the model output to latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa1c0c-794a-4049-80e2-5c8ad3e52a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "image = base(prompt=prompt, output_type=\"latent\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c429c63-7d9d-4c1c-b8a6-0f0e36cc40a6",
   "metadata": {},
   "source": [
    "- For inpainting, load the base and the refiner model in the `StableDiffusionXLInpaintPipeline`, remove the `denoising_end` and `denoising_start` parameters, and choose a smaller number of inference steps for the refiner.\n",
    "- Pass the generated image to the refiner model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5ccd6-ad55-4c9b-adbe-2fd018d885c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = refiner(prompt=prompt, image=image[None, :]).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa97335-0c3e-4f71-95ea-7f70a2924419",
   "metadata": {},
   "source": [
    "#### Micro-conditioning\n",
    "- `SDXL` training involves several additional conditioning techniques, which are referred to as micro-conditioning.\n",
    "  - These include original image size, target image size, and cropping parameters.\n",
    "  - The micro-conditionings can be used at inference time to create high-quality, centered images.\n",
    "\n",
    "- You can use both micro-conditioning and negative micro-conditioning parameters thanks to classifier-free guidance.\n",
    "  - They are available in the `StableDiffusionXLPipeline`, `StableDiffusionXLImg2ImgPipeline`, `StableDiffusionXLInpaintPipeline`, and `StableDiffusionXLControlNetPipeline`.\n",
    " \n",
    "#### Size conditioning\n",
    "- There are two types of size conditioning:\n",
    "  - `original_size` conditioning comes from upscaled images in the training batch (because it would be wasteful to discard the smaller images which make up almost 40% of the total training data).\n",
    "    - Using the default value of (1024, 1024) produces higher-quality images that resemble the 1024x1024 images in the dataset.\n",
    "  - `target_size` conditioning comes from finetuning SDXL to support different image aspect ratios.\n",
    "    - During inference, if you use the default value of (1024, 1024), you’ll get an image that resembles the composition of square images in the dataset.\n",
    "    - We recommend using the same value for `target_size` and `original_size`, but feel free to experiment with other options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce162377-58cb-4eab-823c-be28bb0e0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_original_size=(512, 512),\n",
    "    negative_target_size=(1024, 1024),\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0970295-197d-4457-894e-1c32cabd62b8",
   "metadata": {},
   "source": [
    "#### Crop conditioning\n",
    "- Images generated by previous Stable Diffusion models may sometimes appear to be cropped.\n",
    "  - This is because images are actually cropped during training so that all the images in a batch have the same size.\n",
    "  - By conditioning on crop coordinates, `SDXL` learns that no cropping - coordinates (0, 0) - usually correlates with centered subjects and complete faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124eb281-797a-40e4-b831-a2f8bbbeac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "image = pipeline(prompt=prompt, crops_coords_top_left=(256, 0)).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225e4e8-b748-4b78-b46c-c0cb20f73e1e",
   "metadata": {},
   "source": [
    "- Specify negative cropping coordinates to steer generation away from certain cropping parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886e260-f345-43d4-9e27-9434f96ed95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_original_size=(512, 512),\n",
    "    negative_crops_coords_top_left=(0, 0),\n",
    "    negative_target_size=(1024, 1024),\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38ad95-c36e-42dd-94ee-810a227e74df",
   "metadata": {},
   "source": [
    "#### Use a different prompt for each text-encoder\n",
    "- SDXL uses two text-encoders, so it is possible to pass a different prompt to each text-encoder, which can improve quality.\n",
    "  - Pass your original prompt to prompt and the second prompt to `prompt_2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb47059-0e98-4a61-9bf4-b9f147c22e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "# prompt is passed to OAI CLIP-ViT/L-14\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "# prompt_2 is passed to OpenCLIP-ViT/bigG-14\n",
    "prompt_2 = \"Van Gogh painting\"\n",
    "image = pipeline(prompt=prompt, prompt_2=prompt_2).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb47ae4-44ae-46a8-8a89-4a3701f41efb",
   "metadata": {},
   "source": [
    "#### Optimizations\n",
    "- SDXL is a large model, and you may need to optimize memory to get it to run on your hardware.\n",
    "- Offload the model to the CPU with `enable_model_cpu_offload()` for out-of-memory errors:\n",
    "```\n",
    "- base.to(\"cuda\")\n",
    "- refiner.to(\"cuda\")\n",
    "+ base.enable_model_cpu_offload()\n",
    "+ refiner.enable_model_cpu_offload()\n",
    "```\n",
    "  - Use `torch.compile` for ~20% speed-up (you need torch>=2.0):\n",
    "```\n",
    "+ base.unet = torch.compile(base.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "+ refiner.unet = torch.compile(refiner.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "```\n",
    "  - Enable xFormers to run `SDXL` if torch<2.0:\n",
    "```\n",
    "+ base.enable_xformers_memory_efficient_attention()\n",
    "+ refiner.enable_xformers_memory_efficient_attention()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c087289-dd0c-4627-9373-6ef424dcccd8",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Stable Diffusion XL Turbo**\n",
    "- `SDXL Turbo` is an adversarial time-distilled `Stable Diffusion XL (SDXL)` model capable of running inference in as little as 1 step.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Load model checkpoints\n",
    "- Model weights may be stored in separate subfolders on the Hub or locally, in which case, you should use the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688851b-882e-4ceb-8616-7b630f610991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler\n",
    "\n",
    "from diffusers.utils import load_image, make_image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6770e0-5cb7-4045-9115-04ee68fd2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipeline = pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a903f8f-0cd6-4043-9d82-97f6c2dad794",
   "metadata": {},
   "source": [
    "- You can also use the `from_single_file()` method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`) from the Hub or locally.\n",
    "  - For this loading method, you need to set `timestep_spacing=\"trailing\"` (feel free to experiment with the other scheduler config values to get better results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873e01a-1135-4093-804c-01936aa92612",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipeline = pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d938f5-f2b9-47d7-80a2-fe66436d663e",
   "metadata": {},
   "source": [
    "- You can also use the `from_single_file()` method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`) from the Hub or locally.\n",
    "  - You need to set `timestep_spacing=\"trailing\"` (feel free to experiment with the other scheduler config values to get better results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca324514-491a-4edc-aeb2-b0b18497d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_single_file(\n",
    "    \"https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors\",\n",
    "    torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config, timestep_spacing=\"trailing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97602c0-6738-41b5-bf0c-0489aa74493a",
   "metadata": {},
   "source": [
    "#### Text-to-image\n",
    "- By default, `SDXL Turbo` generates a 512x512 image, and that resolution gives the best results.\n",
    "  - You can try setting the height and width parameters to 768x768 or 1024x1024, but you should expect quality degradations when doing so.\n",
    "\n",
    "- Make sure to set `guidance_scale` to `0.0` to disable, as the model was trained without it.\n",
    "  - A single inference step is enough to generate high quality images. Increasing the number of steps to 2, 3 or 4 should improve image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4fb4a-0b66-4f34-91aa-b5daa270b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipeline_text2image = pipeline_text2image.to(\"cuda\")\n",
    "\n",
    "prompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n",
    "\n",
    "image = pipeline_text2image(prompt=prompt, guidance_scale=0.0, num_inference_steps=1).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5b1a4-b890-47a3-86b4-272c5558d705",
   "metadata": {},
   "source": [
    "#### Speed-up `SDXL Turbo` even more\n",
    "- Compile the UNet if you are using PyTorch version 2.0 or higher.\n",
    "  - The first inference run will be very slow, but subsequent ones will be much faster.\n",
    "\n",
    "- When using the default VAE, keep it in float32 to avoid costly dtype conversions before and after each generation.\n",
    "  - You only need to do this one before your first generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cd4e9-d8df-4bcb-bf5b-e858241713b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "pipe.upcast_vae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c78af-8b66-4dae-96ad-ac671608a75a",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Kandinsky**\n",
    "- The Kandinsky models are a series of multilingual text-to-image generation models.\n",
    "  - The `Kandinsky 2.0` model uses two multilingual text encoders and concatenates those results for the UNet.\n",
    "  - `Kandinsky 2.1` changes the architecture to include an image prior model (CLIP) to generate a mapping between text and image embeddings and uses a `Modulating Quantized Vectors (MoVQ)` decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.\n",
    "  - `Kandinsky 2.2` improves on the previous model by replacing the image encoder of the image prior model with a larger `CLIP-ViT-G` model to improve quality.\n",
    "    - The only difference with `Kandinsky 2.1` is `Kandinsky 2.2` doesn’t accept prompt as an input when decoding the latents. Instead, `Kandinsky 2.2` only accepts `image_embeds` during decoding.\n",
    "  - `Kandinsky 3` simplifies the architecture and shifts away from the two-stage generation process involving the prior model and diffusion model and uses `Flan-UL2` to encode text, a `UNet` with BigGan-deep blocks, and `Sber-MoVQGAN` to decode the latents into images.\n",
    "    - Text understanding and generated image quality are primarily achieved by using a larger text encoder and UNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d194560-ad69-4426-8265-a40660172f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import KandinskyPriorPipeline, KandinskyPipeline\n",
    "from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\n",
    "from diffusers import Kandinsky3Pipeline\n",
    "\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline\n",
    "from diffusers import KandinskyV22Img2ImgPipeline\n",
    "from diffusers import Kandinsky3Img2ImgPipeline\n",
    "\n",
    "from diffusers import KandinskyInpaintPipeline\n",
    "from diffusers import KandinskyV22InpaintPipeline, KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n",
    "\n",
    "from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\n",
    "\n",
    "from diffusers.models.attention_processor import AttnAddedKVProcessor2_0\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.utils import make_image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf09cc4-69a2-4cc6-97ef-673c63d928a0",
   "metadata": {},
   "source": [
    "#### Text-to-image\n",
    "- To use the Kandinsky models for any task, you always start by setting up the prior pipeline to encode the prompt and generate the image embeddings.\n",
    "  - The prior pipeline also generates `negative_image_embeds` that correspond to the negative prompt \"\".\n",
    "  - You can pass an actual `negative_prompt` to the prior pipeline, but this’ll increase the effective batch size of the prior pipeline by 2x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73626a0-576a-4c0a-b8c9-a7f20c359790",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "prompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\n",
    "negative_prompt = \"low quality, bad quality\" # optional to include a negative prompt, but results are usually better\n",
    "image_embeds, negative_image_embeds = prior_pipeline(prompt, guidance_scale=1.0).to_tuple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5923e34-7536-48e8-8f1c-678c98d8964a",
   "metadata": {},
   "source": [
    "- Diffusers also provides an end-to-end API with the `KandinskyCombinedPipeline` and `KandinskyV22CombinedPipeline`, meaning you don’t have to separately load the prior and text-to-image pipeline.\n",
    "  - The combined pipeline automatically loads both the prior model and the decoder.\n",
    "  - You can still set different values for the prior pipeline with the `prior_guidance_scale` and `prior_num_inference_steps` parameters if you want.\n",
    "\n",
    "- Use the `AutoPipelineForText2Image` to automatically call the combined pipelines under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2dc534-2a47-4401-9e57-e5caed18961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\n",
    "pipeline.enable_model_cpu_offload()\n",
    "\n",
    "prompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "\n",
    "image = pipeline(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, guidance_scale=4.0, height=768, width=768).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cc2b5-3b4c-4ffa-ada6-15f921861c75",
   "metadata": {},
   "source": [
    "#### Interpolation\n",
    "- Interpolation allows you to explore the latent space between the image and text embeddings which is a cool way to see some of the prior model’s intermediate outputs.\n",
    "  - Load the prior pipeline and two images you’d like to interpolate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a4781-e1bf-44d0-afb0-7d52256e07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
    "img_1 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png\")\n",
    "img_2 = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg\")\n",
    "make_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d1f6a-b786-49c5-928d-8807e2901692",
   "metadata": {},
   "source": [
    "- Specify the text or images to interpolate, and set the weights for each text or image.\n",
    "  - Experiment with the weights to see how they affect the interpolation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d69c4-7c97-45ad-abca-1ece14931a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_texts = [\"a cat\", img_1, img_2]\n",
    "weights = [0.3, 0.3, 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4649b9-57ad-4f82-9829-b50a9758cc3e",
   "metadata": {},
   "source": [
    "- Call the interpolate function to generate the embeddings, and then pass them to the pipeline to generate the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d06c8f-53f6-472b-a405-7aaa079ebae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "prior_out = prior_pipeline.interpolate(images_texts, weights)\n",
    "\n",
    "pipeline = KandinskyV22Pipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "image = pipeline(prompt, **prior_out, height=768, width=768).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0a06f-062f-4f55-9298-fa16a31d2d4a",
   "metadata": {},
   "source": [
    "#### ControlNet\n",
    "- ControlNet enables conditioning large pretrained diffusion models with additional inputs such as a depth map or edge detection.\n",
    "  - You can condition `Kandinsky 2.2` with a depth map so the model understands and preserves the structure of the depth image.\n",
    " \n",
    "- Use the depth-estimation Pipeline from `Transformers` to process the image and retrieve the depth map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb03a5-40bb-485f-af7f-f2791fc75d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(\n",
    "    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png\"\n",
    ").resize((768, 768))\n",
    "\n",
    "def make_hint(image, depth_estimator):\n",
    "    image = depth_estimator(image)[\"depth\"]\n",
    "    image = np.array(image)\n",
    "    image = image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    detected_map = torch.from_numpy(image).float() / 255.0\n",
    "    hint = detected_map.permute(2, 0, 1)\n",
    "    return hint\n",
    "\n",
    "depth_estimator = pipeline(\"depth-estimation\")\n",
    "hint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ab646-3605-457f-8ec2-91ef534a6a60",
   "metadata": {},
   "source": [
    "- Load the prior pipeline and the `KandinskyV22ControlnetPipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31baf156-355b-45c7-86a0-b3072f4bf03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline = KandinskyV22ControlnetPipeline.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc57bb-2a2d-4171-bb1f-ec01213f80e5",
   "metadata": {},
   "source": [
    "- Generate the image embeddings from a prompt and negative prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9d98b-0d3d-4e4e-b819-aff00a917430",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A robot, 4k photo\"\n",
    "negative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(43)\n",
    "\n",
    "image_emb, zero_image_emb = prior_pipeline(\n",
    "    prompt=prompt, negative_prompt=negative_prior_prompt, generator=generator\n",
    ").to_tuple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88491a44-b0e4-482e-b8cb-c41b8dd86463",
   "metadata": {},
   "source": [
    "- Pass the image embeddings and the depth image to the `KandinskyV22ControlnetPipeline` to generate an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5aa2edc-c8b2-4845-8552-9a4daf920577",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipeline(image_embeds=image_emb, negative_image_embeds=zero_image_emb, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd26ca-4cfe-46c3-bb3e-1d636184d4f8",
   "metadata": {},
   "source": [
    "#### Optimizations\n",
    "- Kandinsky is unique because it requires a prior pipeline to generate the mappings, and a second pipeline to decode the latents into an image.\n",
    "  - Optimization efforts should be focused on the second pipeline because that is where the bulk of the computation is done.\n",
    "  - Here are some tips to improve Kandinsky during inference.\n",
    "\n",
    "- Enable xFormers if you’re using PyTorch < 2.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30753062-c199-4b83-8c17-bfb57305980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\n",
    "pipe.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630f536-3725-46fc-a32b-83ba235ef0a5",
   "metadata": {},
   "source": [
    "- Enable `torch.compile` if you’re using PyTorch >= 2.0 to automatically use scaled dot-product attention (SDPA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540db90-cb87-4c80-b70b-e472d8b76087",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet.to(memory_format=torch.channels_last)\n",
    "pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b17cef-6cb2-41cc-afda-ed1ca5767cb3",
   "metadata": {},
   "source": [
    "- This is the same as explicitly setting the attention processor to use `AttnAddedKVProcessor2_0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfe640-009a-4770-8be1-3d78847e1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet.set_attn_processor(AttnAddedKVProcessor2_0())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af7ddd-3d95-4393-bbdf-bbc85cff2527",
   "metadata": {},
   "source": [
    "- Offload the model to the CPU with `enable_model_cpu_offload()` to avoid out-of-memory errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d75a76-7b73-4550-a497-8c9095c3639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47f273-59d6-4620-92e1-1b8be716f3c1",
   "metadata": {},
   "source": [
    "- The text-to-image pipeline uses the `DDIMScheduler` but you can replace it with another scheduler like `DDPMScheduler` to see how that affects the tradeoff between inference speed and image quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf64f36-26f3-41de-8761-88a7273904e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler.from_pretrained(\"kandinsky-community/kandinsky-2-1\", subfolder=\"ddpm_scheduler\")\n",
    "pipe = DiffusionPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", scheduler=scheduler, torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae449e-64bf-4b55-adb9-94ac8db8e541",
   "metadata": {},
   "source": [
    "-----\n",
    "### **IP-Adapter**\n",
    "- `IP-Adapter` is an image prompt adapter that can be plugged into diffusion models to enable image prompting without any changes to the underlying model.\n",
    "  - This adapter can be reused with other models finetuned from the same base model and it can be combined with other adapters like ControlNet.\n",
    "  - The key idea behind `IP-Adapter` is the decoupled cross-attention mechanism which adds a separate cross-attention layer just for image features instead of using the same cross-attention layer for both text and image features.\n",
    "    - This allows the model to learn more image-specific features.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### General tasks\n",
    "- `set_ip_adapter_scale()` method controls the amount of text or image conditioning to apply to the model.\n",
    "  - A value of `1.0` means the model is only conditioned on the image prompt.\n",
    "  - Lowering this value encourages the model to produce more diverse images, but they may not be as aligned with the image prompt.\n",
    "  - Typically, a value of `0.5` achieves a good balance between the two prompt types and produces good results.\n",
    "\n",
    "- Try adding `low_cpu_mem_usage=True` to the `load_ip_adapter()` method to speed up the loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dec95b08-77b8-4408-a6ae-07a8fde0caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from diffusers.image_processor import IPAdapterMaskProcessor\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoPipelineForText2Image\n",
    "from diffusers import DiffusionPipeline, LCMScheduler\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c44b06-9b9d-464e-a11f-a1df6e3f010d",
   "metadata": {},
   "source": [
    "- Crafting the precise text prompt to generate the image you want can be difficult because it may not always capture what you’d like to express.\n",
    "  - Adding an image alongside the text prompt helps the model better understand what it should generate and can lead to more accurate results.\n",
    "\n",
    "- Load a `Stable Diffusion XL (SDXL)` model and insert an `IP-Adapter` into the model with the `load_ip_adapter()` method.\n",
    "  - Use the subfolder parameter to load the `SDXL` model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d94b0d-c069-4a31-8ecf-f5d1cfca72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f1d07-f41e-4de1-ae03-d01b6ec96ee0",
   "metadata": {},
   "source": [
    "- Create a text prompt and load an image prompt before passing them to the pipeline to generate an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e63d0b-26d4-4ced-9ace-fc7e23a17445",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\")\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "images = pipeline(\n",
    "    prompt=\"a polar bear sitting in a chair drinking a milkshake\",\n",
    "    ip_adapter_image=image,\n",
    "    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=100,\n",
    "    generator=generator,\n",
    ").images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703a227-4a69-45c5-b42d-afc746765df2",
   "metadata": {},
   "source": [
    "#### Configure parameters\n",
    "- There are a couple of `IP-Adapter` parameters that are useful to know about and can help you with your image generation tasks.\n",
    "  - These parameters can make your workflow more efficient or give you more control over image generation.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Image embeddings**\n",
    "  - IP-Adapter enabled pipelines provide the `ip_adapter_image_embeds` parameter to accept precomputed image embeddings.\n",
    "  - This is particularly useful in scenarios where you need to run the `IP-Adapter` pipeline multiple times because you have more than one image.\n",
    "    - ex. `Multi IP-Adapter` is a specific use case where you provide multiple styling images to generate a specific image in a specific style.\n",
    "      - Loading and encoding multiple images each time you use the pipeline would be inefficient.\n",
    "      - Instead, you can precompute and save the image embeddings to disk (which can save a lot of space if you’re using high-quality images) and load them when you need them.\n",
    "    - This parameter also gives you the flexibility to load embeddings from other sources.\n",
    "    - Call the `prepare_ip_adapter_image_embeds()` method to encode and generate the image embeddings.\n",
    "      - Then you can save them to disk with `torch.save`.\n",
    "  - If you’re using IP-Adapter with ip_adapter_image_embedding instead of `ip_adapter_image`’, you can set `load_ip_adapter(image_encoder_folder=None,...)` because you don’t need to load an encoder to generate the image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e32499-0eeb-4fe9-bf60-3fe91b77ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds = pipeline.prepare_ip_adapter_image_embeds(\n",
    "    ip_adapter_image=image,\n",
    "    ip_adapter_image_embeds=None,\n",
    "    device=\"cuda\",\n",
    "    num_images_per_prompt=1,\n",
    "    do_classifier_free_guidance=True,\n",
    ")\n",
    "\n",
    "torch.save(image_embeds, \"image_embeds.ipadpt\")\n",
    "\n",
    "image_embeds = torch.load(\"image_embeds.ipadpt\")\n",
    "images = pipeline(\n",
    "    prompt=\"a polar bear sitting in a chair drinking a milkshake\",\n",
    "    ip_adapter_image_embeds=image_embeds,\n",
    "    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=100,\n",
    "    generator=generator,\n",
    ").images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a8dbc6-3832-4adb-ae9f-63f8c0f75f12",
   "metadata": {},
   "source": [
    "- **IP-Adapter masking**\n",
    "  - Binary masks specify which portion of the output image should be assigned to an `IP-Adapter`.\n",
    "  - This is useful for composing more than one `IP-Adapter` image. For each input `IP-Adapter` image, you must provide a binary mask.\n",
    "  - To start, preprocess the input `IP-Adapter` images with the `~image_processor.IPAdapterMaskProcessor.preprocess()` to generate their masks.\n",
    "    - For optimal results, provide the output height and width to `~image_processor.IPAdapterMaskProcessor.preprocess()`.\n",
    "    - This ensures masks with different aspect ratios are appropriately stretched.\n",
    "    - If the input masks already match the aspect ratio of the generated image, you don’t have to set the height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f08baa-320d-48a3-8dbf-2a1b53cd1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_mask1.png\")\n",
    "mask2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_mask2.png\")\n",
    "\n",
    "output_height = 1024\n",
    "output_width = 1024\n",
    "\n",
    "processor = IPAdapterMaskProcessor()\n",
    "masks = processor.preprocess([mask1, mask2], height=output_height, width=output_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a58533-d0e8-4cc6-bf86-cc53796972c4",
   "metadata": {},
   "source": [
    "- When there is more than one input `IP-Adapter` image, load them as a list and provide the `IP-Adapter` scale list.\n",
    "  - Each of the input `IP-Adapter` images here corresponds to one of the masks generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fdabf-efbe-4819-86bd-9057fa417976",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=[\"ip-adapter-plus-face_sdxl_vit-h.safetensors\"])\n",
    "pipeline.set_ip_adapter_scale([[0.7, 0.7]])  # one scale for each image-mask pair\n",
    "\n",
    "face_image1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl1.png\")\n",
    "face_image2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl2.png\")\n",
    "\n",
    "ip_images = [[face_image1, face_image2]]\n",
    "\n",
    "masks = [masks.reshape(1, masks.shape[0], masks.shape[2], masks.shape[3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45104ecf-45fd-4f75-865c-82713a84e7c3",
   "metadata": {},
   "source": [
    "- Now pass the preprocessed masks to `cross_attention_kwargs` in the pipeline call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff52cbe-5ece-4d25-9307-9d3b6c0cf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "num_images = 1\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"2 girls\",\n",
    "    ip_adapter_image=ip_images,\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=20,\n",
    "    num_images_per_prompt=num_images,\n",
    "    generator=generator,\n",
    "    cross_attention_kwargs={\"ip_adapter_masks\": masks}\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e0e47-364f-428f-a4e6-1599fc72f90b",
   "metadata": {},
   "source": [
    "#### Face model\n",
    "- IP-Adapter’s image prompting and compatibility with other adapters and models makes it a versatile tool for a variety of use cases.\n",
    "\n",
    "- Generating accurate faces is challenging because they are complex and nuanced.\n",
    "- Diffusers supports two `IP-Adapter` checkpoints specifically trained to generate faces from the `h94/IP-Adapter` repository:\n",
    "  - `ip-adapter-full-face_sd15.safetensors` is conditioned with images of cropped faces and removed backgrounds\n",
    "  - `ip-adapter-plus-face_sd15.safetensors` uses patch embeddings and is conditioned with images of cropped faces\n",
    "- Diffusers supports all `IP-Adapter` checkpoints trained with face embeddings extracted by insightface face models.\n",
    "  - Supported models are from the `h94/IP-Adapter-FaceID` repository.\n",
    "\n",
    "- For face models, use the `h94/IP-Adapter` checkpoint.\n",
    "  - It is also recommended to use `DDIMScheduler` or `EulerDiscreteScheduler` for face models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f46a36-6353-41ec-8002-91ba44afc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter-full-face_sd15.bin\")\n",
    "\n",
    "pipeline.set_ip_adapter_scale(0.5)\n",
    "\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_einstein_base.png\")\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(26)\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"A photo of Einstein as a chef, wearing an apron, cooking in a French restaurant\",\n",
    "    ip_adapter_image=image,\n",
    "    negative_prompt=\"lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=100,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656392e2-34aa-432e-9e06-f8fd58636904",
   "metadata": {},
   "source": [
    "- To use `IP-Adapter` FaceID models, first extract face embeddings with insightface.\n",
    "  - Then pass the list of tensors to the pipeline as `ip_adapter_image_embeds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c3dc4-ebbd-44b0-a61a-652c33ee632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter-FaceID\", subfolder=None, weight_name=\"ip-adapter-faceid_sd15.bin\", image_encoder_folder=None)\n",
    "pipeline.set_ip_adapter_scale(0.6)\n",
    "\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl1.png\")\n",
    "\n",
    "ref_images_embeds = []\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "faces = app.get(image)\n",
    "image = torch.from_numpy(faces[0].normed_embedding)\n",
    "ref_images_embeds.append(image.unsqueeze(0))\n",
    "ref_images_embeds = torch.stack(ref_images_embeds, dim=0).unsqueeze(0)\n",
    "neg_ref_images_embeds = torch.zeros_like(ref_images_embeds)\n",
    "id_embeds = torch.cat([neg_ref_images_embeds, ref_images_embeds]).to(dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "images = pipeline(\n",
    "    prompt=\"A photo of a girl\",\n",
    "    ip_adapter_image_embeds=[id_embeds],\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=20, num_images_per_prompt=1,\n",
    "    generator=generator\n",
    ").images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123df33a-a0a1-4bbc-adf4-2c540bbe111d",
   "metadata": {},
   "source": [
    "- Both `IP-Adapter` FaceID Plus and `Plus v2` models require CLIP image embeddings.\n",
    "  - You can prepare face embeddings as shown previously, then you can extract and pass CLIP embeddings to the hidden image projection layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c85ae1-7c18-4b1e-80fd-8f4b4a7a5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_images_embeds = []\n",
    "ip_adapter_images = []\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n",
    "faces = app.get(image)\n",
    "ip_adapter_images.append(face_align.norm_crop(image, landmark=faces[0].kps, image_size=224))\n",
    "image = torch.from_numpy(faces[0].normed_embedding)\n",
    "ref_images_embeds.append(image.unsqueeze(0))\n",
    "ref_images_embeds = torch.stack(ref_images_embeds, dim=0).unsqueeze(0)\n",
    "neg_ref_images_embeds = torch.zeros_like(ref_images_embeds)\n",
    "id_embeds = torch.cat([neg_ref_images_embeds, ref_images_embeds]).to(dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "clip_embeds = pipeline.prepare_ip_adapter_image_embeds(\n",
    "  [ip_adapter_images], None, torch.device(\"cuda\"), num_images, True)[0]\n",
    "\n",
    "pipeline.unet.encoder_hid_proj.image_projection_layers[0].clip_embeds = clip_embeds.to(dtype=torch.float16)\n",
    "pipeline.unet.encoder_hid_proj.image_projection_layers[0].shortcut = False # True if Plus v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0b3ea-030f-4f19-bf10-bbb4f713aa0f",
   "metadata": {},
   "source": [
    "#### Multi IP-Adapter\n",
    "- More than one `IP-Adapter` can be used at the same time to generate specific images in more diverse styles.\n",
    "  - You can use `IP-Adapter-Face` to generate consistent faces and characters, and `IP-Adapter Plus` to generate those faces in a specific style.\n",
    "\n",
    "- Load the image encoder with `CLIPVisionModelWithProjection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786c6cd-f62e-4f41-b539-5bec1388049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f010b-5ef6-4717-aa8f-79fb467425a3",
   "metadata": {},
   "source": [
    "- Load a base model, scheduler, and the IP-Adapters.\n",
    "  - The IP-Adapters to use are passed as a list to the `weight_name` parameter:\n",
    "  - `ip-adapter-plus_sdxl_vit-h` uses patch embeddings and a ViT-H image encoder\n",
    "  - `ip-adapter-plus-face_sdxl_vit-h` has the same architecture but it is conditioned with images of cropped faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562be5c9-e9f6-48e4-a430-4e1d664cb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    image_encoder=image_encoder,\n",
    ")\n",
    "pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.load_ip_adapter(\n",
    "  \"h94/IP-Adapter\",\n",
    "  subfolder=\"sdxl_models\",\n",
    "  weight_name=[\"ip-adapter-plus_sdxl_vit-h.safetensors\", \"ip-adapter-plus-face_sdxl_vit-h.safetensors\"]\n",
    ")\n",
    "pipeline.set_ip_adapter_scale([0.7, 0.3])\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3108b8-200f-49b9-a3ac-cdd31d6232db",
   "metadata": {},
   "source": [
    "- Load an image prompt and a folder containing images of a certain style you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361cce38-1451-4fa6-abb5-2d5e2ed9c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/women_input.png\")\n",
    "style_folder = \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy\"\n",
    "style_images = [load_image(f\"{style_folder}/img{i}.png\") for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f6746-4976-4da6-881b-be4df9e7699b",
   "metadata": {},
   "source": [
    "- Pass the image prompt and style images as a list to the `ip_adapter_image` parameter, and run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b910b-44c8-46b2-87e0-2e15b717b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"wonderwoman\",\n",
    "    ip_adapter_image=[style_images, face_image],\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=50, num_images_per_prompt=1,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3cb81-5d06-4e09-bac8-b113737db179",
   "metadata": {},
   "source": [
    "#### Instant Generation\n",
    "- `Latent Consistency Models (LCM)` are diffusion models that can generate images in as little as 4 steps compared to other diffusion models like `SDXL` that typically require way more steps.\n",
    "  - This is why image generation with an `LCM` feels “instantaneous”.\n",
    "  - `IP-Adapters` can be plugged into an `LCM-LoRA` model to instantly generate images with an image prompt.\n",
    "\n",
    "- `IP-Adapter` weights need to be loaded first, then you can use `load_lora_weights()` to load the `LoRA` style and weight you want to apply to your image.\n",
    "\n",
    "- Try using with a lower `IP-Adapter` scale to condition image generation more on the `herge_style` checkpoint, and remember to use the special token `herge_style` in your prompt to trigger and apply the style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec05ee-1bfe-47e7-a515-814670a442cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sd-dreambooth-library/herge-style\"\n",
    "lcm_lora_id = \"latent-consistency/lcm-lora-sdv1-5\"\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n",
    "pipeline.load_lora_weights(lcm_lora_id)\n",
    "pipeline.scheduler = LCMScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.enable_model_cpu_offload()\n",
    "\n",
    "pipeline.set_ip_adapter_scale(0.4)\n",
    "\n",
    "prompt = \"herge_style woman in armor, best quality, high quality\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "\n",
    "ip_adapter_image = load_image(\"https://user-images.githubusercontent.com/24734142/266492875-2d50d223-8475-44f0-a7c6-08b51cb53572.png\")\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    ip_adapter_image=ip_adapter_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=1,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa0a6f8-7476-4981-87bf-fa78a9634639",
   "metadata": {},
   "source": [
    "#### Structural control\n",
    "- To control image generation to an even greater degree, you can combine `IP-Adapter` with a model like ControlNet.\n",
    "  - A `ControlNet` is also an adapter that can be inserted into a diffusion model to allow for conditioning on an additional control image.\n",
    "  - The control image can be depth maps, edge maps, pose estimations, and more.\n",
    "- Load a `ControlNetModel` checkpoint conditioned on depth maps, insert it into a diffusion model, and load the `IP-Adapter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6cc4f-15b7-4bed-b6f1-00aa9e997a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_model_path = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
    "\n",
    "pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16)\n",
    "pipeline.to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184ba6f-c1cb-4d12-8a26-0578d45d3d0d",
   "metadata": {},
   "source": [
    "- Load the IP-Adapter image and depth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36210c9-357d-4811-8632-fd8242355d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_adapter_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/statue.png\")\n",
    "depth_map = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/depth.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17668a9c-6aa5-4b2c-bbda-c15f42fddd93",
   "metadata": {},
   "source": [
    "- Pass the depth map and `IP-Adapter` image to the pipeline to generate an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f01258-3e18-4c55-8132-b71faf0ef5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "image = pipeline(\n",
    "    prompt=\"best quality, high quality\",\n",
    "    image=depth_map,\n",
    "    ip_adapter_image=ip_adapter_image,\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f60e3-22ef-4ce7-84df-88aa4e5e31cf",
   "metadata": {},
   "source": [
    "#### Style & Layout control\n",
    "- `InstantStyle` is a plug-and-play method on top of `IP-Adapter`, which disentangles style and layout from image prompt to control image generation.\n",
    "  - You can generate images following only the style or layout from image prompt, with significantly improved diversity.\n",
    "  - This is achieved by only activating `IP-Adapters` to specific parts of the model.\n",
    "  - By default `IP-Adapters` are inserted to all layers of the model.\n",
    "  - Use the `set_ip_adapter_scale()` method with a dictionary to assign scales to `IP-Adapter` at different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bafe4eb-e2ed-4c5b-a711-f8fda3f626d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "\n",
    "scale = {\n",
    "    \"down\": {\"block_2\": [0.0, 1.0]},\n",
    "    \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n",
    "}\n",
    "pipeline.set_ip_adapter_scale(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b3c56-397e-4b36-9f10-d0e277a83b6b",
   "metadata": {},
   "source": [
    "- This will activate `IP-Adapter` at the second layer in the model’s down-part block 2 and up-part block 0.\n",
    "  - The former is the layer where IP-Adapter injects layout information and the latter injects style.\n",
    "  - Inserting `IP-Adapter` to these two layers you can generate images following both the style and layout from image prompt, but with contents more aligned to text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd15b3-497d-4bb2-b172-f95246cf05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(26)\n",
    "image = pipeline(\n",
    "    prompt=\"a cat, masterpiece, best quality, high quality\",\n",
    "    ip_adapter_image=style_image,\n",
    "    negative_prompt=\"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n",
    "    guidance_scale=5,\n",
    "    num_inference_steps=30,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4588c2-a70b-4813-a774-c2569f0dd38f",
   "metadata": {},
   "source": [
    "- Inserting `IP-Adapter` to all layers will often generate images that overly focus on image prompt and diminish diversity.\n",
    "  - Activate `IP-Adapter` only in the style layer and then call the pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67120968-fb56-4a54-baff-164802bc959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = {\n",
    "    \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n",
    "}\n",
    "pipeline.set_ip_adapter_scale(scale)\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(26)\n",
    "image = pipeline(\n",
    "    prompt=\"a cat, masterpiece, best quality, high quality\",\n",
    "    ip_adapter_image=style_image,\n",
    "    negative_prompt=\"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n",
    "    guidance_scale=5,\n",
    "    num_inference_steps=30,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20253b9-15a8-4fa5-8666-6336e786b986",
   "metadata": {},
   "source": [
    "----\n",
    "### **OmniGen**\n",
    "- Unlike existing text-to-image models, `OmniGen` is a single model designed to handle a variety of tasks (e.g., text-to-image, image editing, controllable generation).\n",
    "  - Minimalist model architecture, consisting of only a VAE and a transformer module, for joint modeling of text and images.\n",
    "  - It can process any text-image mixed(multi-modal) data as instructions for image generation, rather than relying solely on text.\n",
    "  - For more information, please refer to the paper. This guide will walk you through using OmniGen for various tasks and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cebf8d21-8336-4cee-9c24-74628d6b5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import OmniGenPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e501b74-a2d6-4bc7-8dc7-d370637d9d64",
   "metadata": {},
   "source": [
    "- Model weights may be stored in separate subfolders on the Hub or locally, in which case, you should use the `from_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2922c3-ea54-47ed-97c9-c8339ce7a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OmniGenPipeline.from_pretrained(\"Shitao/OmniGen-v1-diffusers\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5c3b3-1c27-4439-bc3c-92220cc7587e",
   "metadata": {},
   "source": [
    "- `OmniGen` generates a 1024x1024 image.\n",
    "  - Try setting the height and width parameters to generate images with different size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa208ea-a137-4a20-9b7b-35566a0e2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OmniGenPipeline.from_pretrained(\n",
    "    \"Shitao/OmniGen-v1-diffusers\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"Realistic photo. A young woman sits on a sofa, holding a book and facing the camera. She wears delicate silver hoop earrings adorned with tiny, sparkling diamonds that catch the light, with her long chestnut hair cascading over her shoulders. Her eyes are focused and gentle, framed by long, dark lashes. She is dressed in a cozy cream sweater, which complements her warm, inviting smile. Behind her, there is a table with a cup of water in a sleek, minimalist blue mug. The background is a serene indoor setting with soft natural light filtering through a window, adorned with tasteful art and flowers, creating a cozy and peaceful ambiance. 4K, HD.\"\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(111),\n",
    ").images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351a3c9-b69d-4ea0-807d-9812cff9bf14",
   "metadata": {},
   "source": [
    "#### Image Edit\n",
    "- `OmniGen` supports multimodal inputs.\n",
    "  - When the input includes an image, you need to add a placeholder `<|image_1|>` in the text prompt to represent the image.\n",
    "  - It is recommended to `enable use_input_image_size_as_output` to keep the edited image the same size as the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfe2d0-a978-419d-bd7f-08785e070f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OmniGenPipeline.from_pretrained(\n",
    "    \"Shitao/OmniGen-v1-diffusers\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt=\"<img><|image_1|></img> Remove the woman's earrings. Replace the mug with a clear glass filled with sparkling iced cola.\"\n",
    "input_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/t2i_woman_with_book.png\")]\n",
    "image = pipe(\n",
    "    prompt=prompt, \n",
    "    input_images=input_images, \n",
    "    guidance_scale=2, \n",
    "    img_guidance_scale=1.6,\n",
    "    use_input_image_size_as_output=True,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(222)\n",
    ").images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff9a5d-5e36-46c4-9efe-e03e42ca84f5",
   "metadata": {},
   "source": [
    "- `OmniGen` has some interesting features, such as visual reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833a2e0-3fe0-4d1b-bd7a-3d6cd6b20063",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"If the woman is thirsty, what should she take? Find it in the image and highlight it in blue. <img><|image_1|></img>\"\n",
    "input_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/edit.png\")]\n",
    "image = pipe(\n",
    "    prompt=prompt, \n",
    "    input_images=input_images, \n",
    "    guidance_scale=2, \n",
    "    img_guidance_scale=1.6,\n",
    "    use_input_image_size_as_output=True,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9267f482-80bf-4bab-9324-1e4bd35e4400",
   "metadata": {},
   "source": [
    "#### Controllable Generation\n",
    "- `OmniGen` can handle several classic computer vision tasks.\n",
    "  - `OmniGen` can detect human skeletons in input images, which can be used as control conditions to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a1be2-61d6-4706-8b84-0299691a71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OmniGenPipeline.from_pretrained(\n",
    "    \"Shitao/OmniGen-v1-diffusers\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt=\"Detect the skeleton of human in this image: <img><|image_1|></img>\"\n",
    "input_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/edit.png\")]\n",
    "image1 = pipe(\n",
    "    prompt=prompt, \n",
    "    input_images=input_images, \n",
    "    guidance_scale=2, \n",
    "    img_guidance_scale=1.6,\n",
    "    use_input_image_size_as_output=True,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(333)\n",
    ").images[0]\n",
    "image1.save(\"image1.png\")\n",
    "\n",
    "prompt=\"Generate a new photo using the following picture and text as conditions: <img><|image_1|></img>\\n A young boy is sitting on a sofa in the library, holding a book. His hair is neatly combed, and a faint smile plays on his lips, with a few freckles scattered across his cheeks. The library is quiet, with rows of shelves filled with books stretching out behind him.\"\n",
    "input_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/skeletal.png\")]\n",
    "image2 = pipe(\n",
    "    prompt=prompt, \n",
    "    input_images=input_images, \n",
    "    guidance_scale=2, \n",
    "    img_guidance_scale=1.6,\n",
    "    use_input_image_size_as_output=True,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(333)\n",
    ").images[0]\n",
    "image2.save(\"image2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c9b0c-992f-44db-b86d-25800b1c0903",
   "metadata": {},
   "source": [
    "- `OmniGen` can also directly use relevant information from input images to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015dea6c-9e8a-43d3-8215-91fad3db929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OmniGenPipeline.from_pretrained(\n",
    "    \"Shitao/OmniGen-v1-diffusers\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt=\"Following the pose of this image <img><|image_1|></img>, generate a new photo: A young boy is sitting on a sofa in the library, holding a book. His hair is neatly combed, and a faint smile plays on his lips, with a few freckles scattered across his cheeks. The library is quiet, with rows of shelves filled with books stretching out behind him.\"\n",
    "input_images=[load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/edit.png\")]\n",
    "image = pipe(\n",
    "    prompt=prompt, \n",
    "    input_images=input_images, \n",
    "    guidance_scale=2, \n",
    "    img_guidance_scale=1.6,\n",
    "    use_input_image_size_as_output=True,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1af8d-2d69-4d7f-8a84-833bf4db861c",
   "metadata": {},
   "source": [
    "#### ID and Object Preserving\n",
    "- `OmniGen` can generate multiple images based on the people and objects in the input image and supports inputting multiple images simultaneously.\n",
    "  - `OmniGen` can extract desired objects from an image containing multiple objects based on instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e57fbc-f54a-4c3e-9b28-e3494f3c5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OmniGenPipeline.from_pretrained(\n",
    "    \"Shitao/OmniGen-v1-diffusers\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt=\"A man and a woman are sitting at a classroom desk. The man is the man with yellow hair in <img><|image_1|></img>. The woman is the woman on the left of <img><|image_2|></img>\"\n",
    "input_image_1 = load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/3.png\")\n",
    "input_image_2 = load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/4.png\")\n",
    "input_images=[input_image_1, input_image_2]\n",
    "image = pipe(\n",
    "    prompt=prompt, \n",
    "    input_images=input_images, \n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=2.5, \n",
    "    img_guidance_scale=1.6,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(666)\n",
    ").images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55185c12-2a54-4c0f-9942-d134b342b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OmniGenPipeline.from_pretrained(\n",
    "    \"Shitao/OmniGen-v1-diffusers\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt=\"A woman is walking down the street, wearing a white long-sleeve blouse with lace details on the sleeves, paired with a blue pleated skirt. The woman is <img><|image_1|></img>. The long-sleeve blouse and a pleated skirt are <img><|image_2|></img>.\"\n",
    "input_image_1 = load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/emma.jpeg\")\n",
    "input_image_2 = load_image(\"https://raw.githubusercontent.com/VectorSpaceLab/OmniGen/main/imgs/docs_img/dress.jpg\")\n",
    "input_images=[input_image_1, input_image_2]\n",
    "image = pipe(\n",
    "    prompt=prompt, \n",
    "    input_images=input_images, \n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=2.5, \n",
    "    img_guidance_scale=1.6,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(666)\n",
    ").images[0]\n",
    "image.save(\"output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92408caf-9b35-4568-830f-ea5b7f66dcbd",
   "metadata": {},
   "source": [
    "#### Optimization when Using Multiple Images\n",
    "- For text-to-image task, `OmniGen` requires minimal memory and time costs (9GB memory and 31s for a 1024x1024 image on A800 GPU).\n",
    "  - However, when using input images, the computational cost increases.\n",
    "\n",
    "- Like other pipelines, you can reduce memory usage by offloading the model: `pipe.enable_model_cpu_offload()` or `pipe.enable_sequential_cpu_offload()`.\n",
    "  - Decrease computational overhead by reducing the `max_input_image_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb739d70-1401-4c98-9e62-edd986c8836c",
   "metadata": {},
   "source": [
    "----\n",
    "### **Perturbed-Attention Guidance(PAG)**\n",
    "- `Perturbed-Attention Guidance(PAG)` is a new diffusion sampling guidance that improves sample quality across both unconditional and conditional settings, achieving this without requiring further training or the integration of external modules.\n",
    "  - PAG is designed to progressively enhance the structure of synthesized samples throughout the denoising process by considering the self-attention mechanisms’ ability to capture structural information.\n",
    "  - It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, and guiding the denoising process away from these degraded samples.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **General tasks**\n",
    "  - You can apply `PAG` to the `StableDiffusionXLPipeline` for tasks such as text-to-image, image-to-image, and inpainting.\n",
    "  - To enable `PAG` for a specific task, load the pipeline using the AutoPipeline API with the `enable_pag=True` flag and the `pag_applied_layers` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55274e3a-f183-4cca-84b0-733e0aa8ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image, ControlNetModel\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1678d4-19cc-44f7-a18e-988d3cdd8491",
   "metadata": {},
   "source": [
    "- The `pag_applied_layers` argument allows you to specify which layers `PAG` is applied to.\n",
    "  - Use `set_pag_applied_layers` method to update these layers after the pipeline has been created.\n",
    "  - Check out the `pag_applied_layers` section to learn more about applying `PAG` to other layers.\n",
    "- If you already have a pipeline created and loaded, you can enable PAG on it using the `from_pipe` API with the `enable_pag` flag.\n",
    "  - PAG pipeline is created based on the pipeline and task you specified.\n",
    "  - Since we used `AutoPipelineForText2Image` and passed a `StableDiffusionXLPipeline`, a `StableDiffusionXLPAGPipeline` is created accordingly.\n",
    "  - Note that this does not require additional memory, and you will have both `StableDiffusionXLPipeline` and `StableDiffusionXLPAGPipeline` loaded and ready to use.\n",
    "  - You can read more about the `from_pipe` API and how to reuse pipelines in diffuser here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6347ba9-09ab-410f-9a81-89564473d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    enable_pag=True,\n",
    "    pag_applied_layers=[\"mid\"],\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76278bbe-8b7e-4436-b044-e6e54572d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sdxl = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\n",
    "pipeline = AutoPipelineForText2Image.from_pipe(pipeline_sdxl, enable_pag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d24174-10a6-4c53-bfca-83e7b0cde3b3",
   "metadata": {},
   "source": [
    "- To generate an image, you will also need to pass a `pag_scale`.\n",
    "  - When `pag_scale` increases, images gain more semantically coherent structures and exhibit fewer artifacts.\n",
    "  - However overly large guidance scale can lead to smoother textures and slight saturation in the images, similarly to CFG.\n",
    "  - `pag_scale=3.0` is used in the official demo and works well in most of the use cases, but feel free to experiment and select the appropriate value according to your needs\n",
    "    - `PAG` is disabled when `pag_scale=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322dfa4-18ea-40ea-95c9-0d9499d69cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"an insect robot preparing a delicious meal, anime style\"\n",
    "\n",
    "for pag_scale in [0.0, 3.0]:\n",
    "    generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "    images = pipeline(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.0,\n",
    "        generator=generator,\n",
    "        pag_scale=pag_scale,\n",
    "    ).images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061df7d0-28a0-492a-895e-5b746bf4e5c4",
   "metadata": {},
   "source": [
    "#### `PAG` with `ControlNet`\n",
    "- To use `PAG` with `ControlNet`, first create a `controlnet`.\n",
    "  - Pass the controlnet and other PAG arguments to the `from_pretrained` method of the `AutoPipeline` for the specified task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90d743-f2c8-43ba-8b34-117ada2a2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    enable_pag=True,\n",
    "    pag_applied_layers=\"mid\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616813a-08b0-4edc-b27b-f2d4178b97be",
   "metadata": {},
   "source": [
    "- If you already have a controlnet pipeline and want to enable `PAG`, you can use the `from_pipe` API: `AutoPipelineForText2Image.from_pipe(pipeline_controlnet, enable_pag=True)`\n",
    "\n",
    "- You can use the pipeline in the same way you normally use `ControlNet` pipelines, with the added option to specify a `pag_scale` parameter.\n",
    "  - Note that `PAG` works well for unconditional generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39276c-da52-41b0-9e4e-bfb1d8b1f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "canny_image = load_image(\n",
    "    \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/pag_control_input.png\"\n",
    ")\n",
    "\n",
    "for pag_scale in [0.0, 3.0]:\n",
    "    generator = torch.Generator(device=\"cpu\").manual_seed(1)\n",
    "    images = pipeline(\n",
    "        prompt=\"\",\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        image=canny_image,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=0,\n",
    "        generator=generator,\n",
    "        pag_scale=pag_scale,\n",
    "    ).images\n",
    "    images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ae0aa-8ff3-4545-8010-904e24cb172a",
   "metadata": {},
   "source": [
    "#### `PAG` with `IP-Adapter`\n",
    "- `IP-Adapter` is a popular model that can be plugged into diffusion models to enable image prompting without any changes to the underlying model.\n",
    "  - You can enable `PAG` on a pipeline with `IP-Adapter` loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26a10b-36a0-4ee2-bfac-079e9b59ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    image_encoder=image_encoder,\n",
    "    enable_pag=True,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter-plus_sdxl_vit-h.bin\")\n",
    "\n",
    "pag_scales = 5.0\n",
    "ip_adapter_scales = 0.8\n",
    "\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\")\n",
    "\n",
    "pipeline.set_ip_adapter_scale(ip_adapter_scale)\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "images = pipeline(\n",
    "    prompt=\"a polar bear sitting in a chair drinking a milkshake\",\n",
    "    ip_adapter_image=image,\n",
    "    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=3.0,\n",
    "    generator=generator,\n",
    "    pag_scale=pag_scale,\n",
    ").images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5918a-1ab6-4699-8c3c-4bd5255361d9",
   "metadata": {},
   "source": [
    "#### Configure parameters\n",
    "- The `pag_applied_layers` argument allows you to specify which layers `PAG` is applied to.\n",
    "  - By default, it applies only to the mid blocks.\n",
    "    - Changing this setting will significantly impact the output.\n",
    "  - You can use the `set_pag_applied_layers` method to adjust the `PAG` layers after the pipeline is created, helping you find the optimal layers for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383ede8-eb46-4851-82ec-1d352df0c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"an insect robot preparing a delicious meal, anime style\"\n",
    "pipeline.set_pag_applied_layers(pag_layers)\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "images = pipeline(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=generator,\n",
    "    pag_scale=pag_scale,\n",
    ").images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f498e02-0e67-4bc1-a3f4-80a76efbf931",
   "metadata": {},
   "source": [
    "----\n",
    "### **ControlNet**\n",
    "- `ControlNet` is a type of model for controlling image diffusion models by conditioning the model with an additional input image.\n",
    "  - There are many types of conditioning inputs (canny edge, user sketching, human pose, depth, and more) you can use to control a diffusion model.\n",
    "  - This is hugely useful because it affords you greater control over image generation, making it easier to generate specific images without experimenting with different text prompts or denoising values as much.\n",
    "\n",
    "- A `ControlNet` model has two sets of weights (or blocks) connected by a zero-convolution layer:\n",
    "  - a locked copy keeps everything a large pretrained diffusion model has learned\n",
    "  - a trainable copy is trained on the additional conditioning input\n",
    "\n",
    "- Since the locked copy preserves the pretrained model, training and implementing a `ControlNet` on a new conditioning input is as fast as finetuning any other model because you aren’t training the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87844b2-82fa-4e82-9a16-48fc8c53ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers import StableDiffusionControlNetImg2ImgPipeline\n",
    "from diffusers import StableDiffusionControlNetInpaintPipeline\n",
    "\n",
    "from diffusers import StableDiffusionXLControlNetPipeline, AutoencoderKL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1859f-1328-4558-9008-e2b384db6552",
   "metadata": {},
   "source": [
    "- For text-to-image, you normally pass a text prompt to the model.\n",
    "  - But with `ControlNet`, you can specify an additional conditioning input.\n",
    "  - Let’s condition the model with a canny image, a white outline of an image on a black background.\n",
    "  - `ControlNet` can use the canny image as a control to guide the model to generate an image with the same outline.\n",
    "\n",
    "- Load an image and use the opencv-python library to extract the canny image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbbb12-dc87-4ac2-b6cb-f05513d4547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ")\n",
    "\n",
    "image = np.array(original_image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f1806-7f7e-49df-b519-800b28243d67",
   "metadata": {},
   "source": [
    "- Load a `ControlNet` model conditioned on canny edge detection and pass it to the `StableDiffusionControlNetPipeline`.\n",
    "  - Use the faster `UniPCMultistepScheduler` and enable model offloading to speed up inference and reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce00b98-2720-4088-ac7a-016d6919fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc55caa-8908-46d1-9d5a-58c09397df77",
   "metadata": {},
   "source": [
    "- Pass your prompt and canny image to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94231680-e59e-4630-89cc-deab4e219774",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(\n",
    "    \"the mona lisa\", image=canny_image\n",
    ").images[0]\n",
    "make_image_grid([original_image, canny_image, output], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f65e0-e5c2-4c60-8982-8af391696662",
   "metadata": {},
   "source": [
    "#### ControlNet with Stable Diffusion XL\n",
    "- We’ve trained two full-sized `ControlNet` models for `SDXL` conditioned on canny edge detection and depth maps.\n",
    "  - We’re also experimenting with creating smaller versions of these `SDXL`-compatible ControlNet models so it is easier to run on resource-constrained hardware.\n",
    "\n",
    "- Use a `SDXL ControlNet` conditioned on canny images to generate an image.\n",
    "  - Start by loading an image and prepare the canny image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ffbd2-9258-49c9-ac98-5f1b444a5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = load_image(\n",
    "    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\"\n",
    ")\n",
    "\n",
    "image = np.array(original_image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "make_image_grid([original_image, canny_image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9f0f1-1cc9-4dcf-8768-28559b11de2e",
   "metadata": {},
   "source": [
    "- Load a `SDXL ControlNet` model conditioned on canny edge detection and pass it to the `StableDiffusionXLControlNetPipeline`.\n",
    "  - You can also enable model offloading to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d90fae-f8d7-4db6-b8dc-c06b8c3d00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aacafa-5fdf-4e54-867f-7d46f24398f3",
   "metadata": {},
   "source": [
    "- Pass your prompt (and optionally a negative prompt if you’re using one) and canny image to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7d957-2225-4d7b-8147-fa64870e2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n",
    "negative_prompt = 'low quality, bad quality, sketches'\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=canny_image,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    ").images[0]\n",
    "make_image_grid([original_image, canny_image, image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc71d1d-78d0-4dbe-be67-f837fccdfdbb",
   "metadata": {},
   "source": [
    "- Use `StableDiffusionXLControlNetPipeline` in guess mode as well by setting the parameter to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3b78a-bd57-4f77-a4f9-1714c6b95231",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n",
    "negative_prompt = \"low quality, bad quality, sketches\"\n",
    "\n",
    "original_image = load_image(\n",
    "    \"https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\"\n",
    ")\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "image = np.array(original_image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "image = pipe(\n",
    "    prompt, negative_prompt=negative_prompt, controlnet_conditioning_scale=0.5, image=canny_image, guess_mode=True,\n",
    ").images[0]\n",
    "make_image_grid([original_image, canny_image, image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a080134-5e16-40dc-8566-2e363ea8eefd",
   "metadata": {},
   "source": [
    "#### MultiControlNet\n",
    "- Compose multiple `ControlNet` conditionings from different image inputs to create a `MultiControlNet`.\n",
    "  - Mask conditionings such that they don’t overlap\n",
    "  - Experiment with the `controlnet_conditioning_scale` parameter to determine how much weight to assign to each conditioning input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733235b-69b1-4d9a-9dfc-76ac11c286bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n",
    ")\n",
    "image = np.array(original_image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "\n",
    "# zero out middle columns of image where pose will be overlaid\n",
    "zero_start = image.shape[1] // 4\n",
    "zero_end = zero_start + image.shape[1] // 2\n",
    "image[:, zero_start:zero_end] = 0\n",
    "\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "make_image_grid([original_image, canny_image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c0aa5-d931-4d19-81d2-c574366e72f0",
   "metadata": {},
   "source": [
    "- Load a list of `ControlNet` models that correspond to each conditioning, and pass them to the `StableDiffusionXLControlNetPipeline`.\n",
    "  - Use the faster `UniPCMultistepScheduler` and enable model offloading to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ae6c0-ff0f-4861-8ee8-14dd210219c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnets = [\n",
    "    ControlNetModel.from_pretrained(\n",
    "        \"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16\n",
    "    ),\n",
    "    ControlNetModel.from_pretrained(\n",
    "        \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnets, vae=vae, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999672e8-57cd-4eea-99e2-5b238e51e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a giant standing in a fantasy landscape, best quality\"\n",
    "negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "generator = torch.manual_seed(1)\n",
    "\n",
    "images = [openpose_image.resize((1024, 1024)), canny_image.resize((1024, 1024))]\n",
    "\n",
    "images = pipe(\n",
    "    prompt,\n",
    "    image=images,\n",
    "    num_inference_steps=25,\n",
    "    generator=generator,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_images_per_prompt=3,\n",
    "    controlnet_conditioning_scale=[1.0, 0.8],\n",
    ").images\n",
    "make_image_grid([original_image, canny_image, openpose_image,\n",
    "                images[0].resize((512, 512)), images[1].resize((512, 512)), images[2].resize((512, 512))], rows=2, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fcac4-4200-4c15-bb86-75436565ae54",
   "metadata": {},
   "source": [
    "----\n",
    "### **T2I-Adapter**\n",
    "- `T2I-Adapter` is a lightweight adapter for controlling and providing more accurate structure guidance for text-to-image models.\n",
    "  - It works by learning an alignment between the internal knowledge of the text-to-image model and an external control signal, such as edge detection or depth estimation.\n",
    "\n",
    "- The `T2I-Adapter` design is simple, the condition is passed to four feature extraction blocks and three downsample blocks.\n",
    "  - This makes it fast and easy to train different adapters for different conditions which can be plugged into the text-to-image model.\n",
    "  - `T2I-Adapter` is similar to `ControlNet` except it is smaller (~77M parameters) and faster because it only runs once during the diffusion process.\n",
    "  - The downside is that performance may be slightly worse than `ControlNet`.\n",
    "\n",
    "- Make sure you have the following libraries installed.\n",
    "```\n",
    "pip install -q diffusers accelerate controlnet-aux==0.0.7\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Text-to-image models rely on a prompt to generate an image, but sometimes, text alone may not be enough to provide more accurate structural guidance.\n",
    "  - `T2I-Adapter` allows you to provide an additional control image to guide the generation process.\n",
    "  - ex. you can provide a canny image (a white outline of an image on a black background) to guide the model to generate an image with a similar structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d3968-6fc8-4cda-b8b6-8087ae6617e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionAdapterPipeline, T2IAdapter\n",
    "from controlnet_aux.canny import CannyDetector\n",
    "from diffusers import StableDiffusionXLAdapterPipeline, EulerAncestralDiscreteScheduler, AutoencoderKL\n",
    "from diffusers import MultiAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8961d-0d6c-4337-b099-7549894a8419",
   "metadata": {},
   "source": [
    "#### Stable Diffusion 1.5\n",
    "- Create a canny image with the opencv-library.\n",
    "- Load a `T2I-Adapter` conditioned on canny images and pass it to the `StableDiffusionAdapterPipeline`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c1203-7e1d-4983-b012-92623ceeef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb02508-967d-4ba8-8134-f49c6f61f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_canny_sd15v2\", torch_dtype=torch.float16)\n",
    "pipeline = StableDiffusionAdapterPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    adapter=adapter,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd9bb6-b718-45f1-bb34-bb21fdb425db",
   "metadata": {},
   "source": [
    "- Pass your prompt and control image to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68048fc3-7fc9-4cb5-b2b6-dfe59abc9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(\"cuda\").manual_seed(0)\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"cinematic photo of a plush and soft midcentury style rug on a wooden floor, 35mm photograph, film, professional, 4k, highly detailed\",\n",
    "    image=image,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0c1ed-438b-49cd-939e-f5d4f629caa7",
   "metadata": {},
   "source": [
    "#### Stable Diffusion XL\n",
    "- Create a canny image with the `controlnet-aux` library.\n",
    "- Load a `T2I-Adapter` conditioned on canny images and pass it to the `StableDiffusionXLAdapterPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e1c42-8452-490b-b481-534acfb59f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "canny_detector = CannyDetector()\n",
    "\n",
    "image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\n",
    "image = canny_detector(image, detect_resolution=384, image_resolution=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ca584-1e0c-424a-a5ab-d25b05cc88a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "adapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16)\n",
    "pipeline = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    adapter=adapter,\n",
    "    vae=vae,\n",
    "    scheduler=scheduler,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255137f-6aed-4ca2-81a7-4003c4d92c83",
   "metadata": {},
   "source": [
    "- Pass your prompt and control image to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7af6ba-4c0b-4ccb-a2fd-fdcc4e675d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(\"cuda\").manual_seed(0)\n",
    "\n",
    "image = pipeline(\n",
    "  prompt=\"cinematic photo of a plush and soft midcentury style rug on a wooden floor, 35mm photograph, film, professional, 4k, highly detailed\",\n",
    "  image=image,\n",
    "  generator=generator,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d736b5-206a-4b23-b026-7760c0eead5b",
   "metadata": {},
   "source": [
    "#### MultiAdapter\n",
    "- `T2I-Adapters` are also composable, allowing you to use more than one adapter to impose multiple control conditions on an image.\n",
    "  - ex. you can use a pose map to provide structural control and a depth map for depth control.\n",
    "    - This is enabled by the `MultiAdapter` class.\n",
    "\n",
    "- Condition a text-to-image model with a pose and depth adapter.\n",
    "  - Create and place your depth and pose image and in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de7d0e-a0e6-4484-a54e-c365798d4b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_image = load_image(\n",
    "    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_input.png\"\n",
    ")\n",
    "depth_image = load_image(\n",
    "    \"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_input.png\"\n",
    ")\n",
    "cond = [pose_image, depth_image]\n",
    "prompt = [\"Santa Claus walking into an office room with a beautiful city view\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b7ebe-ca39-4ebc-9fce-773403d8d521",
   "metadata": {},
   "source": [
    "- Load the corresponding pose and depth adapters as a list in the `MultiAdapter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab0a7c-be47-48af-87d4-b4cb58c946b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters = MultiAdapter(\n",
    "    [\n",
    "        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_keypose_sd14v1\"),\n",
    "        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_depth_sd14v1\"),\n",
    "    ]\n",
    ")\n",
    "adapters = adapters.to(torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c3b44-d4c6-43f0-a86f-36da85a8a0e4",
   "metadata": {},
   "source": [
    "- Load a `StableDiffusionAdapterPipeline` with the adapters, and pass your prompt and conditioned images to it.\n",
    "  - Use the `adapter_conditioning_scale` to adjust the weight of each adapter on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3dc84-2c9b-4d83-be24-8cb29ec34f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionAdapterPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    torch_dtype=torch.float16,\n",
    "    adapter=adapters,\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipeline(prompt, cond, adapter_conditioning_scale=[0.7, 0.7]).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d4471-2c01-4091-9ca0-4dc199040807",
   "metadata": {},
   "source": [
    "----\n",
    "### **Latent Consistency Model**\n",
    "- `Latent Consistency Models (LCMs)` enable fast high-quality image generation by directly predicting the reverse diffusion process in the latent rather than pixel space.\n",
    "  - `LCMs` try to predict the noiseless image from the noisy image in contrast to typical diffusion models that iteratively remove noise from the noisy image.\n",
    "  - By avoiding the iterative sampling process, `LCMs` are able to generate high-quality images in 2-4 steps instead of 20-30 steps.\n",
    "\n",
    "- `LCMs` are distilled from pretrained models which requires ~32 hours of A100 compute.\n",
    "  - To speed this up, `LCM-LoRAs` train a `LoRA` adapter which have much fewer parameters to train compared to the full model.\n",
    "  - The `LCM-LoRA` can be plugged into a diffusion model once it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10911d4d-2a5d-46d4-a43d-12e772f37fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8bf54-dc16-48d6-bad0-07f01e94ba17",
   "metadata": {},
   "source": [
    "- To use `LCMs`, you need to load the `LCM` checkpoint for your supported model into `UNet2DConditionModel` and replace the scheduler with the `LCMScheduler`.\n",
    "\n",
    "- Typically, batch size is doubled inside the pipeline for classifier-free guidance.\n",
    "  - But `LCM` applies guidance with guidance embeddings and doesn’t need to double the batch size, which leads to faster inference.\n",
    "  - The downside is that negative prompts don’t work with `LCM` because they don’t have any effect on the denoising process.\n",
    "- The ideal range for `guidance_scale` is `[3., 13.]` because that is what the `UNet` was trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985668d-05fe-421b-a507-28d9fe9bfa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"latent-consistency/lcm-sdxl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=8.0\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ace5db-fa0f-4bb5-a52f-3b0d1695f3ea",
   "metadata": {},
   "source": [
    "- To use `LCM-LoRAs`, you need to replace the scheduler with the `LCMScheduler` and load the `LCM-LoRA` weights with the `load_lora_weights()` method.\n",
    "- Typically, batch size is doubled inside the pipeline for classifier-free guidance.\n",
    "  - But `LCM` applies guidance with guidance embeddings and doesn’t need to double the batch size, which leads to faster inference.\n",
    "  - The downside is that negative prompts don’t work with `LCM` because they don’t have any effect on the denoising process.\n",
    "  - You could use guidance with `LCM-LoRAs`, but it is very sensitive to high `guidance_scale` values and can lead to artifacts in the generated image.\n",
    "  - The best values we’ve found are between `[1.0, 2.0]`.\n",
    "\n",
    "- Replace `stabilityai/stable-diffusion-xl-base-1.0` with any finetuned model.\n",
    "  - Try using the animagine-xl checkpoint to generate anime images with `SDXL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2ab6b-22bd-45b4-bc2b-7e6655df6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\n",
    "\n",
    "prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n",
    "generator = torch.manual_seed(42)\n",
    "image = pipe(\n",
    "    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85a029-1ccb-4f03-83c5-70c684d54399",
   "metadata": {},
   "source": [
    "#### Adapters: LoRA\n",
    "- `LCMs` are compatible with adapters like `LoRA`, `ControlNet`, `T2I-Adapter`, and `AnimateDiff`.\n",
    "  - You can bring the speed of `LCMs` to these adapters to generate images in a certain style or condition the model on another input like a canny image.\n",
    "\n",
    "- `LoRA` adapters can be rapidly finetuned to learn a new style from just a few images and plugged into a pretrained model to generate images in that style.\n",
    "  - Load the `LCM` checkpoint for your supported model into `UNet2DConditionModel` and replace the scheduler with the `LCMScheduler`.\n",
    "  - Use the `load_lora_weights()` method to load the LoRA weights into the `LCM` and generate a styled image in a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c59a0c-ed50-40cb-9761-fb596f24aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"latent-consistency/lcm-sdxl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=unet, torch_dtype=torch.float16, variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\n",
    "\n",
    "prompt = \"papercut, a cute fox\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=8.0\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eae0c4-288e-4c8f-8a1f-79685477e5db",
   "metadata": {},
   "source": [
    "#### Adapters: ControlNet\n",
    "- `ControlNet` are adapters that can be trained on a variety of inputs like canny edge, pose estimation, or depth.\n",
    "- `ControlNet` can be inserted into the pipeline to provide additional conditioning and control to the model for more accurate generation.\n",
    "\n",
    "- You can find additional ControlNet models trained on other inputs in lllyasviel’s repository.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Load a `ControlNet` model trained on canny images and pass it to the `ControlNetModel`.\n",
    "  - Then you can load a `LCM` model into `StableDiffusionControlNetPipeline` and replace the scheduler with the `LCMScheduler`.\n",
    "  - Now pass the canny image to the pipeline and generate an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c2075-3b26-45ec-8543-978ad29f6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ").resize((512, 512))\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"SimianLuo/LCM_Dreamshaper_v7\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    \"the mona lisa\",\n",
    "    image=canny_image,\n",
    "    num_inference_steps=4,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "make_image_grid([canny_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93469cc5-33b3-409e-a681-68bd08dc8161",
   "metadata": {},
   "source": [
    "#### Adapters: T2I-Adapter\n",
    "- `T2I-Adapter` is an even more lightweight adapter than `ControlNet`, that provides an additional input to condition a pretrained model with.\n",
    "  - It is faster than `ControlNet` but the results may be slightly worse.\n",
    "\n",
    "- Load a `T2IAdapter` trained on canny images and pass it to the `StableDiffusionXLAdapterPipeline`.\n",
    "  - Load a LCM checkpoint into `UNet2DConditionModel` and replace the scheduler with the `LCMScheduler`.\n",
    "    - Pass the canny image to the pipeline and generate an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35eda0-1323-4b9c-a057-a72cc3eb3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the canny map in low resolution to avoid high-frequency details\n",
    "image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ").resize((384, 384))\n",
    "\n",
    "image = np.array(image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image).resize((1024, 1216))\n",
    "\n",
    "adapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"latent-consistency/lcm-sdxl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    unet=unet,\n",
    "    adapter=adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"the mona lisa, 4k picture, high quality\"\n",
    "negative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=canny_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=5,\n",
    "    adapter_conditioning_scale=0.8,\n",
    "    adapter_conditioning_factor=1,\n",
    "    generator=generator,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4fe46-a3d9-4081-b41d-8f03e6198e7c",
   "metadata": {},
   "source": [
    "----\n",
    "### **Textual inversion**\n",
    "- The `StableDiffusionPipeline` supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images.\n",
    "  - This gives you more control over the generated images and allows you to tailor the model towards specific concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3863ab5-e824-4e2e-8163-c0a86626aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2f93e-a7ce-4891-8281-11bf0ba337ac",
   "metadata": {},
   "source": [
    "#### Stable Diffusion 1 and 2\n",
    "- Pick a Stable Diffusion checkpoint and a pre-learned concept from the `Stable Diffusion Conceptualizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff788c-5497-4422-a487-4da04e920600",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "repo_id_embeds = \"sd-concepts-library/cat-toy\"\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path, torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline.load_textual_inversion(repo_id_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99794574-ecc1-4f8d-9812-e1e607dabe17",
   "metadata": {},
   "source": [
    "- Create a prompt with the pre-learned concept by using the special placeholder token `<cat-toy>`, and choose the number of samples and rows of images you’d like to generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52deb383-2877-49e6-857c-a6f1fc81b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a grafitti in a favela wall with a <cat-toy> on it\"\n",
    "\n",
    "num_samples_per_row = 2\n",
    "num_rows = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf64979-a777-4250-9215-abb91ceed390",
   "metadata": {},
   "source": [
    "- Then run the pipeline (feel free to adjust the parameters like `num_inference_steps` and `guidance_scale` to see how they affect image quality), save the generated images and visualize them with the helper function you created at the beginning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f16a5-cc36-4bb5-a3a2-83947eac0c91",
   "metadata": {},
   "source": [
    "all_images = []\n",
    "for _ in range(num_rows):\n",
    "    images = pipeline(prompt, num_images_per_prompt=num_samples_per_row, num_inference_steps=50, guidance_scale=7.5).images\n",
    "    all_images.extend(images)\n",
    "\n",
    "grid = make_image_grid(all_images, num_rows, num_samples_per_row)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3b196-dee7-4736-b5b8-fc210b04c6bf",
   "metadata": {},
   "source": [
    "#### Stable Diffusion XL\n",
    "- `Stable Diffusion XL (SDXL)` can also use textual inversion vectors for inference.\n",
    "  - In contrast to Stable Diffusion 1 and 2, SDXL has two text encoders so you’ll need two textual inversion embeddings - one for each text encoder model.\n",
    " \n",
    "- Download the `SDXL` textual inversion embeddings and have a closer look at it’s structure:\n",
    "  - There are two tensors, `\"clip_g\"` and `\"clip_l\"`. `\"clip_g\"` corresponds to the bigger text encoder in `SDXL` and refers to `pipe.text_encoder_2` and `\"clip_l\"` refers to `pipe.text_encoder`.\n",
    "  - Load each tensor separately by passing them along with the correct text encoder and tokenizer to `load_textual_inversion()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04773fb7-9405-4d1e-a944-1cf9ddf8ca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clip_g': tensor([[ 0.0077, -0.0112,  0.0065,  ...,  0.0195,  0.0159,  0.0275],\n",
       "         [ 0.0320, -0.0239,  0.0241,  ..., -0.0164,  0.0284, -0.0135],\n",
       "         [-0.0303,  0.0069, -0.0071,  ...,  0.0100, -0.0251,  0.0164],\n",
       "         ...,\n",
       "         [ 0.0136, -0.0042,  0.0027,  ..., -0.0277, -0.0232, -0.0380],\n",
       "         [ 0.0121, -0.0066,  0.0176,  ..., -0.0292,  0.0065, -0.0139],\n",
       "         [-0.0170,  0.0213,  0.0143,  ..., -0.0302, -0.0240, -0.0362]],\n",
       "        dtype=torch.float16),\n",
       " 'clip_l': tensor([[ 0.0023,  0.0192,  0.0213,  ..., -0.0385,  0.0048, -0.0011],\n",
       "         [-0.0079, -0.0240,  0.0062,  ..., -0.0042,  0.0103,  0.0328],\n",
       "         [ 0.0096,  0.0127,  0.0181,  ..., -0.0076, -0.0272, -0.0204],\n",
       "         ...,\n",
       "         [ 0.0210,  0.0003,  0.0207,  ...,  0.0063, -0.0131,  0.0299],\n",
       "         [ 0.0160, -0.0136,  0.0269,  ...,  0.0242,  0.0356, -0.0205],\n",
       "         [ 0.0475, -0.0508, -0.0145,  ...,  0.0070, -0.0089, -0.0163]],\n",
       "        dtype=torch.float16)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = hf_hub_download(\"dn118/unaestheticXL\", filename=\"unaestheticXLv31.safetensors\")\n",
    "state_dict = load_file(file)\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba4703-8a0c-4ac5-9168-3d3a9785fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", variant=\"fp16\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "pipe.load_textual_inversion(state_dict[\"clip_g\"], token=\"unaestheticXLv31\", text_encoder=pipe.text_encoder_2, tokenizer=pipe.tokenizer_2)\n",
    "pipe.load_textual_inversion(state_dict[\"clip_l\"], token=\"unaestheticXLv31\", text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)\n",
    "\n",
    "# the embedding should be used as a negative embedding, so we pass it as a negative prompt\n",
    "generator = torch.Generator().manual_seed(33)\n",
    "image = pipe(\"a woman standing in front of a mountain\", negative_prompt=\"unaestheticXLv31\", generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1c615-7139-4d38-9d7a-2aff3bab7b59",
   "metadata": {},
   "source": [
    "----\n",
    "### **DiffEdit**\n",
    "- **Image editing** typically requires providing a mask of the area to be edited.\n",
    "  - DiffEdit automatically generates the mask for you based on a text query, making it easier overall to create a mask without image editing software.\n",
    "- The DiffEdit algorithm works in three steps:\n",
    "  - the diffusion model denoises an image conditioned on some query text and reference text which produces different noise estimates for different areas of the image; the difference is used to infer a mask to identify which area of the image needs to be changed to match the query text\n",
    "  - the input image is encoded into latent space with DDIM\n",
    "  - the latents are decoded with the diffusion model conditioned on the text query, using the mask as a guide such that pixels outside the mask remain the same as in the input image\n",
    "\n",
    "- The `StableDiffusionDiffEditPipeline` requires an image mask and a set of partially inverted latents.\n",
    "  - The image mask is generated from the `generate_mask()` function, and includes two parameters, `source_prompt` and `target_prompt`.\n",
    "    - These parameters determine what to edit in the image.\n",
    "    - ex. if you want to change a bowl of fruits to a bowl of pears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e7aed-1b8c-4f9a-82b9-77ac28dda4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2bfdf4-70c6-4aad-b3c6-9c9e4a057e76",
   "metadata": {},
   "source": [
    "- The partially inverted latents are generated from the `invert()` function, and it is generally a good idea to include a prompt or caption describing the image to help guide the inverse latent sampling process.\n",
    "- Let’s load the pipeline, scheduler, inverse scheduler, and enable some optimizations to reduce memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94f38e-aa7a-4e9c-a9f4-72318cd7bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_prompt = \"a bowl of fruits\"\n",
    "target_prompt = \"a bowl of pears\"\n",
    "\n",
    "pipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_vae_slicing()\n",
    "\n",
    "img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n",
    "raw_image = load_image(img_url).resize((768, 768))\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbe3a4-0c39-4281-a743-94f4b1a01357",
   "metadata": {},
   "source": [
    "- Use the `generate_mask()` function to generate the image mask.\n",
    "  - You’ll need to pass it the `source_prompt` and `target_prompt` to specify what to edit in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154bfba-33a5-4045-a48a-22b1bea68ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_prompt = \"a bowl of fruits\"\n",
    "target_prompt = \"a basket of pears\"\n",
    "mask_image = pipeline.generate_mask(\n",
    "    image=raw_image,\n",
    "    source_prompt=source_prompt,\n",
    "    target_prompt=target_prompt,\n",
    ")\n",
    "Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\").resize((768, 768))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6cc8f-1f03-40a1-a8fc-09fd927ec99c",
   "metadata": {},
   "source": [
    "- Create the inverted latents and pass it a caption describing the image:\n",
    "  - Pass the image mask and inverted latents to the pipeline.\n",
    "  - The `target_prompt` becomes the prompt now, and the `source_prompt` is used as the `negative_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a460d583-6ff6-41ce-b731-ff9026052879",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_latents = pipeline.invert(prompt=source_prompt, image=raw_image).latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c9ef8-6716-4d74-aeb0-e716594f7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = pipeline(\n",
    "    prompt=target_prompt,\n",
    "    mask_image=mask_image,\n",
    "    image_latents=inv_latents,\n",
    "    negative_prompt=source_prompt,\n",
    ").images[0]\n",
    "mask_image = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\").resize((768, 768))\n",
    "make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097419f1-8b0f-4ee4-99c8-12b4c3a221b7",
   "metadata": {},
   "source": [
    "#### Generate source and target embeddings\n",
    "- The source and target embeddings can be automatically generated with the `Flan-T5` model instead of creating them manually.\n",
    "- Provide some initial text to prompt the model to generate the source and target prompts.\n",
    "- Create a utility function to generate the prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4455a2c5-0c16-4abe-93e7-8c4a464a6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "source_concept = \"bowl\"\n",
    "target_concept = \"basket\"\n",
    "\n",
    "source_text = f\"Provide a caption for images containing a {source_concept}. \"\n",
    "\"The captions should be in English and should be no longer than 150 characters.\"\n",
    "\n",
    "target_text = f\"Provide a caption for images containing a {target_concept}. \"\n",
    "\"The captions should be in English and should be no longer than 150 characters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654cad4-1d5d-4170-89fd-a59bf58d9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_prompts(input_prompt):\n",
    "    input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=0.8, num_return_sequences=16, do_sample=True, max_new_tokens=128, top_k=10\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "source_prompts = generate_prompts(source_text)\n",
    "target_prompts = generate_prompts(target_text)\n",
    "print(source_prompts)\n",
    "print(target_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1878ab-aea5-4167-a984-add0f18c93c3",
   "metadata": {},
   "source": [
    "- Load the text encoder model used by the `StableDiffusionDiffEditPipeline` to encode the text.\n",
    "  - You’ll use the text encoder to compute the text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c88a2-68fb-4adb-9ac7-83cd263278d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionDiffEditPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "pipeline.enable_vae_slicing()\n",
    "\n",
    "\n",
    "def embed_prompts(sentences, tokenizer, text_encoder, device=\"cuda\"):\n",
    "    embeddings = []\n",
    "    for sent in sentences:\n",
    "        text_inputs = tokenizer(\n",
    "            sent,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=None)[0]\n",
    "        embeddings.append(prompt_embeds)\n",
    "    return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)\n",
    "\n",
    "source_embeds = embed_prompts(source_prompts, pipeline.tokenizer, pipeline.text_encoder)\n",
    "target_embeds = embed_prompts(target_prompts, pipeline.tokenizer, pipeline.text_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859b840-f112-4d31-bf8d-6ea6629adf0e",
   "metadata": {},
   "source": [
    "- Pass the embeddings to the generate_mask() and invert() functions, and pipeline to generate the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa94788-8f76-4ebe-83b4-ac1d46bc5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)\n",
    "  pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "  img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n",
    "  raw_image = load_image(img_url).resize((768, 768))\n",
    "\n",
    "  mask_image = pipeline.generate_mask(\n",
    "      image=raw_image,\n",
    "      source_prompt=source_prompt,\n",
    "      target_prompt=target_prompt,\n",
    "# +     source_prompt_embeds=source_embeds,\n",
    "# +     target_prompt_embeds=target_embeds,\n",
    "  )\n",
    "\n",
    "  inv_latents = pipeline.invert(\n",
    "      prompt=source_prompt,\n",
    "# +     prompt_embeds=source_embeds,\n",
    "      image=raw_image,\n",
    "  ).latents\n",
    "\n",
    "  output_image = pipeline(\n",
    "      mask_image=mask_image,\n",
    "      image_latents=inv_latents,\n",
    "      prompt=target_prompt,\n",
    "      negative_prompt=source_prompt,\n",
    "# +     prompt_embeds=target_embeds,\n",
    "# +     negative_prompt_embeds=source_embeds,\n",
    "  ).images[0]\n",
    "  mask_image = Image.fromarray((mask_image.squeeze()*255).astype(\"uint8\"), \"L\")\n",
    "  make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793fbe2a-0944-425c-aa69-0fb09f2c10e3",
   "metadata": {},
   "source": [
    "#### Generate a caption for inversion\n",
    "- While you can use the `source_prompt` as a caption to help generate the partially inverted latents, you can also use the BLIP model to automatically generate a caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061efbc-6259-4751-b48b-dd3e6e8fde7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16, low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88892fdc-333b-4f6a-a7cf-5ddaa771607d",
   "metadata": {},
   "source": [
    "- Create a utility function to generate a caption from the input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e271bd-8165-45b2-9ecb-9e08bfb58e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption(images, caption_generator, caption_processor):\n",
    "    text = \"a photograph of\"\n",
    "\n",
    "    inputs = caption_processor(images, text, return_tensors=\"pt\").to(device=\"cuda\", dtype=caption_generator.dtype)\n",
    "    caption_generator.to(\"cuda\")\n",
    "    outputs = caption_generator.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # offload caption generator\n",
    "    caption_generator.to(\"cpu\")\n",
    "\n",
    "    caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b86ff-6f9a-4455-a9a6-5106caea3098",
   "metadata": {},
   "source": [
    "- Load an input image and generate a caption for it using the `generate_caption` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d2a39-6b9a-4457-a374-986e78fb0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png\"\n",
    "raw_image = load_image(img_url).resize((768, 768))\n",
    "caption = generate_caption(raw_image, model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d447e-01ef-4362-90c2-d54fe932cb94",
   "metadata": {},
   "source": [
    "----\n",
    "### **Trajectory Consistency Distillation-LoRA**\n",
    "- `Trajectory Consistency Distillation (TCD)` enables a model to generate higher quality and more detailed images with fewer steps.\n",
    "- Owing to the effective error mitigation during the distillation process, `TCD` demonstrates superior performance even under conditions of large inference steps.\n",
    "\n",
    "- The major advantages of TCD are:\n",
    "  - Better than Teacher: TCD demonstrates superior generative quality at both small and large inference steps and exceeds the performance of `DPM-Solver++(2S)` with `Stable Diffusion XL (SDXL)`.\n",
    "\n",
    "- For large models like `SDXL`, `TCD` is trained with `LoRA` to reduce memory usage.\n",
    "  - This is also useful because you can reuse LoRAs between different finetuned models, as long as they share the same base model, without further training.\n",
    " \n",
    "<br>\n",
    "\n",
    "#### General tasks\n",
    "- Let’s use the `StableDiffusionXLPipeline` and the `TCDScheduler`.\n",
    "  - Use the `load_lora_weights()` method to load the `SDXL-compatible TCD-LoRA` weights.\n",
    "\n",
    "- A few tips to keep in mind for TCD-LoRA inference are to:\n",
    "  - Keep the `num_inference_steps` between 4 and 50\n",
    "  - Set `eta` (used to control stochasticity at each step) between 0 and 1.\n",
    "  - You should use a higher eta when increasing the number of inference steps, but the downside is that a larger eta in `TCDScheduler` leads to blurrier images.\n",
    "  - A value of `0.3` is recommended to produce good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ee550-49ff-4adb-b660-ca4da13cd718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, TCDScheduler\n",
    "\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\n",
    "from ip_adapter import IPAdapterXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff703f8-15d6-4c0c-8cef-4d011ce77c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "prompt = \"Painting of the orange cat Otto von Garfield, Count of Bismarck-Schönhausen, Duke of Lauenburg, Minister-President of Prussia. Depicted wearing a Prussian Pickelhaube and eating his favorite meal - lasagna.\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb9767-2943-476f-b6d4-c04353fada14",
   "metadata": {},
   "source": [
    "#### Community models\n",
    "- `TCD-LoRA` also works with many community finetuned models and plugins.\n",
    "  - Load the `animagine-xl-3.0` checkpoint which is a community finetuned version of SDXL for generating anime images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba52dd-656b-4c61-8ce1-f85a2e76013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "base_model_id = \"cagliostrolab/animagine-xl-3.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "prompt = \"A man, clad in a meticulously tailored military uniform, stands with unwavering resolve. The uniform boasts intricate details, and his eyes gleam with determination. Strands of vibrant, windswept hair peek out from beneath the brim of his cap.\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=8,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5103b-7ab3-48e0-9060-68b3fbeebe91",
   "metadata": {},
   "source": [
    "- `TCD-LoRA` also supports other LoRAs trained on different styles.\n",
    "  - Load the `TheLastBen/Papercut_SDXL` LoRA and fuse it with the `TCD-LoRA` with the `~loaders.UNet2DConditionLoadersMixin.set_adapters` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330db5e0-750e-4f79-91cd-df11754a3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "styled_lora_id = \"TheLastBen/Papercut_SDXL\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id, adapter_name=\"tcd\")\n",
    "pipe.load_lora_weights(styled_lora_id, adapter_name=\"style\")\n",
    "pipe.set_adapters([\"tcd\", \"style\"], adapter_weights=[1.0, 1.0])\n",
    "\n",
    "prompt = \"papercut of a winter mountain, snow\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60bacd-20ed-4e98-bfba-7c2d77db234e",
   "metadata": {},
   "source": [
    "#### Adapters\n",
    "- TCD-LoRA is very versatile, and it can be combined with other adapter types like ControlNets, IP-Adapter, and AnimateDiff.\n",
    "- **Depth ControlNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ae2ee-673a-4aa5-98fb-391db3007cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(device)\n",
    "feature_extractor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "def get_depth_map(image):\n",
    "    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    with torch.no_grad(), torch.autocast(device):\n",
    "        depth_map = depth_estimator(image).predicted_depth\n",
    "\n",
    "    depth_map = torch.nn.functional.interpolate(\n",
    "        depth_map.unsqueeze(1),\n",
    "        size=(1024, 1024),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "    image = torch.cat([depth_map] * 3, dim=1)\n",
    "\n",
    "    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "controlnet_id = \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    base_model_id,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "prompt = \"stormtrooper lecture, photorealistic\"\n",
    "\n",
    "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\n",
    "depth_image = get_depth_map(image)\n",
    "\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=depth_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]\n",
    "\n",
    "grid_image = make_image_grid([depth_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd726c7-5ca1-47cc-9130-5ec04a542e9a",
   "metadata": {},
   "source": [
    "- **IP Adapter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359d4c1-3fe2-4daf-a77d-093e2e926912",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "base_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "image_encoder_path = \"sdxl_models/image_encoder\"\n",
    "ip_ckpt = \"sdxl_models/ip-adapter_sdxl.bin\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "ref_image = load_image(\"https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/images/woman.png\").resize((512, 512))\n",
    "\n",
    "prompt = \"best quality, high quality, wearing sunglasses\"\n",
    "\n",
    "image = ip_model.generate(\n",
    "    pil_image=ref_image,\n",
    "    prompt=prompt,\n",
    "    scale=0.5,\n",
    "    num_samples=1,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    seed=0,\n",
    ")[0]\n",
    "\n",
    "grid_image = make_image_grid([ref_image, image], rows=1, cols=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-t2i",
   "language": "python",
   "name": "diffusers-t2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
