{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66701ff3-1435-4ab4-88b6-f22ad6a1677e",
   "metadata": {},
   "source": [
    "## **7. Training**\n",
    "\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/overview\n",
    "\n",
    "```\n",
    "> Create a Dataset for Training\n",
    "> Adapt a Model to a New Task\n",
    "> Models\n",
    "  - Unconditional Image Generation\n",
    "  - Text-to-Image\n",
    "  - Stable Diffusion XL\n",
    "  - Kandinsky 2.2, Wuerstchen, ControlNet, T2I-Adapter, InstructPix2Pix, etc.\n",
    "> Methods\n",
    "  - Textual Inversion\n",
    "  - DreamBooth, LoRA, Custom Diffusion, Latent Consistency Distillation\n",
    "\n",
    "```\n",
    "\n",
    "------------------------\n",
    "### **Install**\n",
    "- Diffusers provides a collection of training scripts for you to train your own diffusion models.\n",
    "- Make sure you can successfully run the latest versions of the example scripts by installing the library from source in a new virtual environment:\n",
    "\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "```\n",
    "\n",
    "- Then navigate to the folder of the training script (for example, DreamBooth) and install the `requirements.txt` file.\n",
    "  - Some training scripts have a specific requirement file for SDXL, LoRA or Flax.\n",
    "  - If you’re using one of these scripts, make sure you install its corresponding requirements file.\n",
    "```\n",
    "cd examples/dreambooth\n",
    "pip install -r requirements.txt\n",
    "# to train SDXL with DreamBooth\n",
    "pip install -r requirements_sdxl.txt\n",
    "```\n",
    "\n",
    "- To speedup training and reduce memory-usage, we recommend:\n",
    "  - using `PyTorch 2.0` or higher to automatically use scaled dot product attention during training (you don’t need to make any changes to the training code)\n",
    "  - installing `xFormers` to enable memory-efficient attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0c97d0-439f-469b-8a20-4190642dad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import jax\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import FlaxStableDiffusionPipeline\n",
    "from diffusers import AutoModel\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "from flax.jax_utils import replicate\n",
    "from flax.training.common_utils import shard\n",
    "\n",
    "from datasets import load_dataset\n",
    "from accelerate.utils import write_basic_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f4152-cf26-4e0d-908b-2c87df064d1b",
   "metadata": {},
   "source": [
    "-----------------\n",
    "### **Create a Dataset for Training**\n",
    "- There are many datasets on the Hub to train a model on, but if you can’t find one you’re interested in or want to use your own, you can create a dataset with the `Datasets` library.\n",
    "  - The dataset structure depends on the task you want to train your model on.\n",
    "  - The most basic dataset structure is **a directory of images** for tasks like unconditional image generation.\n",
    "    - Another dataset structure may be **a directory of images and a text file containing their corresponding text captions** for tasks like text-to-image generation.\n",
    "\n",
    "\n",
    "#### Provide a dataset as a folder\n",
    "- For unconditional generation, you can provide your own dataset as a folder of images.\n",
    "  - The training script uses the `ImageFolder` builder from `Datasets` to automatically build a dataset from the folder.\n",
    "  - Your directory structure should look like:\n",
    "```\n",
    "data_dir/xxx.png\n",
    "data_dir/xxy.png\n",
    "data_dir/[...]/xxz.png\n",
    "```\n",
    "\n",
    "- Pass the path to the dataset directory to the `--train_data_dir` argument, and then you can start training:\n",
    "```\n",
    "accelerate launch train_unconditional.py \\\n",
    "    --train_data_dir <path-to-train-directory> \\\n",
    "    <other-arguments>\n",
    "```\n",
    "\n",
    "\n",
    "#### Upload your data to the Hub\n",
    "- Start by creating a dataset with the `ImageFolder` feature, which creates an image column containing the PIL-encoded images.\n",
    "  - You can use the `data_dir` or `data_files` parameters to specify the location of the dataset.\n",
    "  - The `data_files` parameter supports mapping specific files to dataset splits like train or test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c388a-20f5-4a4d-b42e-c95c31686a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1: local folder\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"path_to_your_folder\")\n",
    "\n",
    "# example 2: local files (supported formats are tar, gzip, zip, xz, rar, zstd)\n",
    "dataset = load_dataset(\"imagefolder\", data_files=\"path_to_zip_file\")\n",
    "\n",
    "# example 3: remote files (supported formats are tar, gzip, zip, xz, rar, zstd)\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_files=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\",\n",
    ")\n",
    "\n",
    "# example 4: providing several splits\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\", data_files={\"train\": [\"path/to/file1\", \"path/to/file2\"], \"test\": [\"path/to/file3\", \"path/to/file4\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076700d1-a8f0-464b-8c34-a4cc896d2347",
   "metadata": {},
   "source": [
    "- Then use the `push_to_hub` method to upload the dataset to the Hub.\n",
    "  - The dataset is available for training by passing the dataset name to the `--dataset_name` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6307e2-14b8-4aa7-80a3-9cc10bdea1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you have ran the huggingface-cli login command in a terminal\n",
    "dataset.push_to_hub(\"name_of_your_dataset\")\n",
    "\n",
    "# if you want to push to a private repo, simply pass private=True:\n",
    "dataset.push_to_hub(\"name_of_your_dataset\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bdb44-0bf8-4e79-886c-13b5c05a5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n",
    "  --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\" \\\n",
    "  --dataset_name=\"name_of_your_dataset\" \\\n",
    "  <other-arguments>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ad459-abed-4561-a646-e6f9494ed57c",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Adapt a model to a new task**\n",
    "- Many diffusion systems share the same components, allowing you to adapt a pretrained model for one task to an entirely different task.\n",
    "  - How to adapt a pretrained text-to-image model for inpainting by initializing and modifying the architecture of a pretrained `UNet2DConditionModel`.\n",
    "\n",
    "- Configure `UNet2DConditionModel` parameters\n",
    "  - A `UNet2DConditionModel` by default accepts 4 channels in the input sample.\n",
    "    - Load a pretrained text-to-image model like `stable-diffusion-v1-5/stable-diffusion-v1-5` and take a look at the number of `in_channels`:\n",
    "   \n",
    "- Inpainting requires 9 channels in the input sample. You can check this value in a pretrained inpainting model like `runwayml/stable-diffusion-inpainting`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d951849-dc7d-4d24-898c-128d62cf523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\n",
    "pipeline.unet.config[\"in_channels\"]\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\", use_safetensors=True)\n",
    "pipeline.unet.config[\"in_channels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5e39a-04f5-4ea6-9434-b57ffeb23b9b",
   "metadata": {},
   "source": [
    "- To adapt your text-to-image model for inpainting, you’ll need to change the number of in_channels from 4 to 9.\n",
    "- Initialize a `UNet2DConditionModel` with the pretrained text-to-image model weights, and change `in_channels` to 9.\n",
    "  - Changing the number of `in_channels` means you need to set `ignore_mismatched_sizes=True` and `low_cpu_mem_usage=False` to avoid a size mismatch error because the shape is different now.\n",
    " \n",
    "- The pretrained weights of the other components from the text-to-image model are initialized from their checkpoints, but the input channel weights (`conv_in.weight`) of the unet are randomly initialized.\n",
    "  - It is important to finetune the model for inpainting because otherwise the model returns noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ac308-05ed-4f75-81cf-36d4d30aefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "unet = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"unet\",\n",
    "    in_channels=9,\n",
    "    low_cpu_mem_usage=False,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b04c2-b5aa-4216-b03c-34d78f6932c1",
   "metadata": {},
   "source": [
    "----\n",
    "### **Model | Unconditional image generation**\n",
    "- Unconditional image generation models are not conditioned on text or images during training.\n",
    "  - It only generates images that resemble its training data distribution.\n",
    "\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "```\n",
    "\n",
    "- Then navigate to the example folder containing the training script and install the required dependencies:\n",
    "```\n",
    "cd examples/unconditional_image_generation\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- Initialize an `Accelerate` environment:\n",
    "  - To setup a default `Accelerate` environment without choosing any configurations:\n",
    "\n",
    "```\n",
    "accelerate config\n",
    "accelerate config default\n",
    "```\n",
    "\n",
    "- If your environment doesn’t support an interactive shell like a notebook, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12e053-9322-4cad-a81e-53803df48710",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e66c2d-3e29-4920-afbc-88b9e0412205",
   "metadata": {},
   "source": [
    "#### Script parameters\n",
    "- The training script provides many parameters to help you customize your training run.\n",
    "  - All of the parameters and their descriptions are found in `the parse_args()` function.\n",
    "  - It provides default values for each parameter, such as the training batch size and learning rate, but you can also set your own values in the training command if you’d like.\n",
    "\n",
    "\n",
    "- To speedup training with mixed precision using the bf16 format, add the `--mixed_precision` parameter to the training command:\n",
    "```\n",
    "accelerate launch train_unconditional.py \\\n",
    "--mixed_precision=\"bf16\"\n",
    "```\n",
    "- Some basic and important parameters to specify include:\n",
    "  - `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n",
    "  - `--output_dir`: where to save the trained model\n",
    "  - `--push_to_hub`: whether to push the trained model to the Hub\n",
    "  - `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if training is interrupted, you can continue training from that checkpoint by adding `--resume_from_checkpoint` to your training command\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Training script\n",
    "- The code for preprocessing the dataset and the training loop is found in the `main()` function.\n",
    "  - If you need to adapt the training script, this is where you’ll need to make your changes.\n",
    "- The `train_unconditional` script initializes a `UNet2DModel` if you don’t provide a model configuration.\n",
    "  - You can configure the UNet here if you’d like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a2ea9-c481-44c6-a20e-04159c014a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=args.resolution,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc52f0-dbfb-467c-8980-190765c1172d",
   "metadata": {},
   "source": [
    "- Initializes a scheduler and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524d6e1-b9a9-4288-b127-d710c0026f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scheduler\n",
    "accepts_prediction_type = \"prediction_type\" in set(inspect.signature(DDPMScheduler.__init__).parameters.keys())\n",
    "if accepts_prediction_type:\n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=args.ddpm_num_steps,\n",
    "        beta_schedule=args.ddpm_beta_schedule,\n",
    "        prediction_type=args.prediction_type,\n",
    "    )\n",
    "else:\n",
    "    noise_scheduler = DDPMScheduler(num_train_timesteps=args.ddpm_num_steps, beta_schedule=args.ddpm_beta_schedule)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96140281-4e89-4dd9-b887-16db2521838c",
   "metadata": {},
   "source": [
    "- Then it loads a dataset and you can specify how to preprocess it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229bcfa-056d-4c4f-a02a-198ba4e2d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir=args.train_data_dir, cache_dir=args.cache_dir, split=\"train\")\n",
    "\n",
    "augmentations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n",
    "        transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c25fb6-3116-43d8-baee-6b5d7693c61a",
   "metadata": {},
   "source": [
    "- The training loop handles everything else such as adding noise to the images, predicting the noise residual, calculating the loss, saving checkpoints at specified steps, and saving and pushing the model to the Hub.\n",
    "  - If you want to learn more about how the training loop works, check out the `Understanding` pipelines, models and schedulers tutorial which breaks down the basic pattern of the denoising process.\n",
    " \n",
    "#### Launch the script\n",
    "- Simple GPU\n",
    "```\n",
    "accelerate launch train_unconditional.py \\\n",
    "  --dataset_name=\"huggan/flowers-102-categories\" \\\n",
    "  --output_dir=\"ddpm-ema-flowers-64\" \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --push_to_hub\n",
    "```\n",
    "\n",
    "- Multi-GPU\n",
    "  - Add the `--multi_gpu` parameter to the training command:\n",
    "```\n",
    "accelerate launch --multi_gpu train_unconditional.py \\\n",
    "  --dataset_name=\"huggan/flowers-102-categories\" \\\n",
    "  --output_dir=\"ddpm-ema-flowers-64\" \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --push_to_hub\n",
    "```\n",
    "\n",
    "- The training script creates and saves a checkpoint file in your repository.\n",
    "  - Now you can load and use your trained model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c783a0-ba26-4dcd-88c2-15f4ed16eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\"anton-l/ddpm-butterflies-128\").to(\"cuda\")\n",
    "image = pipeline().images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2738121-4ad7-4dff-b4e2-c939b438f07a",
   "metadata": {},
   "source": [
    "----\n",
    "### **Model | Text-to-Image**\n",
    "- Text-to-image models like Stable Diffusion are conditioned to generate images given a text prompt.\n",
    "  - Training a model can be taxing on your hardware, but if you enable `gradient_checkpointing` and `mixed_precision`, it is possible to train a model on a single 24GB GPU.\n",
    "  - If you’re training with larger batch sizes or want to train faster, it’s better to use GPUs with more than 30GB of memory.\n",
    "    - Reduce your memory footprint by enabling memory-efficient attention with xFormers.\n",
    "    - JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn’t support gradient checkpointing, gradient accumulation or xFormers.\n",
    "  - A GPU with at least 30GB of memory or a TPU v3 is recommended for training with Flax.\n",
    "\n",
    "- Explore the `train_text_to_image.py` training script to help you become familiar with it, and how you can adapt it for your own use-case.\n",
    "  - Before running the script, make sure you install the library from source:\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "```\n",
    "\n",
    "- Navigate to the example folder containing the training script and install the required dependencies for the script you’re using:\n",
    "```\n",
    "# Pytorch\n",
    "cd examples/text_to_image\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Flax\n",
    "cd examples/text_to_image\n",
    "pip install -r requirements_flax.txt\n",
    "```\n",
    "\n",
    "- Initialize an `Accelerate` environment.\n",
    "  - To setup a default `Accelerate` environment without choosing any configurations:\n",
    "\n",
    "```\n",
    "accelerate config\n",
    "accelerate config default\n",
    "```\n",
    "\n",
    "- If your environment doesn’t support an interactive shell, like a notebook, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b30601-9c46-4897-80bb-464d508989df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aad182-769e-46de-8b16-6b4bf3bab7c4",
   "metadata": {},
   "source": [
    "#### Script parameters\n",
    "- The training script provides many parameters to help you customize your training run. \n",
    "  - All of the parameters and their descriptions are found in the `parse_args()` function. \n",
    "  - This function provides default values for each parameter, such as the training batch size and learning rate, but you can also set your own values in the training command if you’d like.\n",
    "\n",
    "- To speedup training with mixed precision using the fp16 format, add the `--mixed_precision` parameter to the training command:\n",
    "```\n",
    "accelerate launch train_text_to_image.py \\\n",
    "  --mixed_precision=\"fp16\"\n",
    "```\n",
    "\n",
    "- Some basic and important parameters include:\n",
    "  - `--pretrained_model_name_or_path`: the name of the model on the Hub or a local path to the pretrained model\n",
    "  - `--dataset_name`: the name of the dataset on the Hub or a local path to the dataset to train on\n",
    "  - `--image_column`: the name of the image column in the dataset to train on\n",
    "  - `--caption_column`: the name of the text column in the dataset to train on\n",
    "  - `--output_dir`: where to save the trained model\n",
    "  - `--push_to_hub`: whether to push the trained model to the Hub\n",
    "  - `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if for some reason training is interrupted, you can continue training from that checkpoint by adding `--resume_from_checkpoint` to your training command\n",
    "Min-SNR weighting\n",
    "\n",
    "- **Min-SNR weighting**\n",
    "    - The Min-SNR weighting strategy can help with training by rebalancing the loss to achieve faster convergence.\n",
    "      - The training script supports predicting epsilon (noise) or v_prediction, but Min-SNR is compatible with both prediction types.\n",
    "      - This weighting strategy is only supported by PyTorch and is unavailable in the Flax training script.\n",
    "    \n",
    "    - Add the `--snr_gamma parameter` and set it to the recommended value of 5.0:\n",
    "    ```\n",
    "    accelerate launch train_text_to_image.py \\\n",
    "      --snr_gamma=5.0\n",
    "    ```\n",
    "\n",
    "#### Training script\n",
    "- The dataset preprocessing code and training loop are found in the `main()` function. If you need to adapt the training script, this is where you’ll need to make your changes.\n",
    "- The `train_text_to_image` script starts by loading a scheduler and tokenizer.\n",
    "  - You can choose to use a different scheduler here if you want.\n",
    "  - And loads the UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49912b58-c057-4ede-8834-d4dc45def0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n",
    ")\n",
    "\n",
    "load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder=\"unet\")\n",
    "model.register_to_config(**load_model.config)\n",
    "\n",
    "model.load_state_dict(load_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024f5ed-0a36-4622-a586-bf1390466527",
   "metadata": {},
   "source": [
    "- Text and image columns of the dataset need to be preprocessed.\n",
    "  - The `tokenize_captions` function handles tokenizing the inputs, and the `train_transforms` function specifies the type of transforms to apply to the image.\n",
    "  - Both of these functions are bundled into `preprocess_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435ce03-46af-4c07-96ce-44a3c2ab3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03b3db-7477-42e4-9bea-f6f899e60b74",
   "metadata": {},
   "source": [
    "#### Launch the script\n",
    "- Made all your changes or you’re okay with the default configuration, you’re ready to launch the training script.\n",
    "- **Pytorch**\n",
    "    - Let’s train on the Naruto BLIP captions dataset to generate your own Naruto characters.\n",
    "    - Set the environment variables `MODEL_NAME` and `dataset_name` to the model and the dataset (either from the Hub or a local path).\n",
    "    - If you’re training on more than one GPU, add the `--multi_gpu` parameter to the accelerate launch command.\n",
    "        ```\n",
    "        export MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "        export dataset_name=\"lambdalabs/naruto-blip-captions\"\n",
    "        \n",
    "        accelerate launch --mixed_precision=\"fp16\"  train_text_to_image.py \\\n",
    "          --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "          --dataset_name=$dataset_name \\\n",
    "          --use_ema \\\n",
    "          --resolution=512 --center_crop --random_flip \\\n",
    "          --train_batch_size=1 \\\n",
    "          --gradient_accumulation_steps=4 \\\n",
    "          --gradient_checkpointing \\\n",
    "          --max_train_steps=15000 \\\n",
    "          --learning_rate=1e-05 \\\n",
    "          --max_grad_norm=1 \\\n",
    "          --enable_xformers_memory_efficient_attention \\\n",
    "          --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
    "          --output_dir=\"sd-naruto-model\" \\\n",
    "          --push_to_hub\n",
    "        ```\n",
    "\n",
    "- **Flax**\n",
    "  - Training with Flax can be faster on TPUs and GPUs.\n",
    "    - Flax is more efficient on a TPU, but GPU performance is also great.\n",
    "    - Set the environment variables `MODEL_NAME` and `dataset_name` to the model and the dataset (either from the Hub or a local path).\n",
    "    ```\n",
    "    export MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "    export dataset_name=\"lambdalabs/naruto-blip-captions\"\n",
    "    \n",
    "    python train_text_to_image_flax.py \\\n",
    "      --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "      --dataset_name=$dataset_name \\\n",
    "      --resolution=512 --center_crop --random_flip \\\n",
    "      --train_batch_size=1 \\\n",
    "      --max_train_steps=15000 \\\n",
    "      --learning_rate=1e-05 \\\n",
    "      --max_grad_norm=1 \\\n",
    "      --output_dir=\"sd-naruto-model\" \\\n",
    "      --push_to_hub\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc429020-dcc6-4b0c-b3d4-6dbe6a2a6c32",
   "metadata": {},
   "source": [
    "- Use your newly trained model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9be5a-023d-4588-93e8-5fd96f801a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"path/to/saved_model\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "image = pipeline(prompt=\"yoda\").images[0]\n",
    "image.save(\"yoda-naruto.png\")\n",
    "\n",
    "# Flax\n",
    "pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\"path/to/saved_model\", dtype=jax.numpy.bfloat16)\n",
    "\n",
    "prompt = \"yoda naruto\"\n",
    "prng_seed = jax.random.PRNGKey(0)\n",
    "num_inference_steps = 50\n",
    "\n",
    "num_samples = jax.device_count()\n",
    "prompt = num_samples * [prompt]\n",
    "prompt_ids = pipeline.prepare_inputs(prompt)\n",
    "\n",
    "# shard inputs and rng\n",
    "params = replicate(params)\n",
    "prng_seed = jax.random.split(prng_seed, jax.device_count())\n",
    "prompt_ids = shard(prompt_ids)\n",
    "\n",
    "images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\n",
    "images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n",
    "image.save(\"yoda-naruto.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606b33d-e1c6-4b2e-ac32-c94868844f8c",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "- Learn how to load `LoRA` weights for inference if you trained your model with LoRA.\n",
    "- Learn more about how certain parameters like guidance scale or techniques such as prompt weighting can help you control inference in the Text-to-image task guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140aafb-e952-4a74-a32e-87082eadc065",
   "metadata": {},
   "source": [
    "----\n",
    "### **Model | Stable Diffusion XL**\n",
    "- Stable Diffusion XL (SDXL) is a larger and more powerful iteration of the Stable Diffusion model, capable of producing higher resolution images.\n",
    "  - SDXL’s UNet is 3x larger and the model adds a second text encoder to the architecture.\n",
    "  - Depending on the hardware available to you, this can be very computationally intensive and it may not run on a consumer GPU like a Tesla T4.\n",
    "  - To help fit this larger model into memory and to speedup training, try enabling `gradient_checkpointing`, `mixed_precision`, and `gradient_accumulation_steps`.\n",
    "  - You can reduce your memory-usage even more by enabling memory-efficient attention with xFormers and using bitsandbytes’ 8-bit optimizer.\n",
    "\n",
    "- Explore the `train_text_to_image_sdxl.py` training script to help you become more familiar with it, and how you can adapt it for your own use-case.\n",
    "- Before running the script, make sure you install the library from source:\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "```\n",
    "\n",
    "- Navigate to the example folder containing the training script and install the required dependencies for the script you’re using:\n",
    "```\n",
    "cd examples/text_to_image\n",
    "pip install -r requirements_sdxl.txt\n",
    "```\n",
    "\n",
    "- Initialize an `Accelerate` environment:\n",
    "```\n",
    "accelerate config\n",
    "```\n",
    "\n",
    "- To setup a default `Accelerate` environment without choosing any configurations:\n",
    "```\n",
    "accelerate config default\n",
    "```\n",
    "\n",
    "- If your environment doesn’t support an interactive shell, like a notebook, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63e0b7-db73-4875-a565-919a8eebc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2aca7a-047d-4698-ab22-18018eb3d3ac",
   "metadata": {},
   "source": [
    "#### Script parameters\n",
    "- The training script provides many parameters to help you customize your training run.\n",
    "  - All of the parameters and their descriptions are found in the `parse_args()` function.\n",
    "  - This function provides default values for each parameter, such as the training batch size and learning rate, but you can also set your own values in the training command if you’d like.\n",
    "\n",
    "- To speedup training with mixed precision using the bf16 format, add the `--mixed_precision` parameter to the training command:\n",
    "```\n",
    "accelerate launch train_text_to_image_sdxl.py \\\n",
    "  --mixed_precision=\"bf16\"\n",
    "```\n",
    "\n",
    "- Most of the parameters are identical to the parameters in the Text-to-image training guide, so you’ll focus on the parameters that are relevant to training SDXL in this guide.\n",
    "  - `--pretrained_vae_model_name_or_path`: path to a pretrained VAE; the SDXL VAE is known to suffer from numerical instability, so this parameter allows you to specify a better VAE\n",
    "  - `--proportion_empty_prompts`: the proportion of image prompts to replace with empty strings\n",
    "  - `--timestep_bias_strategy`: where (earlier vs. later) in the timestep to apply a bias, which can encourage the model to either learn low or high frequency details\n",
    "  - `--timestep_bias_multiplier`: the weight of the bias to apply to the timestep\n",
    "  - `--timestep_bias_begin`: the timestep to begin applying the bias\n",
    "  - `--timestep_bias_end`: the timestep to end applying the bias\n",
    "  - `--timestep_bias_portion`: the proportion of timesteps to apply the bias to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c30a2-0dc6-46b7-87c6-2a329b0433d8",
   "metadata": {},
   "source": [
    "- **Min-SNR weighting**\n",
    "  - The Min-SNR weighting strategy can help with training by rebalancing the loss to achieve faster convergence. \n",
    "  - The training script supports predicting either epsilon (noise) or `v_prediction`, but Min-SNR is compatible with both prediction types. \n",
    "This weighting strategy is only supported by PyTorch and is unavailable in the Flax training script.\n",
    "  - Add the `--snr_gamma` parameter and set it to the recommended value of `5.0`:\n",
    "```\n",
    "accelerate launch train_text_to_image_sdxl.py \\\n",
    "  --snr_gamma=5.0\n",
    "```\n",
    "\n",
    "#### Training script\n",
    "- The training script is also similar to the Text-to-image training guide, but it’s been modified to support SDXL training.\n",
    "  - This guide will focus on the code that is unique to the SDXL training script.\n",
    "  - It starts by creating functions to tokenize the prompts to calculate the prompt embeddings, and to compute the image embeddings with the VAE.\n",
    "- You’ll a function to generate the timesteps weights depending on the number of timesteps and the timestep bias strategy to apply.\n",
    "  - Within the `main()` function, in addition to loading a tokenizer, the script loads a second tokenizer and text encoder because the SDXL architecture uses two of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d0ecb-d350-4c3f-a8e9-8917be68bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision, use_fast=False\n",
    ")\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"tokenizer_2\", revision=args.revision, use_fast=False\n",
    ")\n",
    "\n",
    "text_encoder_cls_one = import_model_class_from_model_name_or_path(\n",
    "    args.pretrained_model_name_or_path, args.revision\n",
    ")\n",
    "text_encoder_cls_two = import_model_class_from_model_name_or_path(\n",
    "    args.pretrained_model_name_or_path, args.revision, subfolder=\"text_encoder_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13abbb2-4f76-4d01-8620-35f9b650b9f5",
   "metadata": {},
   "source": [
    "- The prompt and image embeddings are computed first and kept in memory, which isn’t typically an issue for a smaller dataset, but for larger datasets it can lead to memory problems.\n",
    "  - If this is the case, you should save the pre-computed embeddings to disk separately and load them into memory during the training process (see this PR for more discussion about this topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26a620-8980-489b-a760-da9a747839f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoders = [text_encoder_one, text_encoder_two]\n",
    "tokenizers = [tokenizer_one, tokenizer_two]\n",
    "compute_embeddings_fn = functools.partial(\n",
    "    encode_prompt,\n",
    "    text_encoders=text_encoders,\n",
    "    tokenizers=tokenizers,\n",
    "    proportion_empty_prompts=args.proportion_empty_prompts,\n",
    "    caption_column=args.caption_column,\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\n",
    "train_dataset = train_dataset.map(\n",
    "    compute_vae_encodings_fn,\n",
    "    batched=True,\n",
    "    batch_size=args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps,\n",
    "    new_fingerprint=new_fingerprint_for_vae,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfa2c0-b54d-419c-abc7-ccdc81747fe6",
   "metadata": {},
   "source": [
    "- After calculating the embeddings, the text encoder, VAE, and tokenizer are deleted to free up some memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c8bb1-2e46-4d7d-a444-7f737db2be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "del text_encoders, tokenizers, vae\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc7ed2-b40b-4a3a-983c-5818832a4d4d",
   "metadata": {},
   "source": [
    "- The training loop takes care of the rest.\n",
    "  - If you chose to apply a timestep bias strategy, you’ll see the timestep weights are calculated and added as noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69842f5-7244-45a4-bac5-88cc79f31f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = generate_timestep_weights(args, noise_scheduler.config.num_train_timesteps).to(\n",
    "        model_input.device\n",
    "    )\n",
    "    timesteps = torch.multinomial(weights, bsz, replacement=True).long()\n",
    "\n",
    "noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c63dc-5ae3-46f2-9ac0-399338d8affc",
   "metadata": {},
   "source": [
    "#### Launch the script\n",
    "- Once you’ve made all your changes or you’re okay with the default configuration, you’re ready to launch the training script.\n",
    "- Train on the Naruto BLIP captions dataset to generate your own Naruto characters.\n",
    "  - Set the environment variables `MODEL_NAME` and `DATASET_NAME` to the model and the dataset (either from the Hub or a local path).\n",
    "  - You should also specify a VAE other than the SDXL VAE (either from the Hub or a local path) with `VAE_NAME` to avoid numerical instabilities.\n",
    "  ```\n",
    "  export MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "  export VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\n",
    "  export DATASET_NAME=\"lambdalabs/naruto-blip-captions\"\n",
    "\n",
    "  accelerate launch train_text_to_image_sdxl.py \\\n",
    "    --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "    --pretrained_vae_model_name_or_path=$VAE_NAME \\\n",
    "    --dataset_name=$DATASET_NAME \\\n",
    "    --enable_xformers_memory_efficient_attention \\\n",
    "    --resolution=512 \\\n",
    "    --center_crop \\\n",
    "    --random_flip \\\n",
    "    --proportion_empty_prompts=0.2 \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=4 \\\n",
    "    --gradient_checkpointing \\\n",
    "    --max_train_steps=10000 \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=1e-06 \\\n",
    "    --lr_scheduler=\"constant\" \\\n",
    "    --lr_warmup_steps=0 \\\n",
    "    --mixed_precision=\"fp16\" \\\n",
    "    --report_to=\"wandb\" \\\n",
    "    --validation_prompt=\"a cute Sundar Pichai creature\" \\\n",
    "    --validation_epochs 5 \\\n",
    "    --checkpointing_steps=5000 \\\n",
    "    --output_dir=\"sdxl-naruto-model\" \\\n",
    "    --push_to_hub\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c58029-d587-4899-86ca-40423b8d96d4",
   "metadata": {},
   "source": [
    "- Use your newly trained SDXL model for inference\n",
    "  - `PyTorch XLA` allows you to run PyTorch on XLA devices such as TPUs, which can be faster.\n",
    "  - The initial warmup step takes longer because the model needs to be compiled and optimized.\n",
    "  - Subsequent calls to the pipeline on an input with the same length as the original prompt are much faster because it can reuse the optimized graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df8357-8334-4380-86f1-2cb0c40208fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"path/to/your/model\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "prompt = \"A naruto with green eyes and red legs.\"\n",
    "image = pipeline(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "image.save(\"naruto.png\")\n",
    "\n",
    "# Pytorch XLA\n",
    "device = xm.xla_device()\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\").to(device)\n",
    "\n",
    "prompt = \"A naruto with green eyes and red legs.\"\n",
    "start = time()\n",
    "image = pipeline(prompt, num_inference_steps=inference_steps).images[0]\n",
    "print(f'Compilation time is {time()-start} sec')\n",
    "image.save(\"naruto.png\")\n",
    "\n",
    "start = time()\n",
    "image = pipeline(prompt, num_inference_steps=inference_steps).images[0]\n",
    "print(f'Inference time is {time()-start} sec after compilation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ae2cd-2df7-4d15-b0ca-5ea21954c3eb",
   "metadata": {},
   "source": [
    "#### Next steps\n",
    "- Read the `Stable Diffusion XL` guide to learn how to use it for a variety of different tasks (text-to-image, image-to-image, inpainting), how to use it’s refiner model, and the different types of micro-conditionings.\n",
    "- Check out the `DreamBooth` and `LoRA` training guides to learn how to train a personalized `SDXL` model with just a few example images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b820904-148d-489f-8fd6-3d3f711a522c",
   "metadata": {},
   "source": [
    "-----------------\n",
    "- Other training pipelines also follow the sequence:\n",
    "  - script parameters → training script → launch the script.\n",
    "\n",
    "- The related repositories are as follows:\n",
    "\n",
    "### **Model | Kandinsky 2.2**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/kandinsky\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/kandinsky2_2/text_to_image\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "### **Model | Wuerstchen**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/wuerstchen\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/wuerstchen/text_to_image\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "### **Model | ControlNet**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/controlnet\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/controlnet\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### **Model | T2I-Adapter**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/t2i_adapters\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/t2i_adapter\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "### **Model | InstructPix2Pix**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/instructpix2pix\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/instruct_pix2pix\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96146f-0047-45ce-af80-747a444de459",
   "metadata": {},
   "source": [
    "----\n",
    "### **Method | Textual Inversion**\n",
    "- **Textual Inversion** is a training technique for personalizing image generation models with just a few example images of what you want it to learn.\n",
    "  - This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide.\n",
    "\n",
    "- If you’re training on a GPU with limited vRAM, you should try enabling the `gradient_checkpointing` and `mixed_precision` parameters in the training command.\n",
    "  - You can also reduce your memory footprint by using memory-efficient attention with xFormers.\n",
    "  - JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn’t support gradient checkpointing or xFormers.\n",
    "  - With the same configuration and setup as PyTorch, the Flax training script should be at least `~70%` faster.\n",
    "\n",
    "- Explore the `textual_inversion.py` script to help you become more familiar with it, and how you can adapt it for your own use-case.\n",
    "  - Before running the script, make sure you install the library from source:\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "```\n",
    "\n",
    "- Navigate to the example folder with the training script and install the required dependencies for the script you’re using:\n",
    "```\n",
    "cd examples/textual_inversion\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- Initialize an `Accelerate` environment:\n",
    "  - Setup a default `Accelerate` environment without choosing any configurations:\n",
    "```\n",
    "accelerate config\n",
    "accelerate config default\n",
    "```\n",
    "\n",
    "- If your environment doesn’t support an interactive shell, like a notebook, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7e3c6-bab0-4be7-811b-ad3613788649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840dbdad-211b-4ed4-befe-dd1aa2fa6eaa",
   "metadata": {},
   "source": [
    "#### Script parameters\n",
    "- The training script has many parameters to help you tailor the training run to your needs.\n",
    "  - All of the parameters and their descriptions are listed in the `parse_args()` function.\n",
    "  - Where applicable, Diffusers provides default values for each parameter such as the training batch size and learning rate, but feel free to change these values in the training command if you’d like.\n",
    "\n",
    "- To increase the number of gradient accumulation steps above the default value of 1:\n",
    "```\n",
    "accelerate launch textual_inversion.py \\\n",
    "  --gradient_accumulation_steps=4\n",
    "```\n",
    "\n",
    "- Some other basic and important parameters to specify include:\n",
    "  - `--pretrained_model_name_or_path`: the name of the model on the Hub or a local path to the pretrained model\n",
    "  - `--train_data_dir`: path to a folder containing the training dataset (example images)\n",
    "  - `--output_dir`: where to save the trained model\n",
    "  - `--push_to_hub`: whether to push the trained model to the Hub\n",
    "  - `--checkpointing_steps`: frequency of saving a checkpoint as the model trains; this is useful if for some reason training is interrupted, you can continue training from that checkpoint by adding --resume_from_checkpoint to your training command\n",
    "  - `--num_vectors`: the number of vectors to learn the embeddings with; increasing this parameter helps the model learn better but it comes with increased training costs\n",
    "  - `--placeholder_token`: the special word to tie the learned embeddings to (you must use the word in your prompt for inference)\n",
    "  - `--initializer_token`: a single-word that roughly describes the object or style you’re trying to train on\n",
    "  - `--learnable_property`: whether you’re training the model to learn a new “style” (for example, Van Gogh’s painting style) or “object”\n",
    "\n",
    "#### Training script\n",
    "- Unlike some of the other training scripts, `textual_inversion.py` has a custom dataset class, `TextualInversionDataset` for creating a dataset.\n",
    "- You can customize the image size, placeholder token, interpolation method, whether to crop the image, and more.\n",
    "- If you need to change how the dataset is created, you can modify `TextualInversionDataset`.\n",
    "  - Find the dataset preprocessing code and training loop in the `main()` function.\n",
    "\n",
    "- The script starts by loading the tokenizer, scheduler and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e473d1a-cc1b-40c3-8ce1-604c10e7781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)\n",
    "elif args.pretrained_model_name_or_path:\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "\n",
    "# Load scheduler and models\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76819149-9f43-4928-a816-91c66008247e",
   "metadata": {},
   "source": [
    "- The special placeholder token is added next to the tokenizer, and the embedding is readjusted to account for the new token.\n",
    "  - The script creates a dataset from the `TextualInversionDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152caf15-20a8-43da-8351-094697a3a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextualInversionDataset(\n",
    "    data_root=args.train_data_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    size=args.resolution,\n",
    "    placeholder_token=(\" \".join(tokenizer.convert_ids_to_tokens(placeholder_token_ids))),\n",
    "    repeats=args.repeats,\n",
    "    learnable_property=args.learnable_property,\n",
    "    center_crop=args.center_crop,\n",
    "    set=\"train\",\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.dataloader_num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb70a5d-0485-4009-881f-52286c22418a",
   "metadata": {},
   "source": [
    "- The training loop handles everything else from predicting the noisy residual to updating the embedding weights of the special placeholder token.\n",
    "  - Check out the Understanding pipelines, models and schedulers tutorial which breaks down the basic pattern of the denoising process.\n",
    "\n",
    "#### Launch the script\n",
    "- You’ll download some images of a cat toy and store them in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afa503-0feb-409f-9203-89c73cd943a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"./cat\"\n",
    "snapshot_download(\n",
    "    \"diffusers/cat_toy_example\", local_dir=local_dir, repo_type=\"dataset\", ignore_patterns=\".gitattributes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21025b-e8c4-4223-aaf0-aec650c9940c",
   "metadata": {},
   "source": [
    "- Set the environment variable `MODEL_NAME` to a model id on the Hub or a path to a local model, and `DATA_DIR` to the path where you just downloaded the cat images to.\n",
    "  - The script creates and saves the following files to your repository:\n",
    "    - `learned_embeds.bin`: the learned embedding vectors corresponding to your example images\n",
    "    - `token_identifier.txt`: the special placeholder token\n",
    "    - `type_of_concept.txt`: the type of concept you’re training on (either “object” or “style”)\n",
    "\n",
    "- A full training run takes ~1 hour on a single V100 GPU.\n",
    "\n",
    "- If you’re interested in following along with the training process, you can periodically save generated images as training progresses. Add the following parameters to the training command:\n",
    "\n",
    "```\n",
    "--validation_prompt=\"A <cat-toy> train\"\n",
    "--num_validation_images=4\n",
    "--validation_steps=100\n",
    "```\n",
    "\n",
    "- **Pytorch**\n",
    "```\n",
    "export MODEL_NAME=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "export DATA_DIR=\"./cat\"\n",
    "\n",
    "accelerate launch textual_inversion.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --train_data_dir=$DATA_DIR \\\n",
    "  --learnable_property=\"object\" \\\n",
    "  --placeholder_token=\"<cat-toy>\" \\\n",
    "  --initializer_token=\"toy\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --max_train_steps=3000 \\\n",
    "  --learning_rate=5.0e-04 \\\n",
    "  --scale_lr \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --output_dir=\"textual_inversion_cat\" \\\n",
    "  --push_to_hub\n",
    "```\n",
    "\n",
    "- **Flax**\n",
    "```\n",
    "export MODEL_NAME=\"duongna/stable-diffusion-v1-4-flax\"\n",
    "export DATA_DIR=\"./cat\"\n",
    "\n",
    "python textual_inversion_flax.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --train_data_dir=$DATA_DIR \\\n",
    "  --learnable_property=\"object\" \\\n",
    "  --placeholder_token=\"<cat-toy>\" \\\n",
    "  --initializer_token=\"toy\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --max_train_steps=3000 \\\n",
    "  --learning_rate=5.0e-04 \\\n",
    "  --scale_lr \\\n",
    "  --output_dir=\"textual_inversion_cat\" \\\n",
    "  --push_to_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5fccc4-309a-41ad-8c20-10e79c6c3eb2",
   "metadata": {},
   "source": [
    "- Use your newly trained model for inference like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da6cbe-f19b-44b0-ad18-48ddff0de9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n",
    "image = pipeline(\"A <cat-toy> train\", num_inference_steps=50).images[0]\n",
    "image.save(\"cat-train.png\")\n",
    "\n",
    "# Flax\n",
    "model_path = \"path-to-your-trained-model\"\n",
    "pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(model_path, dtype=jax.numpy.bfloat16)\n",
    "\n",
    "prompt = \"A <cat-toy> train\"\n",
    "prng_seed = jax.random.PRNGKey(0)\n",
    "num_inference_steps = 50\n",
    "\n",
    "num_samples = jax.device_count()\n",
    "prompt = num_samples * [prompt]\n",
    "prompt_ids = pipeline.prepare_inputs(prompt)\n",
    "\n",
    "# shard inputs and rng\n",
    "params = replicate(params)\n",
    "prng_seed = jax.random.split(prng_seed, jax.device_count())\n",
    "prompt_ids = shard(prompt_ids)\n",
    "\n",
    "images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\n",
    "images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n",
    "image.save(\"cat-train.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21255fdb-3bf2-4788-b11a-fda605a1d10e",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "- Learn how to load Textual Inversion embeddings and also use them as negative embeddings.\n",
    "- Learn how to use Textual Inversion for inference with `Stable Diffusion 1/2` and `Stable Diffusion XL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b679ec-729b-4aa7-b6fe-3585ddeaf290",
   "metadata": {},
   "source": [
    "-----------------\n",
    "- Other training pipelines also follow the sequence:\n",
    "  - script parameters → training script → launch the script.\n",
    "\n",
    "- The related repositories are as follows:\n",
    "\n",
    "### **Method | DreamBooth**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/dreambooth\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/dreambooth\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### **Method | LoRA**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/lora\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/text_to_image\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### **Method | Custom Diffusion**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/custom_diffusion\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/custom_diffusion\n",
    "pip install -r requirements.txt\n",
    "pip install clip-retrieval\n",
    "```\n",
    "\n",
    "### **Method | Latent Consistency Distillation**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/main/training/lcm_distill\n",
    "```\n",
    "git clone https://github.com/huggingface/diffusers\n",
    "cd diffusers\n",
    "pip install .\n",
    "\n",
    "cd examples/consistency_distillation\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-t2i",
   "language": "python",
   "name": "diffusers-t2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
