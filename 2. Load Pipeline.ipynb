{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0c3be1-899c-42cf-af14-f71fd4c29d09",
   "metadata": {},
   "source": [
    "## **2. Load Pipeline**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/v0.33.1/en/using-diffusers/loading\n",
    "\n",
    "```\n",
    "> Load Pipelines\n",
    "> Load Community Pipelines and Components\n",
    "> Load Schedulers and Models\n",
    "> Model Files and Layout\n",
    "> Load Adapter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f4d489-1c41-4db6-8d4e-43dd099c4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import HunyuanVideoPipeline\n",
    "from diffusers import StableDiffusionXLPipeline, HeunDiscreteScheduler, AutoencoderKL\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers import LMSDiscreteScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, DPMSolverMultistepScheduler\n",
    "from diffusers import FlaxStableDiffusionPipeline, FlaxDPMSolverMultistepScheduler\n",
    "from diffusers import UNet2DConditionModel, UNet2DModel, StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "from accelerate.utils import compute_module_sizes\n",
    "from diffusers.utils import export_to_gif\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "\n",
    "import jax\n",
    "from flax.jax_utils import replicate\n",
    "from flax.training.common_utils import shard\n",
    "from huggingface_hub import export_folder_as_dduf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9804567-4f04-41f9-949b-022995a7a48a",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### **Load Pipelines**\n",
    "- Diffusion systems consist of multiple components like parameterized models and schedulers that interact in complex ways.\n",
    "  - That is why we designed the `DiffusionPipeline` to wrap the complexity of the entire diffusion system into an easy-to-use API.\n",
    "  - At the same time, the `DiffusionPipeline` is entirely customizable so you can modify each component to build a diffusion system for your use case.\n",
    "\n",
    "\n",
    "#### Load a pipeline\n",
    "- There are two ways to load a pipeline for a task:\n",
    "1. Load the generic `DiffusionPipeline` class and allow it to automatically detect the correct pipeline class from the checkpoint.\n",
    "2. Load a specific pipeline class for a specific task.\n",
    "\n",
    "- The `DiffusionPipeline` class is a simple and generic way to load the latest trending diffusion model from the Hub.\n",
    "  - It uses the `from_pretrained()` method to automatically detect the correct pipeline class for a task from the checkpoint, downloads and caches all the required configuration and weight files, and returns a pipeline ready for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d23c34-74a0-45ec-9ca2-bb28bcc4343f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0df25e7c4c64fe7b2c016dfe3476052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10916a282b841928105cc2a066c992f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generic Pipeline\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)\n",
    "\n",
    "# Specific Pipeline\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077851f-caf4-4664-95f8-61cea932f2f8",
   "metadata": {},
   "source": [
    "#### [**ðŸ‘‰ðŸ‘‰ Diffusers Pipeline Memory Calculator**](https://huggingface.co/docs/diffusers/v0.33.1/en/using-diffusers/loading?pipelines=generic+pipeline#load-a-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e1d70-ad73-4fa1-b3c9-b4c1d993fd92",
   "metadata": {},
   "source": [
    "#### Specifying Component-Specific Data Types\n",
    "- You can customize the data types for individual sub-models by passing a dictionary to the `torch_dtype` parameter.\n",
    "- This allows you to load different components of a pipeline in different floating point precisions.\n",
    "  - If you want to load the transformer with `torch.bfloat16` and all other components with `torch.float16`, you can pass a dictionary mapping:\n",
    "- If a component is not explicitly specified in the dictionary and no default is provided, it will be loaded with `torch.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8373603-2dcb-453e-9840-13504dac287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = HunyuanVideoPipeline.from_pretrained(\n",
    "    \"hunyuanvideo-community/HunyuanVideo\",\n",
    "    torch_dtype={\"transformer\": torch.bfloat16, \"default\": torch.float16},\n",
    ")\n",
    "print(pipe.transformer.dtype, pipe.vae.dtype)  # (torch.bfloat16, torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b01906-8d27-4e04-9c74-348fb8ffd1b9",
   "metadata": {},
   "source": [
    "#### Local pipeline\n",
    "- To load a pipeline locally, use git-lfs to manually download a checkpoint to your local disk.\n",
    "\n",
    "```\n",
    "git-lfs install\n",
    "git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5\n",
    "```\n",
    "\n",
    "- This creates a local folder, `./stable-diffusion-v1-5`, on your disk and you should pass its path to `from_pretrained()`.\n",
    "  - The `from_pretrained()` method wonâ€™t download files from the Hub when it detects a local path, but this also means it **wonâ€™t download and cache the latest changes to a checkpoint**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c56f5-9a10-4988-810c-3c3154a42d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion = DiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f7e72-ab7f-4df5-8511-e3979a0b7029",
   "metadata": {},
   "source": [
    "#### Customize a pipeline\n",
    "- Customize the default `stabilityai/stable-diffusion-xl-base-1.0` checkpoint with:\n",
    "- The `HeunDiscreteScheduler` to generate higher quality images at the expense of slower generation speed.\n",
    "  - You must pass the `subfolder=\"scheduler\"` parameter in `from_pretrained()` to load the scheduler configuration into the correct subfolder of the pipeline repository.\n",
    "  - A more stable VAE that runs in fp16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8e0e5-3cfa-4c3b-88c1-f05620db1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = HeunDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7b038-1129-4afb-8061-ffc0d1f65320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the new schedular and VAE to the StableDiffusionXLPipeline\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "  \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "  scheduler=scheduler,\n",
    "  vae=vae,\n",
    "  torch_dtype=torch.float16,\n",
    "  variant=\"fp16\",\n",
    "  use_safetensors=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1afdd7-9f24-430c-9eaa-88185ad02474",
   "metadata": {},
   "source": [
    "#### Reuse a pipeline\n",
    "- When you load multiple pipelines that share the same model components, it makes sense to reuse the shared components instead of reloading everything into memory again, especially if your hardware is memory-constrained.\n",
    "  - You generated an image with the `StableDiffusionPipeline` but you want to improve its quality with the `StableDiffusionSAGPipeline`.\n",
    "    - **Both of these pipelines share the same pretrained model**, so itâ€™d be a waste of memory to load the same model twice.\n",
    "  - You want to add a model component, like a `MotionAdapter`, to `AnimateDiffPipeline` which was instantiated from an existing `StableDiffusionPipeline`.\n",
    "    - **Both pipelines share the same pretrained model**, so itâ€™d be a waste of memory to load an entirely new pipeline again.\n",
    "   \n",
    "- With the `DiffusionPipeline.from_pipe()` API, you can switch between multiple pipelines to take advantage of their different features without increasing memory-usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb80e76c-b550-4243-a71f-d4f8b8fd4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png\")\n",
    "\n",
    "pipe_sd = DiffusionPipeline.from_pretrained(\"SG161222/Realistic_Vision_V6.0_B1_noVAE\", torch_dtype=torch.float16)\n",
    "pipe_sd.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n",
    "pipe_sd.set_ip_adapter_scale(0.6)\n",
    "pipe_sd.to(\"cuda\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "out_sd = pipe_sd(\n",
    "    prompt=\"bear eats pizza\",\n",
    "    negative_prompt=\"wrong white balance, dark, sketches,worst quality,low quality\",\n",
    "    ip_adapter_image=image,\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "out_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097423b1-fd73-4976-a857-af1e7bad4159",
   "metadata": {},
   "source": [
    "- For reference, you can check how much memory this process consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4a1fc-942f-47b8-9543-8727ca8e6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "print(f\"Max memory allocated: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated())} GB\")\n",
    "\"Max memory allocated: 4.406213283538818 GB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc83d6-7917-42b1-afec-ce31559a04f4",
   "metadata": {},
   "source": [
    "- Reuse the same pipeline components from `StableDiffusionPipeline` in `StableDiffusionSAGPipeline` with the `from_pipe()` method.\n",
    "- Some pipeline methods may not function properly on new pipelines created with `from_pipe()`.\n",
    "  - The `enable_model_cpu_offload()` method installs hooks on the model components based on a unique offloading sequence for each pipeline.\n",
    "  - If the models are executed in a different order in the new pipeline, the CPU offloading may not work correctly.\n",
    "  - To ensure everything works as expected, we recommend **re-applying a pipeline method on a new pipeline created with `from_pipe()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a77ae-6daf-4ecf-b56f-ecf4762bd034",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_sag = StableDiffusionSAGPipeline.from_pipe(\n",
    "    pipe_sd\n",
    ")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "out_sag = pipe_sag(\n",
    "    prompt=\"bear eats pizza\",\n",
    "    negative_prompt=\"wrong white balance, dark, sketches,worst quality,low quality\",\n",
    "    ip_adapter_image=image,\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    "    guidance_scale=1.0,\n",
    "    sag_scale=0.75\n",
    ").images[0]\n",
    "out_sag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de0e42-1c59-4487-b33e-62d4d5988f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max memory allocated: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated())} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12986fc6-5504-4676-ac94-0fd8d41fb245",
   "metadata": {},
   "source": [
    "#### Modify `from_pipe` components\n",
    "- Pipelines loaded with `from_pipe()` can be customized with different model components or methods.\n",
    "  - Whenever you modify the state of the model components, it affects all the other pipelines that share the same components.\n",
    "  - If you call `unload_ip_adapter()` on the `StableDiffusionSAGPipeline`, you wonâ€™t be able to use IP-Adapter with the `StableDiffusionPipeline` because itâ€™s been removed from their shared components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e5d8e6-f78f-4ac3-949e-65a6c14b90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.sag_unload_ip_adapter()\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "out_sd = pipe_sd(\n",
    "    prompt=\"bear eats pizza\",\n",
    "    negative_prompt=\"wrong white balance, dark, sketches,worst quality,low quality\",\n",
    "    ip_adapter_image=image,\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1108f7-7002-4d0c-bf04-3bacb52db19b",
   "metadata": {},
   "source": [
    "#### Safety checker\n",
    "\n",
    "- Diffusers implements a safety checker for Stable Diffusion models which can **generate harmful content**.\n",
    "  - The safety checker screens the generated output against known hardcoded `not-safe-for-work (NSFW)` content.\n",
    "  - If for whatever reason youâ€™d like to disable the safety checker, pass `safety_checker=None` to the `from_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08cda68-0dc0-451a-9a8d-0dd55058fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", safety_checker=None, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455a892-ed17-4252-8972-9c4b47d9652e",
   "metadata": {},
   "source": [
    "#### Checkpoint variants\n",
    "- A checkpoint variant is usually a checkpoint whose weights are:\n",
    "  - Stored in a different floating point type, such as `torch.float16`, because it only requires half the bandwidth and storage to download.\n",
    "  - You canâ€™t use this variant if youâ€™re continuing training or using a CPU.\n",
    "- Non-exponential mean averaged (EMA) weights which shouldnâ€™t be used for inference.\n",
    "  - You should use this variant to continue finetuning a model.\n",
    "- When the checkpoints have identical model structures, but they were trained on different datasets and with a different training setup, they should be stored in separate repositories.\n",
    "- A variant is identical to the original checkpoint. They have exactly the same serialization format (like safetensors), model structure, and their weights have identical tensor shapes.\n",
    "  - `torch_dtype` specifies the floating point precision of the loaded checkpoint.\n",
    "  - If you want to save bandwidth by loading a fp16 variant, you should set `variant=\"fp16\"` and `torch_dtype=torch.float16` to convert the weights to fp16.\n",
    "  - If you only set `torch_dtype=torch.float16`, the default fp32 weights are downloaded first and then converted to fp16.\n",
    "\n",
    "- Variant specifies which files should be loaded from the repository.\n",
    "  - If you want to load a non-EMA variant of a UNet from `stable-diffusion-v1-5/stable-diffusion-v1-5`, set `variant=\"non_ema\"` to download the `non_ema` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42fcea-1458-44be-a478-80fbfcf1b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp16\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", variant=\"fp16\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "\n",
    "# non-EMA\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", variant=\"non_ema\", use_safetensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c0502-8717-46fd-922b-b5bb9594e699",
   "metadata": {},
   "source": [
    "#### DiffusionPipeline\n",
    "- As a class method, `DiffusionPipeline.from_pretrained()` is responsible for two things:\n",
    "  - Download the latest version of the folder structure required for inference and cache it.\n",
    "    - If the latest folder structure is available in the local cache, `DiffusionPipeline.from_pretrained()` reuses the cache and wonâ€™t redownload the files.\n",
    "  - Load the cached weights into the correct pipeline class - retrieved from the `model_index.json` file - and return an instance of it.\n",
    "  - The pipelinesâ€™ underlying folder structure corresponds directly with their class instances.\n",
    "  - The `StableDiffusionPipeline` corresponds to the folder structure in `stable-diffusion-v1-5/stable-diffusion-v1-5`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c735d-ee71-4fcf-bf2d-b22b4c78e417",
   "metadata": {},
   "source": [
    "| Object Name| Description |\n",
    "|--------------|-------------------------------------|\n",
    "| feature_extractor | a CLIPImageProcessor from Transformers. |\n",
    "| safety_checker | a component for screening against harmful content. |\n",
    "| scheduler  | an instance of PNDMScheduler. |\n",
    "| text_encoder | a CLIPTextModel from Transformers. |\n",
    "| tokenizer | a CLIPTokenizer from Transformers. |\n",
    "| unet |  an instance of UNet2DConditionModel. |\n",
    "| vae |  an instance of AutoencoderKL. |\n",
    "\n",
    "\n",
    "- **Components in the repository**\n",
    "```\n",
    ".\n",
    "â”œâ”€â”€ feature_extractor\n",
    "â”‚   â””â”€â”€ preprocessor_config.json\n",
    "â”œâ”€â”€ model_index.json\n",
    "â”œâ”€â”€ safety_checker\n",
    "â”‚   â”œâ”€â”€ config.json\n",
    "|   â”œâ”€â”€ model.fp16.safetensors\n",
    "â”‚   â”œâ”€â”€ model.safetensors\n",
    "â”‚   â”œâ”€â”€ pytorch_model.bin\n",
    "|   â””â”€â”€ pytorch_model.fp16.bin\n",
    "â”œâ”€â”€ scheduler\n",
    "â”‚   â””â”€â”€ scheduler_config.json\n",
    "â”œâ”€â”€ text_encoder\n",
    "â”‚   â”œâ”€â”€ config.json\n",
    "|   â”œâ”€â”€ model.fp16.safetensors\n",
    "â”‚   â”œâ”€â”€ model.safetensors\n",
    "â”‚   |â”€â”€ pytorch_model.bin\n",
    "|   â””â”€â”€ pytorch_model.fp16.bin\n",
    "â”œâ”€â”€ tokenizer\n",
    "â”‚   â”œâ”€â”€ merges.txt\n",
    "â”‚   â”œâ”€â”€ special_tokens_map.json\n",
    "â”‚   â”œâ”€â”€ tokenizer_config.json\n",
    "â”‚   â””â”€â”€ vocab.json\n",
    "â”œâ”€â”€ unet\n",
    "â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â”œâ”€â”€ diffusion_pytorch_model.bin\n",
    "|   |â”€â”€ diffusion_pytorch_model.fp16.bin\n",
    "â”‚   |â”€â”€ diffusion_pytorch_model.f16.safetensors\n",
    "â”‚   |â”€â”€ diffusion_pytorch_model.non_ema.bin\n",
    "â”‚   |â”€â”€ diffusion_pytorch_model.non_ema.safetensors\n",
    "â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors\n",
    "|â”€â”€ vae\n",
    ".   â”œâ”€â”€ config.json\n",
    ".   â”œâ”€â”€ diffusion_pytorch_model.bin\n",
    "    â”œâ”€â”€ diffusion_pytorch_model.fp16.bin\n",
    "    â”œâ”€â”€ diffusion_pytorch_model.fp16.safetensors\n",
    "    â””â”€â”€ diffusion_pytorch_model.safetensors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d12d6c93-8139-4cc3-8506-2d5fa447b901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe3d403b86e4043bc6ae9752c580820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StableDiffusionPipeline {\n",
      "  \"_class_name\": \"StableDiffusionPipeline\",\n",
      "  \"_diffusers_version\": \"0.33.1\",\n",
      "  \"_name_or_path\": \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
      "  \"feature_extractor\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPImageProcessor\"\n",
      "  ],\n",
      "  \"image_encoder\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"requires_safety_checker\": true,\n",
      "  \"safety_checker\": [\n",
      "    \"stable_diffusion\",\n",
      "    \"StableDiffusionSafetyChecker\"\n",
      "  ],\n",
      "  \"scheduler\": [\n",
      "    \"diffusers\",\n",
      "    \"PNDMScheduler\"\n",
      "  ],\n",
      "  \"text_encoder\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTextModel\"\n",
      "  ],\n",
      "  \"tokenizer\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTokenizer\"\n",
      "  ],\n",
      "  \"unet\": [\n",
      "    \"diffusers\",\n",
      "    \"UNet2DConditionModel\"\n",
      "  ],\n",
      "  \"vae\": [\n",
      "    \"diffusers\",\n",
      "    \"AutoencoderKL\"\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bb028-e8fc-47a7-9b99-61c52c86b300",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "### **Load Community Pipelines and Components**\n",
    "#### Load from a local file\n",
    "- Community pipelines can also be loaded from a local file if you pass a file path instead.\n",
    "  - The path to the passed directory must contain a pipeline.py file that contains the pipeline class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7f019-589e-4505-8160-b7e279f989c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    custom_pipeline=\"./path/to/pipeline_directory/\",\n",
    "    clip_model=clip_model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    use_safetensors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b93423-8287-49d4-94cc-ab2a77d067be",
   "metadata": {},
   "source": [
    "#### Load from a specific version\n",
    "- By default, community pipelines are loaded from the latest stable version of Diffusers.\n",
    "  - To load a community pipeline from another version, use the `custom_revision` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b0b49-8152-48c4-98ce-139ec313c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    custom_pipeline=\"clip_guided_stable_diffusion\",\n",
    "    custom_revision=\"main\",\n",
    "    clip_model=clip_model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    use_safetensors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58069c6-82f5-4d03-9aed-b5e3ffe0bee3",
   "metadata": {},
   "source": [
    "#### Load with from_pipe\n",
    "- Community pipelines can also be loaded with the `from_pipe()` method which allows you to load and reuse multiple pipelines without any additional memory overhead (learn more in the Reuse a pipeline guide).\n",
    "  - The memory requirement is determined by the largest single pipeline loaded.\n",
    " \n",
    "- Load a community pipeline that supports long prompts with weighting from a Stable Diffusion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c738ba5-0efe-43ae-9fac-8db4c2c09f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_sd = DiffusionPipeline.from_pretrained(\"emilianJR/CyberRealistic_V3\", torch_dtype=torch.float16)\n",
    "pipe_sd.to(\"cuda\")\n",
    "# load long prompt weighting pipeline\n",
    "pipe_lpw = DiffusionPipeline.from_pipe(\n",
    "    pipe_sd,\n",
    "    custom_pipeline=\"lpw_stable_diffusion\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"cat, hiding in the leaves, ((rain)), zazie rainyday, beautiful eyes, macro shot, colorful details, natural lighting, amazing composition, subsurface scattering, amazing textures, filmic, soft light, ultra-detailed eyes, intricate details, detailed texture, light source contrast, dramatic shadows, cinematic light, depth of field, film grain, noise, dark background, hyperrealistic dslr film still, dim volumetric cinematic lighting\"\n",
    "neg_prompt = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(20)\n",
    "out_lpw = pipe_lpw(\n",
    "    prompt,\n",
    "    negative_prompt=neg_prompt,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    max_embeddings_multiples=3,\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    "    ).images[0]\n",
    "out_lpw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf1747-d535-485c-99a0-a15bcbab2595",
   "metadata": {},
   "source": [
    "### **Load Schedulers and Models**\n",
    "- Diffusion pipelines are a collection of interchangeable schedulers and models that can be mixed and matched to tailor a pipeline to a specific use case.\n",
    "  - The scheduler encapsulates the entire denoising process such as the number of denoising steps and the algorithm for finding the denoised sample.\n",
    "  - A scheduler is not parameterized or trained so they donâ€™t take very much memory.\n",
    "  - The model is usually only concerned with the forward pass of going from a noisy input to a less noisy sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e5ff7cf-bdcf-411a-88a0-990571997fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fbc961376b44d5acaa3f088939168f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1103dd8c-dbfc-4e39-b397-e0664484c155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PNDMScheduler {\n",
       "  \"_class_name\": \"PNDMScheduler\",\n",
       "  \"_diffusers_version\": \"0.33.1\",\n",
       "  \"beta_end\": 0.012,\n",
       "  \"beta_schedule\": \"scaled_linear\",\n",
       "  \"beta_start\": 0.00085,\n",
       "  \"clip_sample\": false,\n",
       "  \"num_train_timesteps\": 1000,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"set_alpha_to_one\": false,\n",
       "  \"skip_prk_steps\": true,\n",
       "  \"steps_offset\": 1,\n",
       "  \"timestep_spacing\": \"leading\",\n",
       "  \"trained_betas\": null\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53941a74-1def-4a79-9e14-77d44135208c",
   "metadata": {},
   "source": [
    "#### Load a scheduler\n",
    "- Schedulers are defined by a configuration file that can be used by a variety of schedulers.\n",
    "  - Load a scheduler with the `SchedulerMixin.from_pretrained()` method, and specify the subfolder parameter to load the configuration file into the correct subfolder of the pipeline repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6aab2e7-2269-49a6-88ec-cf49757b0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim = DDIMScheduler.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78b082-3d0d-4c0c-afb2-2d5a0f025aa9",
   "metadata": {},
   "source": [
    "- Then you can pass the newly loaded scheduler to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933d4a4-d5ad-44df-9d53-bf2a7e3b6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", scheduler=ddim, torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b0278-6f63-4af7-91bb-b06e6568f5b1",
   "metadata": {},
   "source": [
    "#### Compare schedulers\n",
    "- Schedulers have their own unique strengths and weaknesses, making it difficult to quantitatively compare which scheduler works best for a pipeline.\n",
    "  - You typically have to make a trade-off between denoising speed and denoising quality.\n",
    "  - We recommend trying out different schedulers to find one that works best for your use case.\n",
    "  - Call the `pipeline.scheduler.compatibles` attribute to see what schedulers are compatible with a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf3395ec-311d-4e2a-b516-f710b7380210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = \"A photograph of an astronaut riding a horse on Mars, high resolution, high definition.\"\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cbece-5b44-482f-8e4d-ba600a890e18",
   "metadata": {},
   "source": [
    "- To change the pipelines scheduler, use the `from_config()` method to load a different schedulerâ€™s `pipeline.scheduler.config` into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "248849cb-245c-4549-9e35-0fefcb2f93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "image = pipeline(prompt, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca44af3-2460-4b82-80d5-f393164dbb29",
   "metadata": {},
   "source": [
    "#### Flax schedulers\n",
    "- To compare Flax schedulers, you need to additionally load the scheduler state into the model parameters.\n",
    "- Change the default scheduler in `FlaxStableDiffusionPipeline` to use the super fast `FlaxDPMSolverMultistepScheduler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7035bb3-66cd-47cb-b775-ffc634e13780",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler, scheduler_state = FlaxDPMSolverMultistepScheduler.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\"\n",
    ")\n",
    "pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    scheduler=scheduler,\n",
    "    variant=\"bf16\",\n",
    "    dtype=jax.numpy.bfloat16,\n",
    ")\n",
    "params[\"scheduler\"] = scheduler_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96b7f0-2adf-49f5-b1ac-a70fb08820ca",
   "metadata": {},
   "source": [
    "- Take advantage of Flaxâ€™s compatibility with TPUs to generate a number of images in parallel.\n",
    "  - Youâ€™ll need to make a copy of the model parameters for each available device and then split the inputs across them to generate your desired number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187fbf2-05c0-4c7f-83c3-eacad2b636e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1 image per parallel device (8 on TPUv2-8 or TPUv3-8)\n",
    "prompt = \"A photograph of an astronaut riding a horse on Mars, high resolution, high definition.\"\n",
    "num_samples = jax.device_count()\n",
    "prompt_ids = pipeline.prepare_inputs([prompt] * num_samples)\n",
    "\n",
    "prng_seed = jax.random.PRNGKey(0)\n",
    "num_inference_steps = 25\n",
    "\n",
    "# shard inputs and rng\n",
    "params = replicate(params)\n",
    "prng_seed = jax.random.split(prng_seed, jax.device_count())\n",
    "prompt_ids = shard(prompt_ids)\n",
    "\n",
    "images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\n",
    "images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53731c9e-109b-4dfd-9763-7a447e9e827e",
   "metadata": {},
   "source": [
    "#### Models\n",
    "- Models are loaded from the `ModelMixin.from_pretrained()` method, which downloads and caches the latest version of the model weights and configurations.\n",
    "  - If the latest files are available in the local cache, `from_pretrained()` reuses files in the cache instead of re-downloading them.\n",
    "\n",
    "- Models can be loaded from a subfolder with the subfolder argument.\n",
    "  - The model weights for `stable-diffusion-v1-5/stable-diffusion-v1-5` are stored in the unet subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6f765-b8d2-45a0-8200-6e6d95db04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"unet\", use_safetensors=True)\n",
    "\n",
    "# Directly loaded from a repository\n",
    "unet = UNet2DModel.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20c304-9c37-416b-b795-98e3a54d8e84",
   "metadata": {},
   "source": [
    "- To load and save model variants, specify the variant argument in `ModelMixin.from_pretrained()` and `ModelMixin.save_pretrained()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c375b8-f902-48e0-8eed-14b8737ffc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", subfolder=\"unet\", variant=\"non_ema\", use_safetensors=True\n",
    ")\n",
    "unet.save_pretrained(\"./local-unet\", variant=\"non_ema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54fa47-2c0a-4db0-9dda-e4026ccb8d9c",
   "metadata": {},
   "source": [
    "--------------\n",
    "### **Model Files and Layout**\n",
    "- Diffusion models are saved in various file types and organized in different layouts.\n",
    "  - Diffusers stores model weights as safetensors files in Diffusers-multifolder layout and it also supports loading files (like safetensors and ckpt files) from a single-file layout which is commonly used in the diffusion ecosystem.\n",
    " \n",
    "#### Files\n",
    "- PyTorch model weights are typically saved with Pythonâ€™s pickle utility as ckpt or bin files.\n",
    "  - However, pickle is not secure and pickled files may contain malicious code that can be executed.\n",
    "  - This vulnerability is a serious concern given the popularity of model sharing.\n",
    "- To address this security issue, the Safetensors library was developed as a secure alternative to pickle, which saves models as safetensors files.\n",
    "\n",
    "#### SafeTensors\n",
    "- Safetensors is a safe and fast file format for securely storing and loading tensors.\n",
    "  - Safetensors restricts the header size to limit certain types of attacks, supports lazy loading (useful for distributed setups), and has generally faster loading speeds.\n",
    "- Safetensors stores weights in a safetensors file.\n",
    "  - Diffusers loads safetensors files by default if theyâ€™re available and the Safetensors library is installed.\n",
    " \n",
    "- There are two ways safetensors files can be organized:\n",
    "  - **Diffusers-multifolder layout**: there may be several separate safetensors files, one for each pipeline component (text encoder, UNet, VAE), organized in subfolders (check out the stable-diffusion-v1-5/stable-diffusion-v1-5 repository as an example)\n",
    "  - **single-file layout**: all the model weights may be saved in a single file (check out the WarriorMama777/OrangeMixs repository as an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db4254-a176-4c5c-911d-86549f85fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-folder\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Single-file\n",
    "pipeline = StableDiffusionPipeline.from_single_file(\n",
    "    \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53919972-c08d-471f-a3d7-3703e63a60af",
   "metadata": {},
   "source": [
    "#### LoRA files\n",
    "- LoRA is a lightweight adapter that is fast and easy to train, making them especially popular for generating images in a certain way or style.\n",
    "  - These adapters are commonly stored in a safetensors file, and are widely popular on model sharing platforms like civitai.\n",
    "  - LoRAs are loaded into a base model with the `load_lora_weights()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a572ee8-8bc8-4c04-acd2-2aca669374e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"Lykon/dreamshaper-xl-1-0\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# download LoRA weights\n",
    "!wget https://civitai.com/api/download/models/168776 -O blueprintify.safetensors\n",
    "\n",
    "# load LoRA weights\n",
    "pipeline.load_lora_weights(\".\", weight_name=\"blueprintify.safetensors\")\n",
    "prompt = \"bl3uprint, a highly detailed blueprint of the empire state building, explaining how to build all parts, many txt, blueprint grid backdrop\"\n",
    "negative_prompt = \"lowres, cropped, worst quality, low quality, normal quality, artifacts, signature, watermark, username, blurry, more than one bridge, bad architecture\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    generator=torch.manual_seed(0),\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45189054-c8b2-4e82-b60b-548777f5dfb5",
   "metadata": {},
   "source": [
    "#### ckpt\n",
    "- Pickled files may be unsafe because they can be exploited to execute malicious code.\n",
    "  - It is recommended to use safetensors files instead where possible, or convert the weights to safetensors files.\n",
    "- PyTorchâ€™s torch.save function uses Pythonâ€™s pickle utility to serialize and save models.\n",
    "  - These files are saved as a ckpt file and they contain the entire modelâ€™s weights.\n",
    "\n",
    "- Use the `from_single_file()` method to directly load a ckpt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6cb7c-a520-43dc-ae69-98f148765938",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_single_file(\n",
    "    \"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/blob/main/v1-5-pruned.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51897055-d538-49f9-b879-bdb309f77b01",
   "metadata": {},
   "source": [
    "#### Storage layout: Diffusers-multifolder\n",
    "- There are two ways model files are organized, either in a Diffusers-multifolder layout or in a single-file layout.\n",
    "  - The Diffusers-multifolder layout is the default, and each component file (text encoder, UNet, VAE) is stored in a separate subfolder.\n",
    "  - Diffusers also supports loading models from a single-file layout where all the components are bundled together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7de571-eaa3-43e4-ab38-1ad863b1eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98050e-f04a-4c67-898e-48fbe916cf05",
   "metadata": {},
   "source": [
    "- **Benefits**\n",
    "  - Faster to load each component file individually or in parallel.\n",
    "  - Reduced memory usage because you only load the components you need.\n",
    "    - Models like SDXL Turbo, SDXL Lightning, and Hyper-SD have the same components except for the UNet.\n",
    "    - You can reuse their shared components with the `from_pipe()` method without consuming any additional memory (take a look at the Reuse a pipeline guide) and only load the UNet.\n",
    "    - This way, you donâ€™t need to download redundant components and unnecessarily use more memory.\n",
    "  - Reduced storage requirements because if a component, such as the SDXL VAE, is shared across multiple models, you only need to download and store a single copy of it instead of downloading and storing it multiple times.\n",
    "    - For 10 SDXL models, this can save ~3.5GB of storage. The storage savings is even greater for newer models like PixArt Sigma, where the text encoder alone is ~19GB!\n",
    "    - Flexibility to replace a component in the model with a newer or better version.\n",
    "  - Flexibility to replace a component in the model with a newer or better version.\n",
    "  - More visibility and information about a modelâ€™s components, which are stored in a config.json file in each component subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdcbe05-4780-43ea-8e37-0e520fa40ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download one model\n",
    "sdxl_pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# switch UNet for another model\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\",\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "# reuse all the same components in new model except for the UNet\n",
    "turbo_pipeline = StableDiffusionXLPipeline.from_pipe(\n",
    "    sdxl_pipeline, unet=unet,\n",
    ").to(\"cuda\")\n",
    "turbo_pipeline.scheduler = EulerDiscreteScheduler.from_config(\n",
    "    turbo_pipeline.scheduler.config,\n",
    "    timestep+spacing=\"trailing\"\n",
    ")\n",
    "image = turbo_pipeline(\n",
    "    \"an astronaut riding a unicorn on mars\",\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=0.0,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e695f8c-a82f-4fe0-9076-e1d0cb238b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b609737-a7f8-442c-9dfb-b666e779a2dd",
   "metadata": {},
   "source": [
    "#### Storage layout: Single-file\n",
    "- The single-file layout stores all the model weights in a single file.\n",
    "  - All the model components (text encoder, UNet, VAE) weights are kept together instead of separately in subfolders.\n",
    "  - This can be a safetensors or ckpt file.\n",
    "- To load from a single-file layout, use the `from_single_file()` method.\n",
    "- **Benefits**\n",
    "  - Easy compatibility with diffusion interfaces such as ComfyUI or Automatic1111 which commonly use a single-file layout.\n",
    "  - Easier to manage (download and share) a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a7815-7a9d-4c75-b8b3-914ab8e92416",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_single_file(\n",
    "    \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fcb09-24c7-45ef-b2e1-d97349f2d933",
   "metadata": {},
   "source": [
    "#### Storage layout: DDUF\n",
    "- DDUF(DDUF Diffusion Unified Format) is a file format designed to make storing, distributing, and using diffusion models much easier.\n",
    "  - Built on the ZIP file format, DDUF offers a standardized, efficient, and flexible way to package all parts of a diffusion model into a single, easy-to-manage file.\n",
    "  - It provides a balance between Diffusers multi-folder format and the widely popular single-file format.\n",
    "- Pass a checkpoint to the dduf_file parameter to load it in DiffusionPipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e3e3f-5456-4d1e-8de7-3948eaad9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"DDUF/FLUX.1-dev-DDUF\", dduf_file=\"FLUX.1-dev.dduf\", torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "image = pipe(\n",
    "    \"photo a cat holding a sign that says Diffusers\", num_inference_steps=50, guidance_scale=3.5\n",
    ").images[0]\n",
    "image.save(\"cat.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff70c8-0c25-4dcb-b93f-cc716a15c39a",
   "metadata": {},
   "source": [
    "- To save a pipeline as a .dduf checkpoint, use the `export_folder_as_dduf` utility, which takes care of all the necessary file-level validations.\n",
    "\n",
    "```\n",
    "Packaging and loading quantized checkpoints in the DDUF format is supported as long as they respect the multi-folder structure.\n",
    "\n",
    "## Convert layout and files\n",
    "\n",
    "Diffusers provides many scripts and methods to convert storage layouts and file formats to enable broader support across the diffusion ecosystem.\n",
    "\n",
    "Take a look at the [diffusers/scripts](https://github.com/huggingface/diffusers/tree/main/scripts) collection to find a script that fits your conversion needs.\n",
    "\n",
    "> Scripts that have \"`to_diffusers`\" appended at the end mean they convert a model to the Diffusers-multifolder layout. Each script has their own specific set of arguments for configuring the conversion, so make sure you check what arguments are available!\n",
    "\n",
    "For example, to convert a Stable Diffusion XL model stored in Diffusers-multifolder layout to a single-file layout, run the [convert_diffusers_to_original_sdxl.py](https://github.com/huggingface/diffusers/blob/main/scripts/convert_diffusers_to_original_sdxl.py) script. Provide the path to the model to convert, and the path to save the converted model to. You can optionally specify whether you want to save the model as a safetensors file and whether to save the model in half-precision.\n",
    "```\n",
    "\n",
    "```bash\n",
    "python convert_diffusers_to_original_sdxl.py --model_path path/to/model/to/convert --checkpoint_path path/to/save/model/to --use_safetensors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdcd5b-23b4-46e7-a05d-b5052d36c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "save_folder = \"flux-dev\"\n",
    "pipe.save_pretrained(\"flux-dev\")\n",
    "export_folder_as_dduf(\"flux-dev.dduf\", folder_path=save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2521ef-a58e-4b35-a46a-4042d1392883",
   "metadata": {},
   "source": [
    "- You can also save a model to Diffusers-multifolder layout with the `save_pretrained()` method.\n",
    "  - This creates a directory for you if it doesnâ€™t already exist, and it also saves the files as a safetensors file by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ec355-359c-4344-847d-f4968eec9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_single_file(\n",
    "    \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors\",\n",
    ")\n",
    "pipeline.save_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91016430-4a09-4d88-a38a-ced542673a38",
   "metadata": {},
   "source": [
    "#### Single-file layout usage\n",
    "- Pass the file path of the pipeline or model to the from_single_file() method to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660f4f9-425b-44e0-9dba-8b7dcd1b0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "ckpt_path = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0_0.9vae.safetensors\"\n",
    "pipeline = StableDiffusionXLPipeline.from_single_file(ckpt_path)\n",
    "\n",
    "# Model\n",
    "ckpt_path = \"https://huggingface.co/stabilityai/stable-cascade/blob/main/stage_b_lite.safetensors\"\n",
    "model = StableCascadeUNet.from_single_file(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e50232-45ec-46cd-a440-810367201ba7",
   "metadata": {},
   "source": [
    "- Customize components in the pipeline by passing them directly to the `from_single_file()` method.\n",
    "  - You can use a different scheduler in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284be49-6a72-4d67-bca1-96d0605a5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0_0.9vae.safetensors\"\n",
    "scheduler = DDIMScheduler()\n",
    "pipeline = StableDiffusionXLPipeline.from_single_file(ckpt_path, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ad451-b3bc-45e8-81d1-6f9656f69666",
   "metadata": {},
   "source": [
    "- Or you could use a ControlNet model in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b53f38-aa44-45b4-8817-e7d4040bad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.safetensors\"\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_canny\")\n",
    "pipeline = StableDiffusionControlNetPipeline.from_single_file(ckpt_path, controlnet=controlnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ecf74-9ab1-4529-a457-2b2015686446",
   "metadata": {},
   "source": [
    "### **Load Adapter**\n",
    "- There are several training techniques for personalizing diffusion models to generate images of a specific subject or images in certain styles.\n",
    "  - Each of these training methods produces a different type of adapter.\n",
    "  - Some of the adapters generate an entirely new model, while other adapters only modify a smaller set of embeddings or weights.\n",
    "  - This means the loading process for each adapter is also different.\n",
    "\n",
    "#### DreamBooth\n",
    "- DreamBooth finetunes an entire diffusion model on just several images of a subject to generate images of that subject in new styles and settings.\n",
    "  - This method works by using a special word in the prompt that the model learns to associate with the subject image.\n",
    "  - Of all the training methods, DreamBooth produces the largest file size (usually a few GBs) because it is a full checkpoint model.\n",
    "\n",
    "- Load the `herge_style` checkpoint, which is trained on just 10 images drawn by HergÃ©, to generate images in that style.\n",
    "  - For it to work, you need to include the special word `herge_style` in your prompt to trigger the checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "744cab25-ec31-452a-9c4e-0179eb49e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"sd-dreambooth-library/herge-style\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "prompt = \"A cute herge_style brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\n",
    "image = pipeline(prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73403e39-340a-43dc-928b-5ef77ba9087d",
   "metadata": {},
   "source": [
    "#### Textual inversion\n",
    "- Textual inversion is very similar to DreamBooth and it can also personalize a diffusion model to generate certain concepts (styles, objects) from just a few images.\n",
    "  - This method works by training and finding new embeddings that represent the images you provide with a special word in the prompt.\n",
    "  - As a result, the diffusion model weights stay the same and the training process produces a relatively tiny (a few KBs) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae38df-43a2-45e4-b1a4-c8c093c5c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", \n",
    "                                                     torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc3315-3348-4fbc-914d-ee24d004b84a",
   "metadata": {},
   "source": [
    "- Load the textual inversion embeddings with the `load_textual_inversion()` method and generate some images.\n",
    "  - Letâ€™s load the `sd-concepts-library/gta5-artwork` embeddings and youâ€™ll need to include the special word <gta5-artwork> in your prompt to trigger it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784367d8-41c1-4f0b-aee3-5529d8d2535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\n",
    "prompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, <gta5-artwork> style\"\n",
    "image = pipeline(prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1caa98-fa93-406d-b1ad-ed2e7a1388d8",
   "metadata": {},
   "source": [
    "- Textual inversion can also be trained on undesirable things to create negative embeddings to discourage a model from generating images with those undesirable things like blurry images or extra fingers on a hand.\n",
    "  - This can be an easy way to quickly improve your prompt.\n",
    "  - Youâ€™ll also load the embeddings with load_textual_inversion(), but this time, youâ€™ll need two more parameters:\n",
    "    - `weight_name`: specifies the weight file to load if the file was saved in the Diffusers format with a specific name or if the file is stored in the A1111 format\n",
    "    - `token`: specifies the special word to use in the prompt to trigger the embeddings\n",
    "- Load the sayakpaul/EasyNegative-test embeddings and use the token to generate an image with the negative embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c464e-8ec1-4676-b15b-da0d64bef91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.load_textual_inversion(\n",
    "    \"sayakpaul/EasyNegative-test\", weight_name=\"EasyNegative.safetensors\", token=\"EasyNegative\"\n",
    ")\n",
    "prompt = \"A cute brown bear eating a slice of pizza, stunning color scheme, masterpiece, illustration, EasyNegative\"\n",
    "negative_prompt = \"EasyNegative\"\n",
    "\n",
    "image = pipeline(prompt, negative_prompt=negative_prompt, num_inference_steps=50).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d264a-1415-4fe4-af5e-c8110d32c536",
   "metadata": {},
   "source": [
    "#### LoRA\n",
    "- Low-Rank Adaptation (LoRA) is a popular training technique because it is fast and generates smaller file sizes (a couple hundred MBs).\n",
    "  - LoRA can train a model to learn new styles from just a few images.\n",
    "  - It works by inserting new weights into the diffusion model and then only the new weights are trained instead of the entire model.\n",
    "  - This makes LoRAs faster to train and easier to store.\n",
    "  - Use the `load_lora_weights()` method to load the `ostris/super-cereal-sdxl-lora` weights and specify the weights filename from the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395b88b-0d40-41fe-b6a6-7380687b7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "                                                     torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_lora_weights(\"ostris/super-cereal-sdxl-lora\", weight_name=\"cereal_box_sdxl_v1.safetensors\")\n",
    "prompt = \"bears, pizza bites\"\n",
    "image = pipeline(prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f61207-7ad1-4175-bd61-35dff769f5e0",
   "metadata": {},
   "source": [
    "- The `load_lora_weights()` method loads LoRA weights into both the UNet and text encoder.\n",
    "  - It is the preferred way for loading LoRAs because it can handle cases where:\n",
    "    - The LoRA weights donâ€™t have separate identifiers for the UNet and text encoder\n",
    "    - the LoRA weights have separate identifiers for the UNet and text encoder\n",
    "  - To directly load (and save) a LoRA adapter at the model-level, use `~PeftAdapterMixin.load_lora_adapter`, which builds and prepares the necessary model configuration for the adapter.\n",
    "  - Like `load_lora_weights()`, `PeftAdapterMixin.load_lora_adapter` can load LoRAs for both the UNet and text encoder.\n",
    "  - Use the `weight_name` parameter to specify the specific weight file and the prefix parameter to filter for the appropriate state dicts (\"unet\" in this case) to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c392abc-2b4a-46b7-bd38-b91254a0b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "                                                     torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.unet.load_lora_adapter(\"jbilcke-hf/sdxl-cinematic-1\", weight_name=\"pytorch_lora_weights.safetensors\", prefix=\"unet\")\n",
    "\n",
    "# use cnmt in the prompt to trigger the LoRA\n",
    "prompt = \"A cute cnmt eating a slice of pizza, stunning color scheme, masterpiece, illustration\"\n",
    "image = pipeline(prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b7fa8-6988-4ccc-a3f7-f6a32fe4383a",
   "metadata": {},
   "source": [
    "- Save an adapter with `~PeftAdapterMixin.save_lora_adapter`.\n",
    "  - To unload the LoRA weights, use the `unload_lora_weights()` method to discard the LoRA weights and restore the model to its original weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab60e9-b592-40ed-9aa0-b35d9e850d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.unload_lora_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adac0ca-3282-4348-b53a-70685dc9a726",
   "metadata": {},
   "source": [
    "#### Adjust LoRA weight scale\n",
    "- For both `load_lora_weights()` and `load_attn_procs()`, you can pass the `cross_attention_kwargs={\"scale\": 0.5}` parameter to adjust how much of the LoRA weights to use.\n",
    "  - A value of 0 is the same as only using the base model weights, and a value of 1 is equivalent to using the fully finetuned LoRA.\n",
    "\n",
    "- For more granular control on the amount of LoRA weights used per layer, you can use `set_adapters()` and pass a dictionary specifying by how much to scale the weights in each layer by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36da68c-373b-423c-a39c-e4039dc83de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ... # create pipeline\n",
    "pipe.load_lora_weights(..., adapter_name=\"my_adapter\")\n",
    "scales = {\n",
    "    \"text_encoder\": 0.5,\n",
    "    \"text_encoder_2\": 0.5,  # only usable if pipe has a 2nd text encoder\n",
    "    \"unet\": {\n",
    "        \"down\": 0.9,  # all transformers in the down-part will use scale 0.9\n",
    "        # \"mid\"  # in this example \"mid\" is not given, therefore all transformers in the mid part will use the default scale 1.0\n",
    "        \"up\": {\n",
    "            \"block_0\": 0.6,  # all 3 transformers in the 0th block in the up-part will use scale 0.6\n",
    "            \"block_1\": [0.4, 0.8, 1.0],  # the 3 transformers in the 1st block in the up-part will use scales 0.4, 0.8 and 1.0 respectively\n",
    "        }\n",
    "    }\n",
    "}\n",
    "pipe.set_adapters(\"my_adapter\", scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc9d36-fe20-4411-95b1-9272724bee8c",
   "metadata": {},
   "source": [
    "#### Hotswapping LoRA adapters\n",
    "- A common use case when serving multiple adapters is to load one adapter first, generate images, load another adapter, generate more images, load another adapter, etc.\n",
    "  - This workflow normally requires calling `load_lora_weights()`, `set_adapters()`, and possibly `delete_adapters()` to save memory.\n",
    "  - Moreover, if the model is compiled using `torch.compile`, performing these steps requires recompilation, which takes time.\n",
    "\n",
    "- To better support this common workflow, you can **hotswap** a LoRA adapter, to avoid accumulating memory and in some cases, recompilation.\n",
    "  - It requires an adapter to already be loaded, and the new adapter weights are swapped in-place for the existing adapter.\n",
    "\n",
    "- Pass `hotswap=True` when loading a LoRA adapter to enable this feature.\n",
    "  - It is important to indicate the name of the existing adapter, (`default_0` is the default adapter name), to be swapped.\n",
    "  - If you loaded the first adapter with a different name, use that name instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58561831-5f06-4367-882e-9050dcd1a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ...\n",
    "# load adapter 1 as normal\n",
    "pipeline.load_lora_weights(file_name_adapter_1)\n",
    "# generate some images with adapter 1\n",
    "...\n",
    "# now hot swap the 2nd adapter\n",
    "pipeline.load_lora_weights(file_name_adapter_2, hotswap=True, adapter_name=\"default_0\")\n",
    "# generate images with adapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589adf1-4b15-47c1-8133-c061fde5c1c2",
   "metadata": {},
   "source": [
    "- For compiled models, it is often (though not always if the second adapter targets identical LoRA ranks and scales) necessary to call `enable_lora_hotswap()` to avoid recompilation.\n",
    "  - Use `enable_lora_hotswap()` before loading the first adapter, and `torch.compile` should be called after loading the first adapter.\n",
    " \n",
    "- The `target_rank=max_rank` argument is important for setting the maximum rank among all LoRA adapters that will be loaded.\n",
    "  - If you have one adapter with rank 8 and another with rank 16, pass `target_rank=16`.\n",
    "  - You should use a higher value if in doubt. By default, this value is 128.\n",
    "\n",
    "- There can be situations where recompilation is unavoidable. For example, if the hotswapped adapter targets more layers than the initial adapter, then recompilation is triggered. \n",
    "  - Try to load the adapter that targets the most layers first.\n",
    "  - Refer to the PEFT docs on hotswapping for more details about the limitations of this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ba760-07b9-42e9-baf6-bdf7669fa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ...\n",
    "# call this extra method\n",
    "pipe.enable_lora_hotswap(target_rank=max_rank)\n",
    "# now load adapter 1\n",
    "pipe.load_lora_weights(file_name_adapter_1)\n",
    "# now compile the unet of the pipeline\n",
    "pipe.unet = torch.compile(pipeline.unet, ...)\n",
    "# generate some images with adapter 1\n",
    "...\n",
    "# now hot swap adapter 2\n",
    "pipeline.load_lora_weights(file_name_adapter_2, hotswap=True, adapter_name=\"default_0\")\n",
    "# generate images with adapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0b463-aba7-4af8-85e3-4e9bcd846de0",
   "metadata": {},
   "source": [
    "#### Kohya and TheLastBen\n",
    "- Other popular LoRA trainers from the community include those by Kohya and TheLastBen.\n",
    "  - These trainers create different LoRA checkpoints than those trained by Diffusers, but they can still be loaded in the same way.\n",
    " \n",
    "- Kohya\n",
    "```\n",
    "!wget https://civitai.com/api/download/models/168776 -O blueprintify-sd-xl-10.safetensors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1051f-742b-40ff-88b0-8cc3cba6b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kohya\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_lora_weights(\"path/to/weights\", weight_name=\"blueprintify-sd-xl-10.safetensors\")\n",
    "\n",
    "# TheLastBen\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_lora_weights(\"TheLastBen/William_Eggleston_Style_SDXL\", weight_name=\"wegg.safetensors\")\n",
    "\n",
    "# use by william eggleston in the prompt to trigger the LoRA\n",
    "prompt = \"a house by william eggleston, sunrays, beautiful, sunlight, sunrays, beautiful\"\n",
    "image = pipeline(prompt=prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18c6bf-1296-4087-b1e1-1e64eee5c8b7",
   "metadata": {},
   "source": [
    "#### IP-Adapter\n",
    "- IP-Adapter is a lightweight adapter that enables image prompting for any diffusion model.\n",
    "  - This adapter works by **decoupling the cross-attention layers** of the image and text features.\n",
    "  - All the other model components are frozen and only the embedded image features in the UNet are trained.\n",
    "  - As a result, IP-Adapter files are typically only ~100MBs.\n",
    "\n",
    "- Load the IP-Adapter weights and add it to the pipeline with the `load_ip_adapter()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1543b0f-344d-4c1f-a869-dbc03a9c34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png\")\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "images = pipeline(\n",
    "    prompt='best quality, high quality, wearing sunglasses',\n",
    "    ip_adapter_image=image,\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=50,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe774ad0-a3fb-4d0d-ba2f-911f070f1cbc",
   "metadata": {},
   "source": [
    "#### IP-Adapter Plus\n",
    "- P-Adapter relies on an image encoder to generate image features.\n",
    "  - If the IP-Adapter repository contains an `image_encoder` subfolder, the image encoder is automatically loaded and registered to the pipeline.\n",
    "  - Otherwise, youâ€™ll need to explicitly load the image encoder with a `CLIPVisionModelWithProjection` model and pass it to the pipeline.\n",
    "\n",
    "- This is the case for IP-Adapter Plus checkpoints which use the ViT-H image encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e565e-0be4-4c3f-82e4-29421ff3b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    image_encoder=image_encoder,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter-plus_sdxl_vit-h.safetensors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-t2i",
   "language": "python",
   "name": "diffusers-t2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
