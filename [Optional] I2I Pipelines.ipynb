{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d43cf0-3d62-44eb-a561-95f777f8c54a",
   "metadata": {},
   "source": [
    "## **[Optional] I2I <sup>Image-to-image</sup> Pipelines**\n",
    "\n",
    "> Original Source: https://huggingface.co/docs/diffusers/v0.33.1/en/using-diffusers/sdxl_turbo\n",
    "\n",
    "```\n",
    "> Stable Diffusion XL\n",
    "> Stable Diffusion XL Turbo\n",
    "> Kandinsky\n",
    "> IP-Adapter\n",
    "> Perturbed-Attention Guidance(PAG)\n",
    "> ControlNet\n",
    "> Latent Consistency Model(LCM)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e7bf9-35a0-4e95-b6f5-2563f5d6e2de",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Stable Diffusion XL**\n",
    "- `Stable Diffusion XL (SDXL)` is a powerful text-to-image generation model that iterates on the previous Stable Diffusion models in three key ways:\n",
    "  - The `UNet` is 3x larger and `SDXL` combines a second text encoder (`OpenCLIP ViT-bigG/14`) with the original text encoder to significantly increase the number of parameters\n",
    "  - Introduces size and crop-conditioning to preserve training data from being discarded and gain more control over how a generated image should be cropped\n",
    "  - Introduces a two-stage model process; the base model (can also be run as a standalone model) generates an image as an input to the refiner model which adds additional high-quality details\n",
    "\n",
    "- Install:\n",
    "```\n",
    "pip install -q diffusers transformers accelerate invisible-watermark>=0.2.0\n",
    "```\n",
    "\n",
    "- We recommend installing the `invisible-watermark` library to help identify images that are generated. If the invisible-watermark library is installed, it is used by default. To disable the watermarker:\n",
    "\n",
    "```\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(..., add_watermarker=False)\n",
    "```\n",
    "\n",
    "#### Load model checkpoints\n",
    "- Model weights may be stored in separate subfolders on the Hub or locally, in which case, you should use the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b20f0-0de5-4aa3-b078-2757fc49862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline, AutoPipelineForImage2Image\n",
    "from diffusers import StableDiffusionXLInpaintPipeline, AutoPipelineForInpainting\n",
    "\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b91cf-a1b7-4c49-a9c1-0ab9e19f6f37",
   "metadata": {},
   "source": [
    "- For image-to-image, `SDXL` works especially well with image sizes between 768x768 and 1024x1024. Pass an initial image, and a text prompt to condition the image with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848865c-de09-4613-b4e7-d9273405bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "# use from_pipe to avoid consuming additional memory when loading a checkpoint\n",
    "pipeline = AutoPipelineForImage2Image.from_pipe(pipeline_text2image).to(\"cuda\")\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n",
    "init_image = load_image(url)\n",
    "prompt = \"a dog catching a frisbee in the jungle\"\n",
    "image = pipeline(prompt, image=init_image, strength=0.8, guidance_scale=10.5).images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef66a3-295b-4b5b-b02c-aa514f8209e6",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Stable Diffusion XL Turbo**\n",
    "- `SDXL Turbo` is an adversarial time-distilled `Stable Diffusion XL (SDXL)` model capable of running inference in as little as 1 step.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Model weights may be stored in separate subfolders on the Hub or locally, in which case, you should use the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9e45c-82d0-458e-883a-12da6ed120d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler\n",
    "\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cf951-5cfb-4e59-b977-749a320bac5c",
   "metadata": {},
   "source": [
    "- For image-to-image generation, make sure that `num_inference_steps * strength` is larger or equal to 1.\n",
    "  - The image-to-image pipeline will run for `int(num_inference_steps * strength)` steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2536462-615e-4acc-a3c4-d18c6437d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipeline_text2image = pipeline_text2image.to(\"cuda\")\n",
    "\n",
    "# use from_pipe to avoid consuming additional memory when loading a checkpoint\n",
    "pipeline_image2image = AutoPipelineForImage2Image.from_pipe(pipeline_text2image).to(\"cuda\")\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n",
    "init_image = init_image.resize((512, 512))\n",
    "\n",
    "prompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\n",
    "\n",
    "image = pipeline_image2image(prompt, image=init_image, strength=0.5, guidance_scale=0.0, num_inference_steps=2).images[0]\n",
    "make_image_grid([init_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9ddd6-5215-4d22-b2bf-1667e1188d2f",
   "metadata": {},
   "source": [
    "-----\n",
    "### **Kandinsky**\n",
    "- The Kandinsky models are a series of multilingual text-to-image generation models.\n",
    "  - The `Kandinsky 2.0` model uses two multilingual text encoders and concatenates those results for the UNet.\n",
    "  - `Kandinsky 2.1` changes the architecture to include an image prior model (CLIP) to generate a mapping between text and image embeddings and uses a `Modulating Quantized Vectors (MoVQ)` decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.\n",
    "  - `Kandinsky 2.2` improves on the previous model by replacing the image encoder of the image prior model with a larger `CLIP-ViT-G` model to improve quality.\n",
    "    - The only difference with `Kandinsky 2.1` is `Kandinsky 2.2` doesn’t accept prompt as an input when decoding the latents. Instead, `Kandinsky 2.2` only accepts `image_embeds` during decoding.\n",
    "  - `Kandinsky 3` simplifies the architecture and shifts away from the two-stage generation process involving the prior model and diffusion model and uses `Flan-UL2` to encode text, a `UNet` with BigGan-deep blocks, and `Sber-MoVQGAN` to decode the latents into images.\n",
    "    - Text understanding and generated image quality are primarily achieved by using a larger text encoder and UNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74c4ac6-f7f5-48df-93c8-5c17a6110783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import KandinskyPriorPipeline, KandinskyPipeline\n",
    "from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\n",
    "from diffusers import Kandinsky3Pipeline\n",
    "\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline\n",
    "from diffusers import KandinskyV22Img2ImgPipeline\n",
    "from diffusers import Kandinsky3Img2ImgPipeline\n",
    "\n",
    "from diffusers import KandinskyInpaintPipeline\n",
    "from diffusers import KandinskyV22InpaintPipeline, KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline\n",
    "\n",
    "from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline\n",
    "\n",
    "from diffusers.models.attention_processor import AttnAddedKVProcessor2_0\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.utils import make_image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42f065-1114-410c-9ad3-f59792bfaa28",
   "metadata": {},
   "source": [
    "- For image-to-image, pass the initial image and text prompt to condition the image to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49094260-c5e4-4885-8ebb-77257e9bea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
    "pipeline = KandinskyV22Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00a803-604e-4398-9660-b01540bd87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "original_image = load_image(url)\n",
    "original_image = original_image.resize((768, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c3732-6b2e-4db1-a9b2-1d1c7afad322",
   "metadata": {},
   "source": [
    "- Generate the `image_embeds` and `negative_image_embeds` with the prior pipeline\n",
    "  - Pass the original image, and all the prompts and embeddings to the pipeline to generate an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a0967-14e3-49dc-a57f-fe0e2eceb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A fantasy landscape, Cinematic lighting\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "\n",
    "image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt).to_tuple()\n",
    "\n",
    "image = pipeline(image=original_image, image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768, strength=0.3).images[0]\n",
    "make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c428dd-541b-40c4-83dc-e804f7e58f14",
   "metadata": {},
   "source": [
    "- Diffusers also provides an end-to-end API with the `KandinskyImg2ImgCombinedPipeline` and `KandinskyV22Img2ImgCombinedPipeline`, meaning you don’t have to separately load the prior and image-to-image pipeline.\n",
    "  - The combined pipeline automatically loads both the prior model and the decoder.\n",
    "  - You can still set different values for the prior pipeline with the `prior_guidance_scale` and `prior_num_inference_steps` parameters if you want.\n",
    "\n",
    "- Use the `AutoPipelineForImage2Image` to automatically call the combined pipelines under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e33de-4ba8-4a70-abaf-8ac9a2399f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipeline.enable_model_cpu_offload()\n",
    "\n",
    "prompt = \"A fantasy landscape, Cinematic lighting\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "original_image = load_image(url)\n",
    "\n",
    "original_image.thumbnail((768, 768))\n",
    "\n",
    "image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=original_image, strength=0.3).images[0]\n",
    "make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966e3dd-b0a8-45ac-b899-c758411c90b9",
   "metadata": {},
   "source": [
    "#### ControlNet\n",
    "- ControlNet enables conditioning large pretrained diffusion models with additional inputs such as a depth map or edge detection.\n",
    "  - You can condition `Kandinsky 2.2` with a depth map so the model understands and preserves the structure of the depth image.\n",
    " \n",
    "- Use the depth-estimation Pipeline from `Transformers` to process the image and retrieve the depth map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e437a-2970-4e25-903c-f34f7d93145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(\n",
    "    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png\"\n",
    ").resize((768, 768))\n",
    "\n",
    "def make_hint(image, depth_estimator):\n",
    "    image = depth_estimator(image)[\"depth\"]\n",
    "    image = np.array(image)\n",
    "    image = image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    detected_map = torch.from_numpy(image).float() / 255.0\n",
    "    hint = detected_map.permute(2, 0, 1)\n",
    "    return hint\n",
    "\n",
    "depth_estimator = pipeline(\"depth-estimation\")\n",
    "hint = make_hint(img, depth_estimator).unsqueeze(0).half().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9e505-e1b2-4842-8f65-dc7cdfcc716f",
   "metadata": {},
   "source": [
    "- `KandinskyV22PriorEmb2EmbPipeline` to generate the image embeddings from a text prompt and an image\n",
    "- `KandinskyV22ControlnetImg2ImgPipeline` to generate an image from the initial image and the image embeddings\n",
    "\n",
    "- Process and extract a depth map of an initial image of a cat with the depth-estimation Pipeline from `Transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de532f95-8126-4f89-aa47-bae791cbe7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pipeline = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-controlnet-depth\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5aec0d-8dd4-4682-a3da-7be0c3e15784",
   "metadata": {},
   "source": [
    "- Pass a text prompt and the initial image to the prior pipeline to generate the image embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316302fd-b7d7-48da-8fb3-a9e001206e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A robot, 4k photo\"\n",
    "negative_prior_prompt = \"lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature\"\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(43)\n",
    "\n",
    "img_emb = prior_pipeline(prompt=prompt, image=img, strength=0.85, generator=generator)\n",
    "negative_emb = prior_pipeline(prompt=negative_prior_prompt, image=img, strength=1, generator=generator)\n",
    "\n",
    "image = pipeline(image=img, strength=0.5, image_embeds=img_emb.image_embeds, negative_image_embeds=negative_emb.image_embeds, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]\n",
    "make_image_grid([img.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a416b-b1c6-404e-bde3-6e4686ab5593",
   "metadata": {},
   "source": [
    "-----\n",
    "### **IP-Adapter**\n",
    "- `IP-Adapter` is an image prompt adapter that can be plugged into diffusion models to enable image prompting without any changes to the underlying model.\n",
    "  - This adapter can be reused with other models finetuned from the same base model and it can be combined with other adapters like ControlNet.\n",
    "  - The key idea behind `IP-Adapter` is the decoupled cross-attention mechanism which adds a separate cross-attention layer just for image features instead of using the same cross-attention layer for both text and image features.\n",
    "    - This allows the model to learn more image-specific features.\n",
    "   \n",
    "<br>\n",
    "\n",
    "- `set_ip_adapter_scale()` method controls the amount of text or image conditioning to apply to the model.\n",
    "  - A value of `1.0` means the model is only conditioned on the image prompt.\n",
    "  - Lowering this value encourages the model to produce more diverse images, but they may not be as aligned with the image prompt.\n",
    "  - Typically, a value of 0.5 achieves a good balance between the two prompt types and produces good results.\n",
    "\n",
    "- Try adding `low_cpu_mem_usage=True` to the `load_ip_adapter()` method to speed up the loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c9494-52ea-4ca6-8696-4989d58b7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from diffusers.image_processor import IPAdapterMaskProcessor\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoPipelineForText2Image\n",
    "from diffusers import DiffusionPipeline, LCMScheduler\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c814973-4a55-4ca5-9724-e85539bd72a3",
   "metadata": {},
   "source": [
    "- `IP-Adapter` can also help with image-to-image by guiding the model to generate an image that resembles the original image and the image prompt.\n",
    "  - Pass the original image and the IP-Adapter image prompt to the pipeline to generate an image.\n",
    "  - Providing a text prompt to the pipeline is optional, but in this example, a text prompt is used to increase image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee04d2-a543-4093-906a-4ba10fd578f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef19f88-0026-4db3-9315-eb7cc0117291",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_bear_1.png\")\n",
    "ip_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_bear_2.png\")\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(4)\n",
    "images = pipeline(\n",
    "    prompt=\"best quality, high quality\",\n",
    "    image=image,\n",
    "    ip_adapter_image=ip_image,\n",
    "    generator=generator,\n",
    "    strength=0.6,\n",
    ").images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ec87d-7cc1-4cd7-9e19-4049bf5cae6a",
   "metadata": {},
   "source": [
    "----\n",
    "### **Perturbed-Attention Guidance(PAG)**\n",
    "- `Perturbed-Attention Guidance(PAG)` is a new diffusion sampling guidance that improves sample quality across both unconditional and conditional settings, achieving this without requiring further training or the integration of external modules.\n",
    "  - `PAG` is designed to progressively enhance the structure of synthesized samples throughout the denoising process by considering the self-attention mechanisms’ ability to capture structural information.\n",
    "  - It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, and guiding the denoising process away from these degraded samples.\n",
    "\n",
    "<br>\n",
    "\n",
    "- You can apply `PAG` to the `StableDiffusionXLPipeline` for tasks such as text-to-image, image-to-image, and inpainting.\n",
    "- To enable `PAG` for a specific task, load the pipeline using the AutoPipeline API with the `enable_pag=True` flag and the `pag_applied_layers` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3a6bb-b950-4a89-8f3b-44bab92aac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image, ControlNetModel\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b40004-5635-44fe-b5ff-deedb3b0b0e8",
   "metadata": {},
   "source": [
    "- It is also very easy to directly switch from a text-to-image pipeline to `PAG` enabled image-to-image pipeline\n",
    "  - If you have a `PAG` enabled text-to-image pipeline, you can directly switch to a image-to-image pipeline with `PAG` still enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511ca0b-9d0d-473f-b4b5-fdeb4b33b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    enable_pag=True,\n",
    "    pag_applied_layers=[\"mid\"],\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493327a4-2306-4670-ae0c-36ffa7bc5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_t2i = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\n",
    "pipeline = AutoPipelineForImage2Image.from_pipe(pipeline_t2i, enable_pag=True)\n",
    "\n",
    "pipeline_pag = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\n",
    "pipeline = AutoPipelineForImage2Image.from_pipe(pipeline_t2i, enable_pag=True)\n",
    "\n",
    "pipeline_pag = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", enable_pag=True, torch_dtype=torch.float16)\n",
    "pipeline = AutoPipelineForImage2Image.from_pipe(pipeline_t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8305fd-c8ab-4d76-9cd9-178e0e06a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pag_scales =  4.0\n",
    "guidance_scales = 7.0\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n",
    "init_image = load_image(url)\n",
    "prompt = \"a dog catching a frisbee in the jungle\"\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    strength=0.8,\n",
    "    guidance_scale=guidance_scale,\n",
    "    pag_scale=pag_scale,\n",
    "    generator=generator).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a57646-6785-402d-a0d8-74c1c981052c",
   "metadata": {},
   "source": [
    "----\n",
    "### **ControlNet**\n",
    "- `ControlNet` is a type of model for controlling image diffusion models by conditioning the model with an additional input image.\n",
    "  - There are many types of conditioning inputs (canny edge, user sketching, human pose, depth, and more) you can use to control a diffusion model.\n",
    "  - This is hugely useful because it affords you greater control over image generation, making it easier to generate specific images without experimenting with different text prompts or denoising values as much.\n",
    "\n",
    "- A `ControlNet` model has two sets of weights (or blocks) connected by a zero-convolution layer:\n",
    "  - a locked copy keeps everything a large pretrained diffusion model has learned\n",
    "  - a trainable copy is trained on the additional conditioning input\n",
    "\n",
    "- Since the locked copy preserves the pretrained model, training and implementing a `ControlNet` on a new conditioning input is as fast as finetuning any other model because you aren’t training the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6072d7ae-b810-427b-be0f-2cde585ea6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers import StableDiffusionControlNetImg2ImgPipeline\n",
    "from diffusers import StableDiffusionControlNetInpaintPipeline\n",
    "\n",
    "from diffusers import StableDiffusionXLControlNetPipeline, AutoencoderKL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa2efd-74dc-4806-be18-755611a51ddb",
   "metadata": {},
   "source": [
    "- With `ControlNet`, you can pass an additional conditioning input to guide the model.\n",
    "  - Let’s condition the model with a depth map, an image which contains spatial information.\n",
    "  - `ControlNet` can use the depth map as a control to guide the model to generate an image that preserves spatial information.\n",
    "\n",
    "- Use the `StableDiffusionControlNetImg2ImgPipeline` for this task, which is different from the `StableDiffusionControlNetPipeline` because it allows you to pass an initial image as the starting point for the image generation process.\n",
    "\n",
    "- Load an image and use the depth-estimation Pipeline from Transformers to extract the depth map of an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b7b97-b690-463e-9f39-5b8473313943",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg\"\n",
    ")\n",
    "\n",
    "def get_depth_map(image, depth_estimator):\n",
    "    image = depth_estimator(image)[\"depth\"]\n",
    "    image = np.array(image)\n",
    "    image = image[:, :, None]\n",
    "    image = np.concatenate([image, image, image], axis=2)\n",
    "    detected_map = torch.from_numpy(image).float() / 255.0\n",
    "    depth_map = detected_map.permute(2, 0, 1)\n",
    "    return depth_map\n",
    "\n",
    "depth_estimator = pipeline(\"depth-estimation\")\n",
    "depth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed96d9-ca7c-4dbd-8264-fd6511023b59",
   "metadata": {},
   "source": [
    "- Load a `ControlNet` model conditioned on depth maps and pass it to the `StableDiffusionControlNetImg2ImgPipeline`.\n",
    "  - Use the faster `UniPCMultistepScheduler` and enable model offloading to speed up inference and reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28071346-c0ac-4f00-a73f-aae71802ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_depth\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "output = pipe(\n",
    "    \"lego batman and robin\", image=image, control_image=depth_map,\n",
    ").images[0]\n",
    "make_image_grid([image, output], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90bda0e-5297-4e7e-a203-b8ecd977f6dd",
   "metadata": {},
   "source": [
    "----\n",
    "### **Latent Consistency Model(LCMs)**\n",
    "- `Latent Consistency Models (LCMs)` enable fast high-quality image generation by directly predicting the reverse diffusion process in the latent rather than pixel space.\n",
    "  - `LCMs` try to predict the noiseless image from the noisy image in contrast to typical diffusion models that iteratively remove noise from the noisy image.\n",
    "  - By avoiding the iterative sampling process, `LCMs` are able to generate high-quality images in 2-4 steps instead of 20-30 steps.\n",
    "\n",
    "- `LCMs` are distilled from pretrained models which requires ~32 hours of A100 compute.\n",
    "  - To speed this up, `LCM-LoRAs` train a `LoRA` adapter which have much fewer parameters to train compared to the full model.\n",
    "  - The `LCM-LoRA` can be plugged into a diffusion model once it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87aa15f-ba63-48b0-bb2f-990459ebd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, LCMScheduler, AutoPipelineForImage2Image\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63859395-2492-4962-ae2e-14ef4287a74f",
   "metadata": {},
   "source": [
    "- To use `LCMs` for image-to-image, you need to load the `LCM` checkpoint for your supported model into `UNet2DConditionModel` and replace the scheduler with the `LCMScheduler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4edb7-b54f-4d85-ac81-3c002aab79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"SimianLuo/LCM_Dreamshaper_v7\",\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"Lykon/dreamshaper-7\",\n",
    "    unet=unet,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\n",
    "prompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=7.5,\n",
    "    strength=0.5,\n",
    "    generator=generator\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792db05-7b46-48cc-8bbf-dcbdb412c721",
   "metadata": {},
   "source": [
    "- To use `LCM-LoRAs` for image-to-image, you need to replace the scheduler with the `LCMScheduler` and load the `LCM-LoRA` weights with the `load_lora_weights()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5867f17-0dfc-40f2-a216-24361b45f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"Lykon/dreamshaper-7\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "\n",
    "init_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\")\n",
    "prompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=init_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=1,\n",
    "    strength=0.6,\n",
    "    generator=generator\n",
    ").images[0]\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-t2i",
   "language": "python",
   "name": "diffusers-t2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
