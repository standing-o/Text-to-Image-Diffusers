{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24844adf-3c2d-4c46-97b6-7be118bcbec8",
   "metadata": {},
   "source": [
    "## **4. Inference Techniques**\n",
    "> Original Source: https://huggingface.co/docs/diffusers/v0.33.1/en/using-diffusers/overview_techniques\n",
    "\n",
    "```\n",
    "> Create a server\n",
    "> Distributed Inference\n",
    "> Merge LoRAs\n",
    "> Scheduler Features\n",
    "> Pipeline Callbacks\n",
    "> Reproducible Pipelines\n",
    "> Controlling Image Quality\n",
    "> Prompt Techniques\n",
    "\n",
    "```\n",
    "\n",
    "- **Pipeline functionality**: these techniques modify the pipeline or extend it for other applications.\n",
    "  - Pipeline callbacks add new features to a pipeline and a pipeline can also be extended for distributed inference.\n",
    "- **Improve inference quality**: these techniques increase the visual quality of the generated images.\n",
    "  - Enhance your prompts with GPT2 to create better images with lower effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "461303ce-496c-43dd-a430-1982c1d87484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from accelerate import PartialState\n",
    "from diffusers import DiffusionPipeline, EulerDiscreteScheduler, DDIMPipeline, DDIMScheduler\n",
    "from diffusers import FluxPipeline\n",
    "from diffusers import FluxTransformer2DModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.schedulers import AysSchedules, UniPCMultistepScheduler\n",
    "from diffusers.callbacks import SDXLCFGCutoffCallback, IPAdapterScaleCutoffCallback\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "from transformers import GenerationConfig, GPT2LMHeadModel, GPT2Tokenizer, LogitsProcessor, LogitsProcessorList\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_sdxl\n",
    "from sd_embed.embedding_funcs import get_weighted_text_embeddings_sd15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aaf498-cc7f-4f7b-88d8-228961c3c7e3",
   "metadata": {},
   "source": [
    "-------------\n",
    "### **Create a Server**\n",
    "- Diffusers’ pipelines can be used as an inference engine for a server.\n",
    "  - It supports **concurrent and multithreaded requests to generate images** that may be requested by multiple users at the same time.\n",
    " \n",
    "- [`StableDiffusion3Pipeline`]()\n",
    "  - Start by navigating to the examples/server folder and installing all of the dependencies.\n",
    " \n",
    "```\n",
    "pip install .\n",
    "pip install -f requirements.txt\n",
    "```\n",
    "\n",
    "- Launch the server with the following command.\n",
    "\n",
    "```\n",
    "python server.py\n",
    "```\n",
    "\n",
    "- The server is accessed at `http://localhost:8000`.\n",
    "  - You can curl this model with the following command.\n",
    "\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/json\" --data '{\"model\": \"something\", \"prompt\": \"a kitten in front of a fireplace\"}' http://localhost:8000/v1/images/generations\n",
    "```\n",
    "\n",
    "- If you need to upgrade some dependencies, you can use either `pip-tools` or `uv`.\n",
    "\n",
    "```\n",
    "uv pip compile requirements.in -o requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "- The server is built with `FastAPI`. The endpoint for `v1/images/generations` is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316393e-4624-4c56-a84d-81711b504d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/v1/images/generations\")\n",
    "async def generate_image(image_input: TextToImageInput):\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        scheduler = shared_pipeline.pipeline.scheduler.from_config(shared_pipeline.pipeline.scheduler.config)\n",
    "        pipeline = StableDiffusion3Pipeline.from_pipe(shared_pipeline.pipeline, scheduler=scheduler)\n",
    "        generator = torch.Generator(device=\"cuda\")\n",
    "        generator.manual_seed(random.randint(0, 10000000))\n",
    "        output = await loop.run_in_executor(None, lambda: pipeline(image_input.prompt, generator = generator))\n",
    "        logger.info(f\"output: {output}\")\n",
    "        image_url = save_image(output.images[0])\n",
    "        return {\"data\": [{\"url\": image_url}]}\n",
    "    except Exception as e:\n",
    "        if isinstance(e, HTTPException):\n",
    "            raise e\n",
    "        elif hasattr(e, 'message'):\n",
    "            raise HTTPException(status_code=500, detail=e.message + traceback.format_exc())\n",
    "        raise HTTPException(status_code=500, detail=str(e) + traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e9f51-5bb9-4ceb-bb2b-2cab48d6a7bb",
   "metadata": {},
   "source": [
    "- The `generate_image` function is defined as **asynchronous** with the async keyword so that `FastAPI` knows that whatever is happening in this function won’t necessarily return a result right away.\n",
    "  - Once it hits some point in the function that it needs to await some other Task, the main thread goes back to answering other HTTP requests. This is shown in the code below with the await keyword.\n",
    " \n",
    "- **The execution of the pipeline function is placed onto a new thread**, and the main thread performs other things until a result is returned from the pipeline.\n",
    "  - Another important aspect of this implementation is creating a pipeline from `shared_pipeline`.\n",
    "  - The goal behind this is to avoid loading the underlying model more than once onto the GPU while still allowing for each new request that is running on a separate thread to have its own generator and scheduler.\n",
    "  - The scheduler, in particular, is not thread-safe, and it will cause errors like: `IndexError: index 21 is out of bounds for dimension 0 with size 21` if you try to use the same scheduler across multiple threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc42ad-9dde-435b-aeb0-8eaa773d7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await loop.run_in_executor(None, lambda: pipeline(image_input.prompt, generator = generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54131eea-8685-4a3d-994e-7d9036213bae",
   "metadata": {},
   "source": [
    "------------------\n",
    "### **Distributed Inference**\n",
    "- On distributed setups, you can run inference across multiple GPUs with `Accelerate` or `PyTorch Distributed`, which is useful for generating with multiple prompts in parallel.\n",
    "\n",
    "#### `Accelerate`\n",
    "- `Accelerate` is a library designed to make it easy to train or run inference across distributed setups and simplifies the process of setting up the distributed environment, allowing you to focus on your PyTorch code.\n",
    "\n",
    "- To begin, create a Python file and initialize an accelerate.\n",
    "  - PartialState to create a distributed environment; your setup is automatically detected so you don’t need to explicitly define the rank or `world_size`.\n",
    "  - Move the `DiffusionPipeline` to `distributed_state.device` to assign a GPU to each process.\n",
    "- Use the `split_between_processes` utility as a context manager to automatically distribute the prompts between the number of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95dd32f-9b7d-488a-8e5a-e709b93b3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "distributed_state = PartialState()\n",
    "pipeline.to(distributed_state.device)\n",
    "\n",
    "with distributed_state.split_between_processes([\"a dog\", \"a cat\"]) as prompt:\n",
    "    result = pipeline(prompt).images[0]\n",
    "    result.save(f\"result_{distributed_state.process_index}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243f787-8cb1-4e6e-b43c-28efa6adc366",
   "metadata": {},
   "source": [
    "- Use the `--num_processes` argument to specify the number of GPUs to use, and call accelerate launch to run the script:\n",
    "```\n",
    "accelerate launch run_distributed.py --num_processes=2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf72f0-c3e1-4e8e-b103-64f687f95359",
   "metadata": {},
   "source": [
    "#### PyTorch Distributed\n",
    "- PyTorch supports `DistributedDataParallel` which enables data parallelism.\n",
    "  - Create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc36c3c-5e30-4c3f-a46a-93678559d691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca16f02f87ab4bb48e67aab7745547d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sd = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafcb34-7650-4dea-afc0-199a1c80b97d",
   "metadata": {},
   "source": [
    "- You’ll want to create a function to run inference; `init_process_group` handles creating a distributed environment with the type of backend to use, the rank of the current process, and the `world_size` or the number of processes participating.\n",
    "  - If you’re running inference in parallel over 2 GPUs, then the `world_size` is 2.\n",
    "\n",
    "- Move the `DiffusionPipeline` to rank and use `get_rank` to assign a GPU to each process, where each process handles a different prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a911b042-983a-4cc9-8c24-318fec6c0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    sd.to(rank)\n",
    "\n",
    "    if torch.distributed.get_rank() == 0:\n",
    "        prompt = \"a dog\"\n",
    "    elif torch.distributed.get_rank() == 1:\n",
    "        prompt = \"a cat\"\n",
    "\n",
    "    image = sd(prompt).images[0]\n",
    "    image.save(f\"./{'_'.join(prompt)}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f83c7e-e911-490f-9bde-dc5f156d2790",
   "metadata": {},
   "source": [
    "- To run the distributed inference, call `mp.spawn` to run the `run_inference` function on the number of GPUs defined in `world_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249e502-fe23-4896-97ec-da34ad7e9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    world_size = 2\n",
    "    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd971f00-3ba1-41c1-991f-b9cda028df5c",
   "metadata": {},
   "source": [
    "- Once you’ve completed the inference script, use the `--nproc_per_node` argument to specify the number of GPUs to use and call torchrun to run the script:\n",
    "\n",
    "```\n",
    "torchrun run_distributed.py --nproc_per_node=2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd8afd-6286-4f64-9396-69d9a6f5aecc",
   "metadata": {},
   "source": [
    "#### Model sharding\n",
    "- Modern diffusion systems such as Flux are very large and have multiple models.\n",
    "- ex. `Flux.1-Dev` is made up of two text encoders - `T5-XXL` and `CLIP-L` - a diffusion transformer, and a VAE.\n",
    "- **Model sharding** is a technique that distributes models across GPUs when the models don’t fit on a single GPU.\n",
    "- The example below assumes two 16GB GPUs are available for inference.\n",
    "  - Start by computing the text embeddings with the text encoders.\n",
    "  - Keep the text encoders on two GPUs by setting `device_map=\"balanced\"`.\n",
    "    - The balanced strategy evenly distributes the model on all available GPUs.\n",
    "  - Use the `max_memory` parameter to allocate the maximum amount of memory for each text encoder on each GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959df08-6b58-46a1-a85a-56625369081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of a dog with cat-like look\"\n",
    "\n",
    "pipeline = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    transformer=None,\n",
    "    vae=None,\n",
    "    device_map=\"balanced\",\n",
    "    max_memory={0: \"16GB\", 1: \"16GB\"},\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "with torch.no_grad():\n",
    "    print(\"Encoding prompts.\")\n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(\n",
    "        prompt=prompt, prompt_2=None, max_sequence_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98886ba7-8fbc-498e-a33d-5af04ebe087d",
   "metadata": {},
   "source": [
    "- Once the text embeddings are computed, remove them from the GPU to make space for the diffusion transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e300aa-ffd9-4ab6-b83e-57bba042acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "del pipeline.text_encoder\n",
    "del pipeline.text_encoder_2\n",
    "del pipeline.tokenizer\n",
    "del pipeline.tokenizer_2\n",
    "del pipeline\n",
    "\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbec81-5be7-4acc-80da-ec66f1305301",
   "metadata": {},
   "source": [
    "- Load the diffusion transformer next which has 12.5B parameters.\n",
    "  - Set `device_map=\"auto\"` to automatically distribute the model across two 16GB GPUs.\n",
    "  - The auto strategy is backed by `Accelerate` and available as a part of the Big Model Inference feature.\n",
    "  - It starts by distributing a model across the fastest device first (GPU) before moving to slower devices like the CPU and hard drive if needed.\n",
    "  - The trade-off of storing model parameters on slower devices is slower inference latency.\n",
    "  - You can try `print(pipeline.hf_device_map)` to see how the various models are distributed across devices.\n",
    "    - This is useful for tracking the device placement of the models.\n",
    "    - You can also try `print(transformer.hf_device_map)` to see how the transformer model is sharded across devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a144a9d1-6f7c-4015-bc07-eb26972a2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = FluxTransformer2DModel.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\", \n",
    "    subfolder=\"transformer\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "pipeline = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    text_encoder=None,\n",
    "    text_encoder_2=None,\n",
    "    tokenizer=None,\n",
    "    tokenizer_2=None,\n",
    "    vae=None,\n",
    "    transformer=transformer,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Running denoising.\")\n",
    "height, width = 768, 1360\n",
    "latents = pipeline(\n",
    "    prompt_embeds=prompt_embeds,\n",
    "    pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=3.5,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    output_type=\"latent\",\n",
    ").images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433b838-1de9-4e54-9752-2b0ecc44a3af",
   "metadata": {},
   "source": [
    "- Remove the pipeline and transformer from memory as they’re no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fb315-dc7e-4c24-a2f1-3a1caaaff992",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipeline.transformer\n",
    "del pipeline\n",
    "\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcead50-edc4-4032-bf38-936aa7daed0e",
   "metadata": {},
   "source": [
    "- Decode the latents with the VAE into an image. The VAE is typically small enough to be loaded on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccbf23-ea38-445d-ac0d-2babf03ec7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(ckpt_id, subfolder=\"vae\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "vae_scale_factor = 2 ** (len(vae.config.block_out_channels))\n",
    "image_processor = VaeImageProcessor(vae_scale_factor=vae_scale_factor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Running decoding.\")\n",
    "    latents = FluxPipeline._unpack_latents(latents, height, width, vae_scale_factor)\n",
    "    latents = (latents / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "\n",
    "    image = vae.decode(latents, return_dict=False)[0]\n",
    "    image = image_processor.postprocess(image, output_type=\"pil\")\n",
    "    image[0].save(\"split_transformer.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff64a47-ea40-41d0-bf6b-ac517e4db3dd",
   "metadata": {},
   "source": [
    "-----------\n",
    "### **Merge LoRAs**\n",
    "- Merging multiple LoRA weights together to produce images that are a blend of different styles.\n",
    "- To improve inference speed and reduce memory-usage of merged LoRAs, you’ll also see how to use the `fuse_lora()` method to fuse the LoRA weights with the original weights of the underlying model.\n",
    "  - Load a `Stable Diffusion XL (SDXL)` checkpoint and the `KappaNeuro/studio-ghibli-style` and `Norod78/sdxl-chalkboarddrawing-lora` LoRAs with the `load_lora_weights()` method.\n",
    "  - You’ll need to assign each LoRA an `adapter_name` to combine them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8123590-a678-4716-9146-fc13c44361ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\", weight_name=\"ikea_instructions_xl_v1_5.safetensors\", adapter_name=\"ikea\")\n",
    "pipeline.load_lora_weights(\"lordjia/by-feng-zikai\", weight_name=\"fengzikai_v1.0_XL.safetensors\", adapter_name=\"feng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166583fc-4bc5-46df-a1b1-9c4416abf8db",
   "metadata": {},
   "source": [
    "#### `set_adapters`\n",
    "- The `set_adapters()` method merges LoRA adapters by concatenating their weighted matrices.\n",
    "- Use the adapter name to specify which LoRAs to merge, and the `adapter_weights` parameter to control the scaling for each LoRA.\n",
    "  - If `adapter_weights=[0.5, 0.5]`, then the merged LoRA output is an average of both LoRAs.\n",
    "  - Try adjusting the adapter weights to see how it affects the generated image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984048b2-6b0b-4009-8d2f-c6571756f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_adapters([\"ikea\", \"feng\"], adapter_weights=[0.7, 0.8])\n",
    "\n",
    "generator = torch.manual_seed(0)\n",
    "prompt = \"A bowl of ramen shaped like a cute kawaii bear, by Feng Zikai\"\n",
    "image = pipeline(prompt, generator=generator, cross_attention_kwargs={\"scale\": 1.0}).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191fd53-40ea-4233-b084-9532172afe29",
   "metadata": {},
   "source": [
    "#### `add_weighted_adapter`\n",
    "- There are three steps to merge LoRAs with the `add_weighted_adapter` method:\n",
    "  - Create a `PeftModel` from the underlying model and LoRA checkpoint.\n",
    "  - Load a base UNet model and the LoRA adapters.\n",
    "- Merge the adapters using the `add_weighted_adapter` method and the merging method of your choice.\n",
    "- Load a UNet that corresponds to the UNet in the LoRA checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f0087-2d6d-4e80-9c7d-2eb67b63edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    "    subfolder=\"unet\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc3785-2b67-4388-bb3b-a632bfff11fb",
   "metadata": {},
   "source": [
    "- Load the SDXL pipeline and the LoRA checkpoints, starting with the `ostris/ikea-instructions-lora-sdxl` LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200066b4-53fa-48bc-9031-d65cd8e12c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    "    unet=unet\n",
    ").to(\"cuda\")\n",
    "pipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\", weight_name=\"ikea_instructions_xl_v1_5.safetensors\", adapter_name=\"ikea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b47de-e85c-4954-bba0-2ded5ec807c4",
   "metadata": {},
   "source": [
    "- Create a `PeftModel` from the loaded LoRA checkpoint by combining the SDXL UNet and the LoRA UNet from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d10551-043d-45f3-9a88-c285501b79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl_unet = copy.deepcopy(unet)\n",
    "ikea_peft_model = get_peft_model(\n",
    "    sdxl_unet,\n",
    "    pipeline.unet.peft_config[\"ikea\"],\n",
    "    adapter_name=\"ikea\"\n",
    ")\n",
    "\n",
    "original_state_dict = {f\"base_model.model.{k}\": v for k, v in pipeline.unet.state_dict().items()}\n",
    "ikea_peft_model.load_state_dict(original_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f9977-7e91-427e-879b-9abd08358f91",
   "metadata": {},
   "source": [
    "- Repeat this process to create a `PeftModel` from the `lordjia/by-feng-zikai LoRA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8da4b-b947-406b-9ddb-af9ecfca8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.delete_adapters(\"ikea\")\n",
    "sdxl_unet.delete_adapters(\"ikea\")\n",
    "\n",
    "pipeline.load_lora_weights(\"lordjia/by-feng-zikai\", weight_name=\"fengzikai_v1.0_XL.safetensors\", adapter_name=\"feng\")\n",
    "pipeline.set_adapters(adapter_names=\"feng\")\n",
    "\n",
    "feng_peft_model = get_peft_model(\n",
    "    sdxl_unet,\n",
    "    pipeline.unet.peft_config[\"feng\"],\n",
    "    adapter_name=\"feng\"\n",
    ")\n",
    "\n",
    "original_state_dict = {f\"base_model.model.{k}\": v for k, v in pipe.unet.state_dict().items()}\n",
    "feng_peft_model.load_state_dict(original_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bff35-fc88-4bfb-bdc0-9f0a827f02e1",
   "metadata": {},
   "source": [
    "- Load a base UNet model and then load the adapters onto it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1587e-ced9-44a7-8959-e0d51ac749de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    "    subfolder=\"unet\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_unet, \"stevhliu/ikea_peft_model\", use_safetensors=True, subfolder=\"ikea\", adapter_name=\"ikea\")\n",
    "model.load_adapter(\"stevhliu/feng_peft_model\", use_safetensors=True, subfolder=\"feng\", adapter_name=\"feng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df91b6c-d8a7-41e2-9cb0-2467aa3cb688",
   "metadata": {},
   "source": [
    "- Merge the adapters using the `add_weighted_adapter` method and the merging method of your choice (learn more about other merging methods in this blog post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f703adf-33b6-4341-9620-4a3eac026eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_weighted_adapter(\n",
    "    adapters=[\"ikea\", \"feng\"],\n",
    "    weights=[1.0, 1.0],\n",
    "    combination_type=\"dare_linear\",\n",
    "    adapter_name=\"ikea-feng\"\n",
    ")\n",
    "model.set_adapters(\"ikea-feng\")\n",
    "\n",
    "model = model.to(dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", unet=model, variant=\"fp16\", torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipeline(\"A bowl of ramen shaped like a cute kawaii bear, by Feng Zikai\", generator=torch.manual_seed(0)).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e06b2-fa85-441f-a131-90e2a80517c3",
   "metadata": {},
   "source": [
    "#### fuse_lora\n",
    "- Both the `set_adapters()` and `add_weighted_adapter` methods require loading the base model and the LoRA adapters separately which incurs some overhead.\n",
    "  - The `fuse_lora()` method allows you to **fuse the LoRA weights directly with the original weights of the underlying model**.\n",
    "  - You’re only loading the model once which can increase inference and lower memory-usage.\n",
    "\n",
    "- You can use PEFT to easily `fuse/unfuse multiple adapters` directly into the model weights (both UNet and text encoder) using the `fuse_lora()` method, which can lead to a speed-up in inference and lower VRAM usage.\n",
    "\n",
    "- Fuse these LoRAs into the UNet with the `fuse_lora()` method.\n",
    "  - The `lora_scale` parameter controls how much to scale the output by with the LoRA weights.\n",
    "  - It is important to make the `lora_scale` adjustments in the `fuse_lora()` method because it won’t work if you try to pass scale to the `cross_attention_kwargs` in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4647-88f2-43f8-9bad-b5b1122bc8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\", weight_name=\"ikea_instructions_xl_v1_5.safetensors\", adapter_name=\"ikea\")\n",
    "pipeline.load_lora_weights(\"lordjia/by-feng-zikai\", weight_name=\"fengzikai_v1.0_XL.safetensors\", adapter_name=\"feng\")\n",
    "\n",
    "pipeline.set_adapters([\"ikea\", \"feng\"], adapter_weights=[0.7, 0.8])\n",
    "\n",
    "pipeline.fuse_lora(adapter_names=[\"ikea\", \"feng\"], lora_scale=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b1e8b-71e5-4383-bcf2-4cee0c2a0869",
   "metadata": {},
   "source": [
    "- Then you should use `unload_lora_weights()` to unload the LoRA weights since they’ve already been fused with the underlying base model.\n",
    "- Call `save_pretrained()` to save the fused pipeline locally or you could call `push_to_hub()` to push the fused pipeline to the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a762e3e-e3e3-49e5-921e-da9ce16cb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.unload_lora_weights()\n",
    "# save locally\n",
    "pipeline.save_pretrained(\"path/to/fused-pipeline\")\n",
    "# save to the Hub\n",
    "pipeline.push_to_hub(\"fused-ikea-feng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424dac4-4a27-4330-b338-93d450a63d1d",
   "metadata": {},
   "source": [
    "- You can quickly load the fused pipeline and use it for inference without needing to separately load the LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf253930-1c34-4344-85b1-5a7a3e38b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"username/fused-ikea-feng\", torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipeline(\"A bowl of ramen shaped like a cute kawaii bear, by Feng Zikai\", generator=torch.manual_seed(0)).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24667a3-6bf4-4320-b7bf-5a5c33524c31",
   "metadata": {},
   "source": [
    "- You can call `~~loaders.lora_base.LoraBaseMixin.unfuse_lora` to restore the original model’s weights (for example, if you want to use a different lora_scale value).\n",
    "  - This only works if you’ve only fused one LoRA adapter to the original model.\n",
    "  - If you’ve fused multiple LoRAs, you’ll need to reload the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1604e2-59c5-47c5-9a49-81f21cb0d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.unfuse_lora()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d8430-cd10-4bc0-bde1-afd981a5afdb",
   "metadata": {},
   "source": [
    "#### torch.compile\n",
    "- `torch.compile` can speed up your pipeline even more, but the LoRA weights must be fused first and then unloaded.\n",
    "  - The UNet is compiled because it is such a computationally intensive component of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4f40c-0f23-41b7-8ebb-98ef66b50ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model and LoRAs\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_lora_weights(\"ostris/ikea-instructions-lora-sdxl\", weight_name=\"ikea_instructions_xl_v1_5.safetensors\", adapter_name=\"ikea\")\n",
    "pipeline.load_lora_weights(\"lordjia/by-feng-zikai\", weight_name=\"fengzikai_v1.0_XL.safetensors\", adapter_name=\"feng\")\n",
    "\n",
    "# activate both LoRAs and set adapter weights\n",
    "pipeline.set_adapters([\"ikea\", \"feng\"], adapter_weights=[0.7, 0.8])\n",
    "\n",
    "# fuse LoRAs and unload weights\n",
    "pipeline.fuse_lora(adapter_names=[\"ikea\", \"feng\"], lora_scale=1.0)\n",
    "pipeline.unload_lora_weights()\n",
    "\n",
    "# torch.compile\n",
    "pipeline.unet.to(memory_format=torch.channels_last)\n",
    "pipeline.unet = torch.compile(pipeline.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "image = pipeline(\"A bowl of ramen shaped like a cute kawaii bear, by Feng Zikai\", generator=torch.manual_seed(0)).images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87032d59-aab8-4a48-96db-7a94d89e3e38",
   "metadata": {},
   "source": [
    "-----------\n",
    "### **Scheduler Features**\n",
    "- The scheduler is an important component of any diffusion model because it controls the entire denoising (or sampling) process.\n",
    "  - With Diffusers, you can modify the scheduler configuration to use custom noise schedules, sigmas, and rescale the noise schedule.\n",
    "  - Changing these parameters can have profound effects on inference quality and speed.\n",
    "\n",
    "#### Timestep schedules\n",
    "- The timestep or noise schedule determines the amount of noise at each sampling step.\n",
    "  - The scheduler uses this to generate an image with the corresponding amount of noise at each step.\n",
    "\n",
    "- `Align Your Steps (AYS)` is a method for optimizing a sampling schedule to generate a high-quality image in as little as 10 steps.\n",
    "  - The optimal 10-step schedule for Stable Diffusion XL is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c361ccc-508c-42d4-a489-7ef83542fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_schedule = AysSchedules[\"StableDiffusionXLTimesteps\"]\n",
    "print(sampling_schedule)\n",
    "\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"SG161222/RealVisXL_V4.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, algorithm_type=\"sde-dpmsolver++\")\n",
    "\n",
    "prompt = \"A cinematic shot of a cute little rabbit wearing a jacket and doing a thumbs up\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(2487854446)\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"\",\n",
    "    generator=generator,\n",
    "    timesteps=sampling_schedule,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10097f-29a4-4d97-9efe-0af86015bd30",
   "metadata": {},
   "source": [
    "#### Timestep spacing\n",
    "- The way sample steps are selected in the schedule can affect the quality of the generated image, especially w.r.t rescaling the noise schedule, which can enable a model to generate much brighter or darker images.\n",
    "- Diffusers provides three timestep spacing methods:\n",
    "  - `Leading` creates evenly spaced steps\n",
    "  - `Linspace` includes the first and last steps and evenly selects the remaining intermediate steps\n",
    "  - `Trailing` only includes the last step and evenly selects the remaining intermediate steps starting from the end\n",
    "    - It is recommended to use the `trailing` spacing method because it generates higher quality images with more details when there are fewer sample steps.\n",
    "    - But the difference in quality is not as obvious for more standard sample step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8f1ae-5768-45a2-8a75-4183e293cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"SG161222/RealVisXL_V4.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "prompt = \"A cinematic shot of a cute little black cat sitting on a pumpkin at night\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(2487854446)\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"\",\n",
    "    generator=generator,\n",
    "    num_inference_steps=5,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c0e1b-5f72-4bc2-8760-57576268c807",
   "metadata": {},
   "source": [
    "#### Sigmas\n",
    "- The `sigmas` parameter is **the amount of noise added at each timestep according to the timestep schedule**.\n",
    "  - Like the timesteps parameter, you can customize the sigmas parameter to control how much noise is added at each step.\n",
    "  - When you use a custom sigmas value, the timesteps are calculated from the custom sigmas value and the default scheduler configuration is ignored.\n",
    "\n",
    "- You can manually pass the sigmas for something like the `10-step AYS schedule` from before to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6103b-37c2-4944-8dc9-e6c73e52d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "  \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "  torch_dtype=torch.float16,\n",
    "  variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "sigmas = [14.615, 6.315, 3.771, 2.181, 1.342, 0.862, 0.555, 0.380, 0.234, 0.113, 0.0]\n",
    "prompt = \"anthropomorphic capybara wearing a suit and working with a computer\"\n",
    "generator = torch.Generator(device='cuda').manual_seed(123)\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=10,\n",
    "    sigmas=sigmas,\n",
    "    generator=generator\n",
    ").images[0]\n",
    "\n",
    "print(f\" timesteps: {pipe.scheduler.timesteps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b44f2-bd4a-4da9-9b72-0a857314bce3",
   "metadata": {},
   "source": [
    "#### Karras sigmas\n",
    "- Karras scheduler’s use the timestep schedule and sigmas from the Elucidating the Design Space of Diffusion-Based Generative Models paper.\n",
    "  - This scheduler variant applies a smaller amount of noise per step as it approaches the end of the sampling process compared to other schedulers, and can increase the level of details in the generated image.\n",
    "\n",
    "Enable Karras sigmas by setting use_karras_sigmas=True in the scheduler.\n",
    "- Karras sigmas should not be used for models that weren’t trained with them.\n",
    "  - ex. The base `Stable Diffusion XL` model shouldn’t use Karras sigmas but the `DreamShaperXL` model can since they are trained with Karras sigmas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea2c54-c9ed-49df-b89b-9f00ff35612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"SG161222/RealVisXL_V4.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, algorithm_type=\"sde-dpmsolver++\", use_karras_sigmas=True)\n",
    "\n",
    "prompt = \"A cinematic shot of a cute little rabbit wearing a jacket and doing a thumbs up\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(2487854446)\n",
    "image = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"\",\n",
    "    generator=generator,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32775b-707c-445b-bfae-a58c6aa81810",
   "metadata": {},
   "source": [
    "#### Rescale noise schedule\n",
    "- Common noise schedules allowed some signal to leak into the last timestep.\n",
    "  - This signal leakage at inference can cause models to only generate images with medium brightness.\n",
    "  - By enforcing a zero signal-to-noise ratio (SNR) for the timstep schedule and sampling from the last timestep, the model can be improved to generate very bright or dark images.\n",
    "\n",
    "- Load the `ptx0/pseudo-journey-v2` checkpoint which was trained with `v_prediction` and the `DDIMScheduler`.\n",
    "  - `rescale_betas_zero_snr=True` to rescale the noise schedule to zero SNR\n",
    "  - `timestep_spacing=\"trailing\"` to start sampling from the last timestep\n",
    "\n",
    "- Set `guidance_rescale` in the pipeline to prevent over-exposure.\n",
    "  - A lower value increases brightness but some of the details may appear washed out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e75506-066b-424f-b2cd-93ce7f447e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler = DDIMScheduler.from_config(\n",
    "    pipeline.scheduler.config, rescale_betas_zero_snr=True, timestep_spacing=\"trailing\"\n",
    ")\n",
    "pipeline.to(\"cuda\")\n",
    "prompt = \"cinematic photo of a snowy mountain at night with the northern lights aurora borealis overhead, 35mm photograph, film, professional, 4k, highly detailed\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(23)\n",
    "image = pipeline(prompt, guidance_rescale=0.7, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b4afa8-ded1-4872-b429-cf023f7d0f92",
   "metadata": {},
   "source": [
    "-----------\n",
    "### **Pipeline Callbacks**\n",
    "- The denoising loop of a pipeline can be modified with custom defined functions using the `callback_on_step_end` parameter.\n",
    "  - The callback function is executed at the end of each step, and modifies the pipeline attributes and variables for the next step.\n",
    "  - This is really useful for dynamically adjusting certain pipeline attributes or modifying tensor variables.\n",
    "\n",
    "\n",
    "#### Official callbacks\n",
    "- We provide a list of callbacks you can plug into an existing pipeline and modify the denoising loop.\n",
    "  - `SDCFGCutoffCallback`: Disables the CFG after a certain number of steps for all SD 1.5 pipelines, including text-to-image, image-to-image, inpaint, and controlnet.\n",
    "  - SDXLCFGCutoffCallback`: Disables the CFG after a certain number of steps for all SDXL pipelines, including text-to-image, image-to-image, inpaint, and controlnet.\n",
    "  - `IPAdapterScaleCutoffCallback`: Disables the IP Adapter after a certain number of steps for all pipelines supporting IP-Adapter.\n",
    "If you want to add a new official callback, feel free to open a feature request or submit a PR.\n",
    "\n",
    "- To set up a callback, you need to specify the number of denoising steps after which the callback comes into effect.\n",
    "  - `cutoff_step_ratio`: Float number with the ratio of the steps.\n",
    "  - `cutoff_step_index`: Integer number with the exact number of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a676fb-e962-4137-9d29-fdded5c69d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = SDXLCFGCutoffCallback(cutoff_step_ratio=0.4)\n",
    "# can also be used with cutoff_step_index\n",
    "# callback = SDXLCFGCutoffCallback(cutoff_step_ratio=None, cutoff_step_index=10)\n",
    "\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, use_karras_sigmas=True)\n",
    "\n",
    "prompt = \"a sports car at the road, best quality, high quality, high detail, 8k resolution\"\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(2628670641)\n",
    "\n",
    "out = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=\"\",\n",
    "    guidance_scale=6.5,\n",
    "    num_inference_steps=25,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=callback,\n",
    ")\n",
    "\n",
    "out.images[0].save(\"official_callback.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5ec9f-05cd-4e84-902e-122173283603",
   "metadata": {},
   "source": [
    "#### Dynamic classifier-free guidance\n",
    "- Dynamic classifier-free guidance (CFG) is a feature that allows you to disable CFG after a certain number of inference steps which can help you save compute with minimal cost to performance.\n",
    "  - `pipeline` (or the pipeline instance) provides access to important properties such as `num_timesteps` and `guidance_scale`.\n",
    "    - You can modify these properties by updating the underlying attributes. You’ll disable CFG by setting `pipeline._guidance_scale=0.0`.\n",
    "  - `step_index` and `timestep` tell you where you are in the denoising loop.\n",
    "    - Use `step_index` to turn off CFG after reaching 40% of `num_timesteps`.\n",
    "  - `callback_kwargs` is a dict that contains tensor variables you can modify during the denoising loop.\n",
    "    - It only includes variables specified in the `callback_on_step_end_tensor_inputs argument`, which is passed to the pipeline’s `__call__` method.\n",
    "    - Different pipelines may use different sets of variables, so please check a pipeline’s `_callback_tensor_inputs` attribute for the list of variables you can modify.\n",
    "    - Some common variables include latents and `prompt_embeds`. For this function, change the batch size of `prompt_embeds` after setting `guidance_scale=0.0` in order for it to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d936cea-02e1-4fae-9136-be7b91717d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_dynamic_cfg(pipe, step_index, timestep, callback_kwargs):\n",
    "    # adjust the batch_size of prompt_embeds according to guidance_scale\n",
    "    if step_index == int(pipeline.num_timesteps * 0.4):\n",
    "            prompt_embeds = callback_kwargs[\"prompt_embeds\"]\n",
    "            prompt_embeds = prompt_embeds.chunk(2)[-1]\n",
    "\n",
    "            # update guidance_scale and prompt_embeds\n",
    "            pipeline._guidance_scale = 0.0\n",
    "            callback_kwargs[\"prompt_embeds\"] = prompt_embeds\n",
    "    return callback_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c27b9-bda7-4b3f-aa0c-17be6c30c58e",
   "metadata": {},
   "source": [
    "- Pass the callback function to the `callback_on_step_end` parameter and the `prompt_embeds` to `callback_on_step_end_tensor_inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1a521-85d8-4016-b75a-7adead2a0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(1)\n",
    "out = pipeline(\n",
    "    prompt,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=callback_dynamic_cfg,\n",
    "    callback_on_step_end_tensor_inputs=['prompt_embeds']\n",
    ")\n",
    "\n",
    "out.images[0].save(\"out_custom_cfg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736f603-7c20-4f00-957f-ac5c9a013293",
   "metadata": {},
   "source": [
    "#### Interrupt the diffusion process\n",
    "- Stopping the diffusion process early is useful when building UIs that work with Diffusers because it allows users to stop the generation process if they’re unhappy with the intermediate results.\n",
    "  - You can incorporate this into your pipeline with a callback.\n",
    "\n",
    "- This callback function should take the following arguments: `pipeline`, `i`, `t`, and `callback_kwargs` (this must be returned).   - Set the pipeline’s `_interrupt` attribute to `True` to stop the diffusion process after a certain number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a6acb-88ee-4392-aca6-831b4051b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "num_inference_steps = 50\n",
    "\n",
    "def interrupt_callback(pipeline, i, t, callback_kwargs):\n",
    "    stop_idx = 10\n",
    "    if i == stop_idx:\n",
    "        pipeline._interrupt = True\n",
    "\n",
    "    return callback_kwargs\n",
    "\n",
    "pipeline(\n",
    "    \"A photo of a cat\",\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    callback_on_step_end=interrupt_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86565fb-a112-43bf-bf43-2ea8aed514b2",
   "metadata": {},
   "source": [
    "#### IP Adapter Cutoff\n",
    "- IP Adapter is an image prompt adapter that can be used for diffusion models without any changes to the underlying model.\n",
    "  - We can use the IP Adapter Cutoff Callback to disable the IP Adapter after a certain number of steps.\n",
    "  - To set up the callback, you need to specify the number of denoising steps after which the callback comes into effect.\n",
    "    - `cutoff_step_ratio`: Float number with the ratio of the steps.\n",
    "    - `cutoff_step_index`: Integer number with the exact number of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4a2b0-a16b-4268-ac64-b234e07bb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
    "pipeline.set_ip_adapter_scale(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e682c-0b48-4628-be14-23750d21bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "pipeline.load_ip_adapter(\n",
    "    \"h94/IP-Adapter\", \n",
    "    subfolder=\"sdxl_models\", \n",
    "    weight_name=\"ip-adapter_sdxl.bin\"\n",
    ")\n",
    "\n",
    "pipeline.set_ip_adapter_scale(0.6)\n",
    "\n",
    "\n",
    "callback = IPAdapterScaleCutoffCallback(\n",
    "    cutoff_step_ratio=None, \n",
    "    cutoff_step_index=5\n",
    ")\n",
    "\n",
    "image = load_image(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_adapter_diner.png\"\n",
    ")\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(2628670641)\n",
    "\n",
    "images = pipeline(\n",
    "    prompt=\"a tiger sitting in a chair drinking orange juice\",\n",
    "    ip_adapter_image=image,\n",
    "    negative_prompt=\"deformed, ugly, wrong proportion, low res, bad anatomy, worst quality, low quality\",\n",
    "    generator=generator,\n",
    "    num_inference_steps=50,\n",
    "    callback_on_step_end=callback,\n",
    ").images\n",
    "\n",
    "images[0].save(\"custom_callback_img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3db91b-2752-4294-8e82-31ed090b1f74",
   "metadata": {},
   "source": [
    "#### Display image after each generation step\n",
    "- Display an image after each generation step by accessing and converting the latents after each step into an image.\n",
    "  - The latent space is compressed to 128x128, so the images are also 128x128 which is useful for a quick preview.\n",
    " \n",
    "- Use the function below to convert the SDXL latents (4 channels) to RGB tensors (3 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ef51e-33ba-4a12-98a7-037f16e39625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latents_to_rgb(latents):\n",
    "    weights = (\n",
    "        (60, -60, 25, -70),\n",
    "        (60,  -5, 15, -50),\n",
    "        (60,  10, -5, -35),\n",
    "    )\n",
    "\n",
    "    weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))\n",
    "    biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(latents.device)\n",
    "    rgb_tensor = torch.einsum(\"...lxy,lr -> ...rxy\", latents, weights_tensor) + biases_tensor.unsqueeze(-1).unsqueeze(-1)\n",
    "    image_array = rgb_tensor.clamp(0, 255).byte().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return Image.fromarray(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667b6b3-ad42-4a69-ad80-473b2ca89d87",
   "metadata": {},
   "source": [
    "- Create a function to decode and save the latents into an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2de775-43dc-4b30-b876-56b8e5855169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tensors(pipe, step, timestep, callback_kwargs):\n",
    "    latents = callback_kwargs[\"latents\"]\n",
    "\n",
    "    image = latents_to_rgb(latents[0])\n",
    "    image.save(f\"{step}.png\")\n",
    "\n",
    "    return callback_kwargs\n",
    "Pass the decode_tensors function to the callback_on_step_end parameter to decode the tensors after each step. You also need to specify what you want to modify in the callback_on_step_end_tensor_inputs parameter, which in this case are the latents.\n",
    "Copied\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"A croissant shaped like a cute bear.\",\n",
    "    negative_prompt=\"Deformed, ugly, bad anatomy\",\n",
    "    callback_on_step_end=decode_tensors,\n",
    "    callback_on_step_end_tensor_inputs=[\"latents\"],\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebb0ea-d153-4c21-9472-4198e0fb406e",
   "metadata": {},
   "source": [
    "-----------\n",
    "### **Reproducible Pipelines**\n",
    "- Diffusion models are inherently random which is what allows it to generate different outputs every time it is run.\n",
    "  - But there are certain times when you want to generate the same output every time, like when you’re testing, replicating results, and even improving image quality.\n",
    "  - While you can’t expect to get identical results across platforms, you can expect reproducible results across releases and platforms within a certain tolerance range (though even this may vary).\n",
    "\n",
    "- Show you how to control randomness for deterministic generation on a CPU and GPU.\n",
    "\n",
    "#### Control randomness\n",
    "- During inference, pipelines rely heavily on random sampling operations which include creating the Gaussian noise tensors to denoise and adding noise to the scheduling step.\n",
    "  - Take a look at the tensor values in the `DDIMPipeline` after two inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdc572-d19d-4c28-88a4-f55bfa4d8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim = DDIMPipeline.from_pretrained( \"google/ddpm-cifar10-32\", use_safetensors=True)\n",
    "image = ddim(num_inference_steps=2, output_type=\"np\").images\n",
    "print(np.abs(image).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69ad71-fb7c-4601-9d5b-6b37d1c68183",
   "metadata": {},
   "source": [
    "- Each time the pipeline is run, `torch.randn` uses a different random seed to create the Gaussian noise tensors.\n",
    "  - This leads to a different result each time it is run and enables the diffusion pipeline to generate a different random image each time.\n",
    "  - But if you need to reliably generate the same image, that depends on whether you’re running the pipeline on a CPU or GPU.\n",
    "\n",
    "- It might seem unintuitive to pass Generator objects to a pipeline instead of the integer value representing the seed.\n",
    "  - However, this is the recommended design when working with probabilistic models in PyTorch because a Generator is a random state that can be passed to multiple pipelines in a sequence.\n",
    "  - As soon as the Generator is consumed, the state is changed in place which means even if you passed the same Generator to a different pipeline, it won’t produce the same result because the state is already changed.\n",
    "\n",
    "- To generate reproducible results on a **CPU**, you’ll need to use a PyTorch Generator and set a seed.\n",
    "  - Now when you run the code, it always prints a value of 1491.1711 because the Generator object with the seed is passed to all the random functions in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244bcfea-e0a2-4cd2-9f54-681fc1b27c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim = DDIMPipeline.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "image = ddim(num_inference_steps=2, output_type=\"np\", generator=generator).images\n",
    "print(np.abs(image).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917971ad-bfc7-49f8-b380-1a902f46419d",
   "metadata": {},
   "source": [
    "- Writing a reproducible pipeline on a **GPU** is a bit trickier, and full reproducibility across different hardware is not guaranteed because matrix multiplication - which diffusion pipelines require a lot of - is less deterministic on a GPU than a CPU.     - If you run the same code example from the CPU example, you’ll get a different result even though the seed is identical.\n",
    "  - This is because the GPU uses a different random number generator than the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e007c-1315-407d-b6c5-4280870b2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim = DDIMPipeline.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)\n",
    "ddim.to(\"cuda\")\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
    "image = ddim(num_inference_steps=2, output_type=\"np\", generator=generator).images\n",
    "print(np.abs(image).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288bedb-c0ac-4d68-8980-adee65012a57",
   "metadata": {},
   "source": [
    "- To avoid this issue, Diffusers has a `randn_tensor()` function for creating random noise on the CPU, and then moving the tensor to a GPU if necessary.\n",
    "  - The `randn_tensor()` function is used everywhere inside the pipeline.\n",
    "  - Now you can call `torch.manual_seed` which automatically creates a CPU Generator that can be passed to the pipeline even if it is being run on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d0913-74d0-43dc-ba70-f7b508a29cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim = DDIMPipeline.from_pretrained(\"google/ddpm-cifar10-32\", use_safetensors=True)\n",
    "ddim.to(\"cuda\")\n",
    "generator = torch.manual_seed(0)\n",
    "image = ddim(num_inference_steps=2, output_type=\"np\", generator=generator).images\n",
    "print(np.abs(image).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30737449-496d-4031-a54a-e8987e8cec19",
   "metadata": {},
   "source": [
    "#### Deterministic algorithms\n",
    "- You can also configure PyTorch to use deterministic algorithms to create a **reproducible pipeline**.\n",
    "  - The downside is that deterministic algorithms may be slower than non-deterministic ones and you may observe a decrease in performance.\n",
    "  - Non-deterministic behavior occurs when operations are launched in more than one CUDA stream.\n",
    "    - To avoid this, set the environment variable `CUBLAS_WORKSPACE_CONFIG` to `:16:8` to only use one buffer size during runtime.\n",
    "\n",
    "- PyTorch typically benchmarks multiple algorithms to select the fastest one, but if you want reproducibility, you should disable this feature because the benchmark may select different algorithms each time.\n",
    "  - Set Diffusers `enable_full_determinism` to enable deterministic algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357f0e8-8eb1-4c12-9537-d4b1567ab18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_full_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069569e4-5219-4fc3-aab7-0053eb5b97fe",
   "metadata": {},
   "source": [
    "- You’ll get identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e1169-7696-410d-912f-63744bef5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", use_safetensors=True).to(\"cuda\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "g = torch.Generator(device=\"cuda\")\n",
    "\n",
    "prompt = \"A bear is playing a guitar on Times Square\"\n",
    "\n",
    "g.manual_seed(0)\n",
    "result1 = pipe(prompt=prompt, num_inference_steps=50, generator=g, output_type=\"latent\").images\n",
    "\n",
    "g.manual_seed(0)\n",
    "result2 = pipe(prompt=prompt, num_inference_steps=50, generator=g, output_type=\"latent\").images\n",
    "\n",
    "print(\"L_inf dist =\", abs(result1 - result2).max())\n",
    "\"L_inf dist = tensor(0., device='cuda:0')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77839aa2-78c6-4ff5-8993-3ef390f7b215",
   "metadata": {},
   "source": [
    "#### Deterministic batch generation\n",
    "- A practical application of creating reproducible pipelines is deterministic batch generation.\n",
    "- You generate a batch of images and select one image to improve with a more detailed prompt.\n",
    "  - The main idea is to pass a list of Generator’s to the pipeline and tie each Generator to a seed so you can reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92510b2e-a309-40e9-a47a-3fdaedf7834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "\n",
    "generator = [torch.Generator(device=\"cuda\").manual_seed(i) for i in range(4)]\n",
    "prompt = \"Labrador in the style of Vermeer\"\n",
    "images = pipeline(prompt, generator=generator, num_images_per_prompt=4).images[0]\n",
    "make_image_grid(images, rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5cbecb-f19f-4ffb-baae-3917d5eda1a3",
   "metadata": {},
   "source": [
    "- Let’s improve the first image (you can choose any image you want) which corresponds to the Generator with seed 0.\n",
    "  - Add some additional text to your prompt and then make sure you reuse the same Generator with seed 0.\n",
    "  - All the generated images should resemble the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596aa9a-ee08-4faa-9979-d435bcaacedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [prompt + t for t in [\", highly realistic\", \", artsy\", \", trending\", \", colorful\"]]\n",
    "generator = [torch.Generator(device=\"cuda\").manual_seed(0) for i in range(4)]\n",
    "images = pipeline(prompt, generator=generator).images\n",
    "make_image_grid(images, rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c4575-53a8-4824-8b85-a8438a2696bb",
   "metadata": {},
   "source": [
    "-----------\n",
    "### **Controlling Image Quality**\n",
    "- The components of a diffusion model, like the UNet and scheduler, can be optimized to improve the quality of generated images leading to better details.\n",
    "  - These techniques are especially useful if you don’t have the resources to simply use a larger model for inference.\n",
    "  - You can enable these techniques during inference without any additional training.\n",
    "\n",
    "- FreeU improves image details by rebalancing the UNet’s backbone and skip connection weights.\n",
    "  - The skip connections can cause the model to overlook some of the backbone semantics which may lead to unnatural image details in the generated image.\n",
    "  - This technique does not require any additional training and can be applied on the fly during inference for tasks like image-to-image and text-to-video.\n",
    "\n",
    "- Use the `enable_freeu()` method on your pipeline and configure the scaling factors for the backbone (b1 and b2) and skip connections (s1 and s2).\n",
    "  - The number after each scaling factor corresponds to the stage in the UNet where the factor is applied.\n",
    "  - Take a look at the FreeU repository for reference hyperparameters for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d48817-d952-4e51-a055-8e16e649e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16, safety_checker=None\n",
    ").to(\"cuda\")\n",
    "pipeline.enable_freeu(s1=0.9, s2=0.2, b1=1.5, b2=1.6)\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "prompt = \"\"\n",
    "image = pipeline(prompt, generator=generator).images[0]\n",
    "\n",
    "pipeline.disable_freeu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3c12d-a52a-41af-9d13-ee51b314392e",
   "metadata": {},
   "source": [
    "-----------\n",
    "### **Prompt Techniques**\n",
    "- Prompts are important because they describe what you want a diffusion model to generate.\n",
    "  - The best prompts are detailed, specific, and well-structured to help the model realize your vision.\n",
    "  - But crafting a great prompt takes time and effort and sometimes it may not be enough because language and words can be imprecise.\n",
    "\n",
    "#### Prompt engineering\n",
    "- New diffusion models do a pretty good job of generating high-quality images from a basic prompt, but it is still **important to create a well-written prompt to get the best results**.\n",
    "- Here are a few tips for writing a good prompt:\n",
    "\n",
    "```\n",
    "1. What is the image medium? Is it a photo, a painting, a 3D illustration, or something else?\n",
    "2. What is the image subject? Is it a person, animal, object, or scene?\n",
    "3. What details would you like to see in the image? This is where you can get really creative and have a lot of fun experimenting with different words to bring your image to life. For example, what is the lighting like? What is the vibe and aesthetic? What kind of art or illustration style are you looking for? The more specific and precise words you use, the better the model will understand what you want to generate.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40db2f-0e82-4103-b041-d2bc82e6eeb8",
   "metadata": {},
   "source": [
    "#### Prompt enhancing with GPT2\n",
    "- Prompt enhancing is a technique for quickly improving prompt quality without spending too much effort constructing one.\n",
    "  - It uses a model like GPT2 pretrained on Stable Diffusion text prompts to automatically enrich a prompt with additional important keywords to generate high-quality images.\n",
    "\n",
    "- The technique works by curating a list of specific keywords and forcing the model to generate those words to enhance the original prompt.\n",
    "  - Your prompt can be “a cat” and GPT2 can enhance the prompt to “cinematic film still of a cat basking in the sun on a roof in Turkey, highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain quality sharp focus beautiful detailed intricate stunning amazing epic”.\n",
    "  - You should also use a offset noise LoRA to improve the contrast in bright and dark images and create better lighting overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08eb94-d06e-431b-83f8-7fab5889c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = {\n",
    "    \"cinematic\": \"cinematic film still of {prompt}, highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain\",\n",
    "    \"anime\": \"anime artwork of {prompt}, anime style, key visual, vibrant, studio anime, highly detailed\",\n",
    "    \"photographic\": \"cinematic photo of {prompt}, 35mm photograph, film, professional, 4k, highly detailed\",\n",
    "    \"comic\": \"comic of {prompt}, graphic illustration, comic art, graphic novel art, vibrant, highly detailed\",\n",
    "    \"lineart\": \"line art drawing {prompt}, professional, sleek, modern, minimalist, graphic, line art, vector graphics\",\n",
    "    \"pixelart\": \" pixel-art {prompt}, low-res, blocky, pixel art style, 8-bit graphics\",\n",
    "}\n",
    "\n",
    "words = [\n",
    "    \"aesthetic\", \"astonishing\", \"beautiful\", \"breathtaking\", \"composition\", \"contrasted\", \"epic\", \"moody\", \"enhanced\",\n",
    "    \"exceptional\", \"fascinating\", \"flawless\", \"glamorous\", \"glorious\", \"illumination\", \"impressive\", \"improved\",\n",
    "    \"inspirational\", \"magnificent\", \"majestic\", \"hyperrealistic\", \"smooth\", \"sharp\", \"focus\", \"stunning\", \"detailed\",\n",
    "    \"intricate\", \"dramatic\", \"high\", \"quality\", \"perfect\", \"light\", \"ultra\", \"highly\", \"radiant\", \"satisfying\",\n",
    "    \"soothing\", \"sophisticated\", \"stylish\", \"sublime\", \"terrific\", \"touching\", \"timeless\", \"wonderful\", \"unbelievable\",\n",
    "    \"elegant\", \"awesome\", \"amazing\", \"dynamic\", \"trendy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c0d85-e34e-4210-944c-8864bd842fca",
   "metadata": {},
   "source": [
    "- You may have noticed in the words list, there are certain words that can be paired together to create something more meaningful.\n",
    "  - The words “high” and “quality” can be combined to create “high quality”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43a73e-d6c3-4ad4-9d9c-3ae8f675ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\"highly detailed\", \"high quality\", \"enhanced quality\", \"perfect composition\", \"dynamic light\"]\n",
    "\n",
    "def find_and_order_pairs(s, pairs):\n",
    "    words = s.split()\n",
    "    found_pairs = []\n",
    "    for pair in pairs:\n",
    "        pair_words = pair.split()\n",
    "        if pair_words[0] in words and pair_words[1] in words:\n",
    "            found_pairs.append(pair)\n",
    "            words.remove(pair_words[0])\n",
    "            words.remove(pair_words[1])\n",
    "\n",
    "    for word in words[:]:\n",
    "        for pair in pairs:\n",
    "            if word in pair.split():\n",
    "                words.remove(word)\n",
    "                break\n",
    "    ordered_pairs = \", \".join(found_pairs)\n",
    "    remaining_s = \", \".join(words)\n",
    "    return ordered_pairs, remaining_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49234628-823f-44f4-b4b6-386c47b1e421",
   "metadata": {},
   "source": [
    "- Implement a custom `LogitsProcessor` class that assigns tokens in the words list a value of 0 and assigns tokens not in the words list a negative value so they aren’t picked during generation.\n",
    "  - Generation is biased towards words in the words list.\n",
    "  - After a word from the list is used, it is also assigned a negative value so it isn’t picked again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f63268-0040-492f-92b0-b2cefee6d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, bias):\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        if len(input_ids.shape) == 2:\n",
    "            last_token_id = input_ids[0, -1]\n",
    "            self.bias[last_token_id] = -1e10\n",
    "        return scores + self.bias\n",
    "\n",
    "word_ids = [tokenizer.encode(word, add_prefix_space=True)[0] for word in words]\n",
    "bias = torch.full((tokenizer.vocab_size,), -float(\"Inf\")).to(\"cuda\")\n",
    "bias[word_ids] = 0\n",
    "processor = CustomLogitsProcessor(bias)\n",
    "processor_list = LogitsProcessorList([processor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad8426-2a13-40dd-a88a-94de31e9594a",
   "metadata": {},
   "source": [
    "- Combine the prompt and the cinematic style prompt defined in the styles dictionary earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfae2bd-a1ba-4d77-be6f-2d1c202802ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a cat basking in the sun on a roof in Turkey\"\n",
    "style = \"cinematic\"\n",
    "\n",
    "prompt = styles[style].format(prompt=prompt)\n",
    "prompt\n",
    "\"cinematic film still of a cat basking in the sun on a roof in Turkey, highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5106ec-394f-47a8-bdd0-8a00f85f4fa6",
   "metadata": {},
   "source": [
    "- Load a GPT2 tokenizer and model from the `Gustavosta/MagicPrompt-Stable-Diffusion` checkpoint (this specific checkpoint is trained to generate prompts) to enhance the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36b28c-30fd-49b3-9e21-7954653c5ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"Gustavosta/MagicPrompt-Stable-Diffusion\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"Gustavosta/MagicPrompt-Stable-Diffusion\", torch_dtype=torch.float16).to(\n",
    "    \"cuda\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "token_count = inputs[\"input_ids\"].shape[1]\n",
    "max_new_tokens = 50 - token_count\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.7,\n",
    "    top_k=50,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.eos_token_id,\n",
    "    pad_token=model.config.pad_token_id,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        generation_config=generation_config,\n",
    "        logits_processor=proccesor_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c4e49-b690-422d-a734-b60bd7557d16",
   "metadata": {},
   "source": [
    "- You can combine the input prompt and the generated prompt.\n",
    "  - Feel free to take a look at what the generated prompt (`generated_part`) is, the word pairs that were found (`pairs`), and the remaining words (`words`).\n",
    "  - This is all packed together in the `enhanced_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f243102-7fe9-4a3d-9a6c-66b90d09acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens = [tokenizer.decode(generated_id, skip_special_tokens=True) for generated_id in generated_ids]\n",
    "input_part, generated_part = output_tokens[0][: len(prompt)], output_tokens[0][len(prompt) :]\n",
    "pairs, words = find_and_order_pairs(generated_part, word_pairs)\n",
    "formatted_generated_part = pairs + \", \" + words\n",
    "enhanced_prompt = input_part + \", \" + formatted_generated_part\n",
    "enhanced_prompt\n",
    "[\"cinematic film still of a cat basking in the sun on a roof in Turkey, highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain quality sharp focus beautiful detailed intricate stunning amazing epic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75a6b4-26a0-4804-b772-e1eba8d4b41b",
   "metadata": {},
   "source": [
    "- Load a pipeline and the offset noise LoRA with a low weight to generate an image with the enhanced prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7812380-ea71-4a39-a174-d807dff9dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"RunDiffusion/Juggernaut-XL-v9\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline.load_lora_weights(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    weight_name=\"sd_xl_offset_example-lora_1.0.safetensors\",\n",
    "    adapter_name=\"offset\",\n",
    ")\n",
    "pipeline.set_adapters([\"offset\"], adapter_weights=[0.2])\n",
    "\n",
    "image = pipeline(\n",
    "    enhanced_prompt,\n",
    "    width=1152,\n",
    "    height=896,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91ce1b-eb71-467d-a2d8-706b11281eae",
   "metadata": {},
   "source": [
    "#### Prompt weighting\n",
    "- Prompt weighting provides a way to emphasize or de-emphasize certain parts of a prompt, allowing for more control over the generated image.\n",
    "  - A prompt can include several concepts, which gets turned into contextualized text embeddings.\n",
    "  - The embeddings are used by the model to condition its cross-attention layers to generate an image (read the Stable Diffusion blog post to learn more about how it works).\n",
    "\n",
    "- Prompt weighting works by increasing or decreasing the scale of the text embedding vector that corresponds to its concept in the prompt because you may not necessarily want the model to focus on all concepts equally.\n",
    "  - The easiest way to prepare the prompt embeddings is to use Stable Diffusion Long Prompt Weighted Embedding (`sd_embed`).\n",
    "  - Once you have the prompt-weighted embeddings, you can pass them to any pipeline that has a `prompt_embeds` (and optionally `negative_prompt_embeds`) parameter, such as `StableDiffusionPipeline`, `StableDiffusionControlNetPipeline`, and `StableDiffusionXLPipeline`.\n",
    "\n",
    "- Make sure you have the latest version of `sd_embed` installed:\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/xhinker/sd_embed.git@main\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deaaa00-cbfc-4202-88ec-26ece13e6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"Lykon/dreamshaper-xl-1-0\", torch_dtype=torch.float16)\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af146b8-85e9-4bc2-80eb-a325e36881bb",
   "metadata": {},
   "source": [
    "- To upweight or downweight a concept, surround the text with parentheses.\n",
    "  - More parentheses applies a heavier weight on the text.\n",
    "  - You can also append a numerical multiplier to the text to indicate how much you want to increase or decrease its weights by.\n",
    "\n",
    "- Create a prompt and use a combination of parentheses and numerical multipliers to upweight various text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47c222-c7a1-4d98-afd8-1394b6db1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. \n",
    "This imaginative creature features the distinctive, bulky body of a hippo, \n",
    "but with a texture and appearance resembling a golden-brown, crispy waffle. \n",
    "The creature might have elements like waffle squares across its skin and a syrup-like sheen. \n",
    "It's set in a surreal environment that playfully combines a natural water habitat of a hippo with elements of a breakfast table setting, \n",
    "possibly including oversized utensils or plates in the background. \n",
    "The image should evoke a sense of playful absurdity and culinary fantasy.\n",
    "\"\"\"\n",
    "\n",
    "neg_prompt = \"\"\"\\\n",
    "skin spots,acnes,skin blemishes,age spot,(ugly:1.2),(duplicate:1.2),(morbid:1.21),(mutilated:1.2),\\\n",
    "(tranny:1.2),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.2),(bad proportions:1.3),\\\n",
    "extra limbs,(disfigured:1.2),(missing arms:1.2),(extra legs:1.2),(fused fingers:1.5),\\\n",
    "(too many fingers:1.5),(unclear eyes:1.2),lowers,bad hands,missing fingers,extra digit,\\\n",
    "bad hands,missing fingers,(extra arms and legs),(worst quality:2),(low quality:2),\\\n",
    "(normal quality:2),lowres,((monochrome)),((grayscale))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097806c-38c8-4b1d-b14b-4a2114b7b8cb",
   "metadata": {},
   "source": [
    "- Use the `get_weighted_text_embeddings_sdxl` function to generate the prompt embeddings and the negative prompt embeddings.\n",
    "  - It’ll also generated the pooled and negative pooled prompt embeddings since you’re using the SDXL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3485174-9291-4d1a-a75d-09110dc4a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "( \n",
    "  prompt_embeds,\n",
    "  prompt_neg_embeds,\n",
    "  pooled_prompt_embeds,\n",
    "  negative_pooled_prompt_embeds\n",
    ") = get_weighted_text_embeddings_sdxl(\n",
    "    pipe,\n",
    "    prompt=prompt,\n",
    "    neg_prompt=neg_prompt\n",
    ")\n",
    "\n",
    "image = pipe(\n",
    "    prompt_embeds=prompt_embeds,\n",
    "    negative_prompt_embeds=prompt_neg_embeds,\n",
    "    pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "    negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "    num_inference_steps=30,\n",
    "    height=1024,\n",
    "    width=1024 + 512,\n",
    "    guidance_scale=4.0,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(2)\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff07b9-7dff-417e-b9ef-d0ac3958196c",
   "metadata": {},
   "source": [
    "#### Textual inversion\n",
    "- Textual inversion is a technique for learning a specific concept from some images which you can use to generate new images conditioned on that concept.\n",
    "  - Create a pipeline and use the `load_textual_inversion()` function to load the textual inversion embeddings (feel free to browse the Stable Diffusion Conceptualizer for 100+ trained concepts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbca8fd-cce8-4d2a-bdc0-735823ea1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "  \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "  torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "pipe.load_textual_inversion(\"sd-concepts-library/midjourney-style\")\n",
    "\n",
    "prompt = \"\"\"<midjourney-style> A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. \n",
    "This imaginative creature features the distinctive, bulky body of a hippo, \n",
    "but with a texture and appearance resembling a golden-brown, crispy waffle. \n",
    "The creature might have elements like waffle squares across its skin and a syrup-like sheen. \n",
    "It's set in a surreal environment that playfully combines a natural water habitat of a hippo with elements of a breakfast table setting, \n",
    "possibly including oversized utensils or plates in the background. \n",
    "The image should evoke a sense of playful absurdity and culinary fantasy.\n",
    "\"\"\"\n",
    "\n",
    "neg_prompt = \"\"\"\\\n",
    "skin spots,acnes,skin blemishes,age spot,(ugly:1.2),(duplicate:1.2),(morbid:1.21),(mutilated:1.2),\\\n",
    "(tranny:1.2),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.2),(bad proportions:1.3),\\\n",
    "extra limbs,(disfigured:1.2),(missing arms:1.2),(extra legs:1.2),(fused fingers:1.5),\\\n",
    "(too many fingers:1.5),(unclear eyes:1.2),lowers,bad hands,missing fingers,extra digit,\\\n",
    "bad hands,missing fingers,(extra arms and legs),(worst quality:2),(low quality:2),\\\n",
    "(normal quality:2),lowres,((monochrome)),((grayscale))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dca54c-5e4f-4852-b129-a750b7617ce7",
   "metadata": {},
   "source": [
    "- Use the `get_weighted_text_embeddings_sd15` function to generate the prompt embeddings and the negative prompt embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788331b-2888-4b67-a84d-27de4359a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "( \n",
    "  prompt_embeds,\n",
    "  prompt_neg_embeds,\n",
    ") = get_weighted_text_embeddings_sd15(\n",
    "    pipe,\n",
    "    prompt=prompt,\n",
    "    neg_prompt=neg_prompt\n",
    ")\n",
    "\n",
    "image = pipe(\n",
    "    prompt_embeds=prompt_embeds,\n",
    "    negative_prompt_embeds=prompt_neg_embeds,\n",
    "    height=768,\n",
    "    width=896,\n",
    "    guidance_scale=4.0,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(2)\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7b908-5b99-48e1-b993-3da1a91279bf",
   "metadata": {},
   "source": [
    "#### DreamBooth\n",
    "- DreamBooth is a technique for generating contextualized images of a subject given just a few images of the subject to train on.\n",
    "  - It is similar to textual inversion, but DreamBooth trains the full model whereas textual inversion only fine-tunes the text embeddings.\n",
    "  - This means you should use `from_pretrained()` to load the DreamBooth model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0b5e9-998c-44f4-8473-baa42bebdce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"sd-dreambooth-library/dndcoverart-v1\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"\"\"dndcoverart of A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. \n",
    "This imaginative creature features the distinctive, bulky body of a hippo, \n",
    "but with a texture and appearance resembling a golden-brown, crispy waffle. \n",
    "The creature might have elements like waffle squares across its skin and a syrup-like sheen. \n",
    "It's set in a surreal environment that playfully combines a natural water habitat of a hippo with elements of a breakfast table setting, \n",
    "possibly including oversized utensils or plates in the background. \n",
    "The image should evoke a sense of playful absurdity and culinary fantasy.\n",
    "\"\"\"\n",
    "\n",
    "neg_prompt = \"\"\"\\\n",
    "skin spots,acnes,skin blemishes,age spot,(ugly:1.2),(duplicate:1.2),(morbid:1.21),(mutilated:1.2),\\\n",
    "(tranny:1.2),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.2),(bad proportions:1.3),\\\n",
    "extra limbs,(disfigured:1.2),(missing arms:1.2),(extra legs:1.2),(fused fingers:1.5),\\\n",
    "(too many fingers:1.5),(unclear eyes:1.2),lowers,bad hands,missing fingers,extra digit,\\\n",
    "bad hands,missing fingers,(extra arms and legs),(worst quality:2),(low quality:2),\\\n",
    "(normal quality:2),lowres,((monochrome)),((grayscale))\n",
    "\"\"\"\n",
    "\n",
    "(\n",
    "    prompt_embeds\n",
    "    , prompt_neg_embeds\n",
    ") = get_weighted_text_embeddings_sd15(\n",
    "    pipe\n",
    "    , prompt = prompt\n",
    "    , neg_prompt = neg_prompt\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers-t2i",
   "language": "python",
   "name": "diffusers-t2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
